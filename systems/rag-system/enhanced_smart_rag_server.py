#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ™ºèƒ½RAGç³»ç»ŸæœåŠ¡å™¨
çœŸæ­£çš„AIæ–‡æ¡£ç†è§£å’Œé—®ç­”èƒ½åŠ›
"""

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import json
import os
import re
from datetime import datetime
from pathlib import Path
import pytz
import logging
import argparse
from typing import List, Dict, Any
import hashlib
from docx import Document

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# é…ç½®
TIMEZONE = pytz.timezone('Asia/Shanghai')
current_file = Path(__file__)
UPLOAD_FOLDER = current_file.parent / 'uploads'
UPLOAD_FOLDER.mkdir(exist_ok=True)

# æ•°æ®å­˜å‚¨
chat_history = []
documents = []
document_embeddings = {}

class EnhancedDocumentProcessor:
    """å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.stop_words = {
            'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'å—', 'å‘¢', 
            'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å¸®', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ',
            'the', 'is', 'at', 'which', 'on', 'and', 'or', 'but', 'a', 'an'
        }
    
    def extract_document_structure(self, content: str, filename: str) -> Dict[str, Any]:
        """æå–æ–‡æ¡£ç»“æ„å’Œå…³é”®ä¿¡æ¯"""
        lines = content.split('\n')
        
        structure = {
            'filename': filename,
            'content': content,
            'headers': [],
            'sections': {},
            'key_points': [],
            'features': [],
            'statistics': {},
            'summary': ''
        }
        
        current_section = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æå–æ ‡é¢˜
            if line.startswith('#'):
                if current_section and current_content:
                    structure['sections'][current_section] = '\n'.join(current_content)
                
                header_level = len(line) - len(line.lstrip('#'))
                header_text = line.lstrip('#').strip()
                structure['headers'].append({
                    'level': header_level,
                    'text': header_text,
                    'line': line
                })
                current_section = header_text
                current_content = []
            
            # æå–åˆ—è¡¨é¡¹å’Œç‰¹æ€§
            elif line.startswith(('- ', '* ', '+ ', 'â”œâ”€â”€', 'â””â”€â”€')):
                feature = line.lstrip('- *+â”œâ”€â”€â””â”€â”€ ').strip()
                if len(feature) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„é¡¹ç›®
                    structure['features'].append(feature)
                current_content.append(line)
            
            # æå–å…³é”®ç‚¹ï¼ˆåŒ…å«ç‰¹æ®Šç¬¦å·çš„è¡Œï¼‰
            elif any(symbol in line for symbol in ['âœ…', 'ğŸ¯', 'â­', 'ğŸš€', 'ğŸ’¡', 'ğŸ“Š', 'ğŸ”§']):
                structure['key_points'].append(line)
                current_content.append(line)
            
            else:
                current_content.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªsection
        if current_section and current_content:
            structure['sections'][current_section] = '\n'.join(current_content)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        structure['statistics'] = {
            'total_lines': len(lines),
            'headers_count': len(structure['headers']),
            'sections_count': len(structure['sections']),
            'features_count': len(structure['features']),
            'key_points_count': len(structure['key_points']),
            'character_count': len(content),
            'word_count': len(content.split())
        }
        
        # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
        structure['summary'] = self.generate_smart_summary(structure)
        
        return structure
    
    def generate_smart_summary(self, structure: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦"""
        filename = structure['filename']
        headers = structure['headers']
        features = structure['features']
        key_points = structure['key_points']
        stats = structure['statistics']
        
        summary = f"**ğŸ“„ {filename}**\n"
        
        # æ·»åŠ å¾½ç« å’Œæè¿°
        content = structure['content']
        if 'badge' in content.lower() or '[![' in content:
            badges = re.findall(r'\[!\[.*?\]\(.*?\)\]\(.*?\)', content)
            if badges:
                summary += f"å¾½ç« ä¿¡æ¯: {len(badges)} ä¸ªé¡¹ç›®å¾½ç« \n"
        
        # è¯†åˆ«é¡¹ç›®ç±»å‹
        project_type = self.identify_project_type(content)
        if project_type:
            summary += f"é¡¹ç›®ç±»å‹: {project_type}\n"
        
        # ä¸»è¦ç« èŠ‚
        if headers:
            summary += "ä¸»è¦ç« èŠ‚:\n"
            for header in headers[:8]:  # æœ€å¤šæ˜¾ç¤º8ä¸ªæ ‡é¢˜
                level_indicator = "  " * (header['level'] - 1)
                summary += f"{level_indicator}â€¢ {header['text']}\n"
        
        # æ ¸å¿ƒç‰¹æ€§
        if features:
            summary += "\næ ¸å¿ƒç‰¹æ€§:\n"
            for feature in features[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªç‰¹æ€§
                # æ¸…ç†ç‰¹æ€§æ–‡æœ¬
                clean_feature = re.sub(r'[ğŸ”ºğŸŒŒğŸ›¡ï¸ğŸ¯ğŸŒŸğŸ“¥ğŸ”ğŸ§ âš¡ğŸ”§ğŸµğŸ“šğŸ„ğŸ§¬ğŸ”¬ğŸš€]', '', feature).strip()
                if len(clean_feature) > 10:
                    summary += f"  â€¢ {clean_feature}\n"
        
        # å…³é”®äº®ç‚¹
        if key_points:
            summary += "\nå…³é”®äº®ç‚¹:\n"
            for point in key_points[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªå…³é”®ç‚¹
                clean_point = point.strip()
                if len(clean_point) > 10:
                    summary += f"  â€¢ {clean_point}\n"
        
        # æŠ€æœ¯æ ˆè¯†åˆ«
        tech_stack = self.identify_tech_stack(content)
        if tech_stack:
            summary += f"\næŠ€æœ¯æ ˆ: {', '.join(tech_stack)}\n"
        
        # ç»Ÿè®¡ä¿¡æ¯
        summary += f"\nğŸ“Š æ–‡æ¡£ç»Ÿè®¡: {stats['character_count']} ä¸ªå­—ç¬¦ï¼Œ{stats['word_count']} ä¸ªè¯ï¼Œ{stats['headers_count']} ä¸ªç« èŠ‚"
        
        return summary
    
    def identify_project_type(self, content: str) -> str:
        """è¯†åˆ«é¡¹ç›®ç±»å‹"""
        content_lower = content.lower()
        
        if 'rag' in content_lower and ('ai' in content_lower or 'æ™ºèƒ½' in content):
            return "AIæ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
        elif 'genesis' in content_lower and 'brain' in content_lower:
            return "AIå¤§è„‘ç³»ç»Ÿ"
        elif 'api' in content_lower and 'server' in content_lower:
            return "APIæœåŠ¡ç³»ç»Ÿ"
        elif 'web' in content_lower and ('frontend' in content_lower or 'å‰ç«¯' in content):
            return "Webå‰ç«¯åº”ç”¨"
        elif 'database' in content_lower or 'æ•°æ®åº“' in content:
            return "æ•°æ®åº“ç³»ç»Ÿ"
        elif 'machine learning' in content_lower or 'ml' in content_lower:
            return "æœºå™¨å­¦ä¹ é¡¹ç›®"
        elif 'research' in content_lower or 'ç§‘ç ”' in content:
            return "ç§‘ç ”å¹³å°"
        else:
            return "ç»¼åˆç³»ç»Ÿå¹³å°"
    
    def identify_tech_stack(self, content: str) -> List[str]:
        """è¯†åˆ«æŠ€æœ¯æ ˆ"""
        tech_stack = []
        content_lower = content.lower()
        
        # ç¼–ç¨‹è¯­è¨€
        if 'python' in content_lower:
            tech_stack.append('Python')
        if 'javascript' in content_lower or 'node' in content_lower:
            tech_stack.append('JavaScript/Node.js')
        if 'typescript' in content_lower:
            tech_stack.append('TypeScript')
        
        # æ¡†æ¶
        if 'flask' in content_lower:
            tech_stack.append('Flask')
        if 'streamlit' in content_lower:
            tech_stack.append('Streamlit')
        if 'vite' in content_lower:
            tech_stack.append('Vite')
        if 'react' in content_lower:
            tech_stack.append('React')
        
        # AI/ML
        if any(term in content_lower for term in ['openai', 'gpt', 'llm', 'ai']):
            tech_stack.append('AI/LLM')
        if 'vector' in content_lower or 'embedding' in content_lower:
            tech_stack.append('å‘é‡æ•°æ®åº“')
        
        # æ•°æ®åº“
        if 'sqlite' in content_lower:
            tech_stack.append('SQLite')
        if 'postgresql' in content_lower:
            tech_stack.append('PostgreSQL')
        
        # éƒ¨ç½²
        if 'docker' in content_lower:
            tech_stack.append('Docker')
        if 'cloudflare' in content_lower:
            tech_stack.append('Cloudflare')
        
        return tech_stack

class EnhancedRAGEngine:
    """å¢å¼ºç‰ˆRAGå¼•æ“"""
    
    def __init__(self):
        self.doc_processor = EnhancedDocumentProcessor()
    
    def search_documents(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æ–‡æ¡£æœç´¢"""
        if not documents:
            return []
        
        query_lower = query.lower()
        results = []
        
        # æå–æŸ¥è¯¢å…³é”®è¯
        query_keywords = self.extract_keywords(query)
        logger.info(f"æŸ¥è¯¢å…³é”®è¯: {query_keywords}")
        
        for doc in documents:
            score = self.calculate_relevance_score(doc, query_keywords, query_lower)
            if score > 0:
                # æå–ç›¸å…³å†…å®¹
                relevant_content = self.extract_relevant_content(doc, query_keywords, query_lower)
                results.append({
                    'document': doc,
                    'score': score,
                    'relevant_content': relevant_content
                })
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:max_results]
    
    def extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯"""
        # ä¸­æ–‡åˆ†è¯ï¼ˆç®€å•ç‰ˆï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff]+', query)
        english_words = re.findall(r'[a-zA-Z]+', query.lower())
        
        keywords = []
        
        # å¤„ç†ä¸­æ–‡
        for chinese in chinese_chars:
            if len(chinese) >= 2:
                # æå–2-4å­—çš„è¯ç»„
                for i in range(len(chinese)):
                    for length in [4, 3, 2]:
                        if i + length <= len(chinese):
                            word = chinese[i:i+length]
                            if word not in self.doc_processor.stop_words:
                                keywords.append(word)
        
        # å¤„ç†è‹±æ–‡
        keywords.extend([word for word in english_words if word not in self.doc_processor.stop_words and len(word) > 2])
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_keywords = []
        for keyword in keywords:
            if keyword not in seen:
                seen.add(keyword)
                unique_keywords.append(keyword)
        
        return unique_keywords[:10]  # æœ€å¤š10ä¸ªå…³é”®è¯
    
    def calculate_relevance_score(self, doc: Dict[str, Any], keywords: List[str], query_lower: str) -> float:
        """è®¡ç®—æ–‡æ¡£ç›¸å…³æ€§åˆ†æ•°"""
        content = doc.get('content', '').lower()
        structure = doc.get('structure', {})
        
        score = 0.0
        
        # å…³é”®è¯åŒ¹é…
        for keyword in keywords:
            keyword_lower = keyword.lower()
            
            # æ ‡é¢˜åŒ¹é…ï¼ˆé«˜æƒé‡ï¼‰
            for header in structure.get('headers', []):
                if keyword_lower in header.get('text', '').lower():
                    score += 5.0
            
            # ç‰¹æ€§åŒ¹é…ï¼ˆä¸­æƒé‡ï¼‰
            for feature in structure.get('features', []):
                if keyword_lower in feature.lower():
                    score += 3.0
            
            # å…³é”®ç‚¹åŒ¹é…ï¼ˆä¸­æƒé‡ï¼‰
            for point in structure.get('key_points', []):
                if keyword_lower in point.lower():
                    score += 3.0
            
            # å†…å®¹åŒ¹é…ï¼ˆåŸºç¡€æƒé‡ï¼‰
            content_matches = content.count(keyword_lower)
            score += content_matches * 1.0
        
        # æŸ¥è¯¢çŸ­è¯­å®Œæ•´åŒ¹é…ï¼ˆé«˜æƒé‡ï¼‰
        if query_lower in content:
            score += 10.0
        
        return score
    
    def extract_relevant_content(self, doc: Dict[str, Any], keywords: List[str], query_lower: str) -> str:
        """æå–ç›¸å…³å†…å®¹"""
        structure = doc.get('structure', {})
        content = doc.get('content', '')
        
        relevant_parts = []
        
        # æ·»åŠ åŒ¹é…çš„æ ‡é¢˜
        matching_headers = []
        for header in structure.get('headers', []):
            header_text = header.get('text', '').lower()
            if any(keyword.lower() in header_text for keyword in keywords) or query_lower in header_text:
                matching_headers.append(header.get('line', ''))
        
        if matching_headers:
            relevant_parts.append("ç›¸å…³ç« èŠ‚:")
            relevant_parts.extend(matching_headers[:3])
        
        # æ·»åŠ åŒ¹é…çš„ç‰¹æ€§
        matching_features = []
        for feature in structure.get('features', []):
            feature_lower = feature.lower()
            if any(keyword.lower() in feature_lower for keyword in keywords) or query_lower in feature_lower:
                matching_features.append(f"â€¢ {feature}")
        
        if matching_features:
            relevant_parts.append("\næ ¸å¿ƒç‰¹æ€§:")
            relevant_parts.extend(matching_features[:5])
        
        # æ·»åŠ åŒ¹é…çš„å…³é”®ç‚¹
        matching_points = []
        for point in structure.get('key_points', []):
            point_lower = point.lower()
            if any(keyword.lower() in point_lower for keyword in keywords) or query_lower in point_lower:
                matching_points.append(f"â€¢ {point}")
        
        if matching_points:
            relevant_parts.append("\nå…³é”®äº®ç‚¹:")
            relevant_parts.extend(matching_points[:3])
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šåŒ¹é…ï¼Œè¿”å›æ‘˜è¦
        if not relevant_parts:
            return structure.get('summary', content[:500] + '...')
        
        return '\n'.join(relevant_parts)
    
    def generate_intelligent_response(self, message: str, search_results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæ™ºèƒ½å›ç­”"""
        message_lower = message.lower()
        
        # å¦‚æœæ²¡æœ‰æœç´¢ç»“æœ
        if not search_results:
            if any(keyword in message_lower for keyword in ['æ€»ç»“', 'ä»‹ç»', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'åŠŸèƒ½', 'ç‰¹æ€§']):
                return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚è¯·å…ˆä¸Šä¼ æ–‡æ¡£ï¼Œç„¶åæˆ‘å°±å¯ä»¥å¸®æ‚¨åˆ†æå’Œå›ç­”ç›¸å…³é—®é¢˜äº†ã€‚"
            else:
                return "æ‚¨å¥½ï¼æˆ‘æ˜¯NEXUS AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼è¯·ä¸Šä¼ æ–‡æ¡£åï¼Œæˆ‘å°±å¯ä»¥å¸®æ‚¨åˆ†æå’Œå›ç­”ç›¸å…³é—®é¢˜ã€‚"
        
        # åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­”
        response_parts = []
        
        # é—®é¢˜ç±»å‹è¯†åˆ«
        if 'æ€»ç»“' in message_lower or 'ä»‹ç»' in message_lower:
            response_parts.append("åŸºäºæ‚¨ä¸Šä¼ çš„æ–‡æ¡£ï¼Œæˆ‘ä¸ºæ‚¨æ€»ç»“å¦‚ä¸‹ï¼š\n")
            
            for i, result in enumerate(search_results, 1):
                doc = result['document']
                structure = doc.get('structure', {})
                response_parts.append(f"**{i}. {structure.get('filename', 'æ–‡æ¡£')}**")
                response_parts.append(structure.get('summary', ''))
                response_parts.append("")
        
        elif any(keyword in message_lower for keyword in ['åŠŸèƒ½', 'ç‰¹æ€§', 'èƒ½åŠ›', 'ç‰¹ç‚¹']):
            response_parts.append("æ ¹æ®æ–‡æ¡£åˆ†æï¼Œä¸»è¦åŠŸèƒ½ç‰¹æ€§åŒ…æ‹¬ï¼š\n")
            
            all_features = []
            for result in search_results:
                structure = result['document'].get('structure', {})
                all_features.extend(structure.get('features', []))
            
            # å»é‡å¹¶é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹æ€§
            unique_features = list(dict.fromkeys(all_features))
            for feature in unique_features[:10]:
                clean_feature = re.sub(r'[ğŸ”ºğŸŒŒğŸ›¡ï¸ğŸ¯ğŸŒŸğŸ“¥ğŸ”ğŸ§ âš¡ğŸ”§ğŸµğŸ“šğŸ„ğŸ§¬ğŸ”¬ğŸš€]', '', feature).strip()
                if len(clean_feature) > 5:
                    response_parts.append(f"â€¢ {clean_feature}")
        
        elif any(keyword in message_lower for keyword in ['å¦‚ä½•', 'æ€ä¹ˆ', 'ä½¿ç”¨', 'æ“ä½œ']):
            response_parts.append("æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n")
            
            for result in search_results:
                relevant_content = result['relevant_content']
                if relevant_content:
                    response_parts.append(relevant_content)
                    response_parts.append("")
        
        else:
            # é€šç”¨é—®ç­”
            response_parts.append("æ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n")
            
            for i, result in enumerate(search_results, 1):
                response_parts.append(f"**{i}. ç›¸å…³å†…å®¹ï¼š**")
                response_parts.append(result['relevant_content'])
                response_parts.append("")
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if search_results:
            total_docs = len(documents)
            matched_docs = len(search_results)
            response_parts.append(f"\nğŸ“Š æœç´¢ç»Ÿè®¡ï¼šåœ¨ {total_docs} ä¸ªæ–‡æ¡£ä¸­æ‰¾åˆ° {matched_docs} ä¸ªç›¸å…³æ–‡æ¡£")
        
        return '\n'.join(response_parts)

# åˆå§‹åŒ–å¢å¼ºç‰ˆRAGå¼•æ“
rag_engine = EnhancedRAGEngine()

def get_current_time():
    """è·å–å½“å‰æ—¶é—´ï¼ˆä¸­å›½æ—¶åŒºï¼‰"""
    return datetime.now(TIMEZONE)

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    current_time = get_current_time()
    
    return jsonify({
        "status": "healthy",
        "message": "å¢å¼ºç‰ˆæ™ºèƒ½RAGä»£ç†æœåŠ¡å™¨è¿è¡Œæ­£å¸¸",
        "ai_system": "å¢å¼ºç‰ˆæœ¬åœ°æ™ºèƒ½å“åº”ç³»ç»Ÿ",
        "version": "2.0.0-Enhanced",
        "timestamp": current_time.isoformat(),
        "data": {
            "chat_history_count": len(chat_history),
            "documents_count": len(documents),
            "system_status": "è¿è¡Œæ­£å¸¸",
            "timezone": "Asia/Shanghai",
            "uptime": "ç³»ç»Ÿè¿è¡Œä¸­",
            "features": [
                "æ™ºèƒ½æ–‡æ¡£ç»“æ„åˆ†æ",
                "å¢å¼ºç‰ˆå…³é”®è¯æå–",
                "ä¸Šä¸‹æ–‡ç›¸å…³æ€§è®¡ç®—",
                "æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ",
                "æŠ€æœ¯æ ˆè¯†åˆ«"
            ]
        }
    })

@app.route('/api/system/status', methods=['GET'])
def system_status():
    """ç³»ç»ŸçŠ¶æ€æ£€æŸ¥ - ä¸ºå‰ç«¯æä¾›è¯¦ç»†çŠ¶æ€ä¿¡æ¯"""
    current_time = get_current_time()
    
    return jsonify({
        "status": "active",
        "message": "ç³»ç»Ÿè¿è¡Œæ­£å¸¸",
        "timestamp": current_time.isoformat(),
        "data": {
            "chat_history_count": len(chat_history),
            "documents_count": len(documents),
            "system_health": "healthy",
            "uptime": "è¿è¡Œä¸­",
            "version": "2.0.0-Enhanced",
            "features_active": len([
                "æ™ºèƒ½æ–‡æ¡£ç»“æ„åˆ†æ",
                "å¢å¼ºç‰ˆå…³é”®è¯æå–", 
                "ä¸Šä¸‹æ–‡ç›¸å…³æ€§è®¡ç®—",
                "æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ",
                "æŠ€æœ¯æ ˆè¯†åˆ«"
            ]),
            "api_endpoints": {
                "health": "/api/health",
                "upload": "/api/upload", 
                "chat": "/api/chat",
                "documents": "/api/documents",
                "chat_history": "/api/chat/history",
                "system_status": "/api/system/status"
            }
        }
    })

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """æ–‡ä»¶ä¸Šä¼ """
    try:
        if 'file' not in request.files:
            return jsonify({"success": False, "message": "æ²¡æœ‰æ–‡ä»¶"}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({"success": False, "message": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400
        
        if file:
            filename = file.filename
            filepath = UPLOAD_FOLDER / filename
            file.save(filepath)
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            content = ""
            file_extension = filename.lower().split('.')[-1]
            
            if file_extension == 'docx':
                # å¤„ç†.docxæ–‡ä»¶
                try:
                    doc = Document(filepath)
                    paragraphs = []
                    for paragraph in doc.paragraphs:
                        if paragraph.text.strip():
                            paragraphs.append(paragraph.text.strip())
                    content = '\n'.join(paragraphs)
                    logger.info(f"æˆåŠŸè§£æ.docxæ–‡ä»¶: {filename}, æ®µè½æ•°: {len(paragraphs)}")
                except Exception as e:
                    logger.error(f"è§£æ.docxæ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
                    return jsonify({
                        "success": False, 
                        "message": f"è§£æ.docxæ–‡ä»¶å¤±è´¥: {str(e)}"
                    }), 500
            else:
                # å¤„ç†æ–‡æœ¬æ–‡ä»¶
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    try:
                        with open(filepath, 'r', encoding='gbk') as f:
                            content = f.read()
                    except Exception as e:
                        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
                        return jsonify({
                            "success": False, 
                            "message": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"
                        }), 500
            
            # ä½¿ç”¨å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†å™¨
            structure = rag_engine.doc_processor.extract_document_structure(content, filename)
            
            # å­˜å‚¨æ–‡æ¡£
            doc_id = hashlib.md5(f"{filename}{content}".encode()).hexdigest()
            document = {
                "id": doc_id,
                "filename": filename,
                "content": content,
                "structure": structure,
                "upload_time": get_current_time().isoformat(),
                "file_size": len(content)
            }
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_doc = next((doc for doc in documents if doc['id'] == doc_id), None)
            if existing_doc:
                return jsonify({
                    "success": True,
                    "message": f"æ–‡æ¡£ {filename} å·²å­˜åœ¨",
                    "document_info": {
                        "filename": filename,
                        "size": len(content),
                        "structure_info": structure['statistics']
                    }
                })
            
            documents.append(document)
            
            logger.info(f"æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {filename}, å¤§å°: {len(content)} å­—ç¬¦")
            
            return jsonify({
                "success": True,
                "message": f"æ–‡æ¡£ {filename} ä¸Šä¼ æˆåŠŸ",
                "document_info": {
                    "filename": filename,
                    "size": len(content),
                    "structure_info": structure['statistics'],
                    "summary": structure['summary'][:200] + "..." if len(structure['summary']) > 200 else structure['summary']
                }
            })
            
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ é”™è¯¯: {e}")
        return jsonify({"success": False, "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}), 500

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤©æ¥å£"""
    try:
        data = request.get_json()
        message = data.get('message', '')
        conversation_id = data.get('conversation_id', 'default')
        
        if not message:
            return jsonify({"success": False, "message": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"}), 400
        
        current_time = get_current_time()
        
        # ä½¿ç”¨å¢å¼ºç‰ˆRAGå¼•æ“æœç´¢å’Œç”Ÿæˆå›ç­”
        search_results = rag_engine.search_documents(message)
        response = rag_engine.generate_intelligent_response(message, search_results)
        
        # è®°å½•èŠå¤©å†å²
        chat_record = {
            "id": len(chat_history) + 1,
            "conversation_id": conversation_id,
            "message": message,
            "response": response,
            "timestamp": current_time.isoformat(),
            "search_results_count": len(search_results)
        }
        
        chat_history.append(chat_record)
        
        logger.info(f"èŠå¤©è®°å½•: {message[:50]}... -> {len(search_results)} ä¸ªæœç´¢ç»“æœ")
        
        return jsonify({
            "success": True,
            "status": "success",
            "chat_id": chat_record["id"],
            "response": response,
            "timestamp": current_time.isoformat(),
            "search_info": {
                "documents_searched": len(documents),
                "results_found": len(search_results)
            }
        })
        
    except Exception as e:
        logger.error(f"èŠå¤©å¤„ç†é”™è¯¯: {e}")
        return jsonify({"success": False, "message": f"å¤„ç†å¤±è´¥: {str(e)}"}), 500

@app.route('/api/documents', methods=['GET'])
def get_documents():
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    doc_list = []
    for doc in documents:
        structure = doc.get('structure', {})
        doc_list.append({
            "id": doc['id'],
            "filename": doc['filename'],
            "upload_time": doc['upload_time'],
            "file_size": doc['file_size'],
            "statistics": structure.get('statistics', {}),
            "summary": structure.get('summary', '')[:100] + "..." if len(structure.get('summary', '')) > 100 else structure.get('summary', '')
        })
    
    return jsonify({
        "success": True,
        "documents": doc_list,
        "total_count": len(documents)
    })

@app.route('/api/chat/history', methods=['GET'])
def get_chat_history():
    """è·å–èŠå¤©å†å²"""
    conversation_id = request.args.get('conversation_id', 'default')
    
    filtered_history = [
        chat for chat in chat_history 
        if chat.get('conversation_id') == conversation_id
    ]
    
    return jsonify({
        "success": True,
        "chat_history": filtered_history,
        "total_count": len(filtered_history)
    })

@app.route('/api/clear/test', methods=['POST'])
def clear_test_data():
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    global documents, chat_history
    
    try:
        # å®šä¹‰æµ‹è¯•å…³é”®è¯
        test_keywords = ['test', 'demo', 'sample', 'æµ‹è¯•', 'ç¤ºä¾‹', 'æ ·æœ¬']
        
        # æ‰¾åˆ°éœ€è¦åˆ é™¤çš„æ–‡æ¡£
        deleted_documents = []
        remaining_documents = []
        
        for doc in documents:
            filename = doc.get('structure', {}).get('filename', '').lower()
            is_test_doc = any(keyword in filename for keyword in test_keywords)
            
            if is_test_doc:
                deleted_documents.append({
                    'filename': doc.get('structure', {}).get('filename', ''),
                    'id': doc.get('id', ''),
                    'upload_time': doc.get('upload_time', '')
                })
            else:
                remaining_documents.append(doc)
        
        # æ¸…ç†æµ‹è¯•æ–‡æ¡£
        documents = remaining_documents
        
        # æ¸…ç†ç›¸å…³çš„èŠå¤©å†å²ï¼ˆç®€å•å®ç°ï¼šæ¸…ç†æ‰€æœ‰å†å²ï¼‰
        deleted_chats = len(chat_history)
        chat_history = []
        
        return jsonify({
            "success": True,
            "deleted_documents": deleted_documents,
            "deleted_chats": deleted_chats,
            "remaining_documents": len(remaining_documents),
            "timestamp": datetime.now().isoformat(),
            "message": f"æˆåŠŸæ¸…ç† {len(deleted_documents)} ä¸ªæµ‹è¯•æ–‡æ¡£å’Œ {deleted_chats} æ¡èŠå¤©è®°å½•"
        })
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e),
            "message": "æ¸…ç†æµ‹è¯•æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯"
        }), 500

@app.route('/api/chat/clear', methods=['POST'])
def clear_chat_history():
    """æ¸…ç©ºèŠå¤©è®°å½•"""
    global chat_history
    
    try:
        cleared_count = len(chat_history)
        chat_history = []
        
        return jsonify({
            "success": True,
            "cleared_count": cleared_count,
            "timestamp": datetime.now().isoformat(),
            "message": f"æˆåŠŸæ¸…ç©º {cleared_count} æ¡èŠå¤©è®°å½•"
        })
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e),
            "message": "æ¸…ç©ºèŠå¤©è®°å½•æ—¶å‘ç”Ÿé”™è¯¯"
        }), 500

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆæ™ºèƒ½RAGæœåŠ¡å™¨')
    parser.add_argument('--port', type=int, default=8502, help='æœåŠ¡å™¨ç«¯å£')
    parser.add_argument('--host', default='0.0.0.0', help='æœåŠ¡å™¨ä¸»æœº')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    print(f"ğŸ§  å¢å¼ºç‰ˆæ™ºèƒ½RAGç³»ç»Ÿå¯åŠ¨ä¸­...")
    print(f"ğŸ“¡ å¥åº·æ£€æŸ¥: http://{args.host}:{args.port}/api/health")
    print(f"ğŸ’¬ èŠå¤©æ¥å£: http://{args.host}:{args.port}/api/chat")
    print(f"ğŸ“¤ ä¸Šä¼ æ¥å£: http://{args.host}:{args.port}/api/upload")
    print(f"ğŸ“š æ–‡æ¡£åˆ—è¡¨: http://{args.host}:{args.port}/api/documents")
    
    app.run(host=args.host, port=args.port, debug=args.debug)