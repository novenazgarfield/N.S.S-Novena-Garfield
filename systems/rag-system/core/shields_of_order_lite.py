"""
ğŸ›¡ï¸ ç§©åºä¹‹ç›¾ - è½»é‡ç‰ˆ (Shields of Order Lite)
=============================================

å®ç°"å¤§å®ªç« "ç¬¬ä¸‰ç« ï¼šçŸ¥è¯†çš„"å®ˆæŠ¤"
- äºŒçº§ç²¾ç‚¼æœºåˆ¶ (åŸºäºè§„åˆ™çš„é‡æ’åº)
- æ˜Ÿå›¾å¯¼èˆªç­–ç•¥ (åˆ†å±‚æ£€ç´¢)
- é˜²è…çƒ‚æœºåˆ¶ (è´¨é‡ä¿éšœ)

ä¸ä¾èµ–sentence-transformersï¼Œé¿å…ç‰ˆæœ¬å†²çª
Author: N.S.S-Novena-Garfield Project
Version: 2.0.0 - "Genesis" Chapter 3 Lite
"""

import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import hashlib
import sqlite3
from pathlib import Path
import json
import re
import math
from collections import Counter

from utils.logger import logger
from config import StorageConfig

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    content: str
    metadata: Dict[str, Any]
    initial_score: float
    rerank_score: Optional[float] = None
    retrieval_level: str = "atom"  # atom, paragraph, document
    source_document: str = ""
    relevance_explanation: str = ""
    
    def get_final_score(self) -> float:
        """è·å–æœ€ç»ˆè¯„åˆ†"""
        return self.rerank_score if self.rerank_score is not None else self.initial_score

@dataclass
class QueryComplexity:
    """æŸ¥è¯¢å¤æ‚åº¦åˆ†æ"""
    complexity_level: str  # simple, medium, complex
    requires_hierarchical: bool
    suggested_strategy: str
    confidence: float
    analysis_details: Dict[str, Any]

class LiteReRanker:
    """è½»é‡çº§é‡æ’åºå™¨ - åŸºäºè§„åˆ™çš„äºŒçº§ç²¾ç‚¼æœºåˆ¶"""
    
    def __init__(self):
        # ä¸­æ–‡åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›', 'å—', 'å‘¢', 'å§', 'å•Š'
        }
        
        # é‡è¦è¯æ±‡æƒé‡
        self.important_words = {
            'äººå·¥æ™ºèƒ½': 2.0, 'AI': 2.0, 'æœºå™¨å­¦ä¹ ': 2.0, 'æ·±åº¦å­¦ä¹ ': 2.0,
            'ç¥ç»ç½‘ç»œ': 1.8, 'ç®—æ³•': 1.5, 'æ¨¡å‹': 1.5, 'æ•°æ®': 1.3,
            'æŠ€æœ¯': 1.2, 'ç³»ç»Ÿ': 1.2, 'æ–¹æ³•': 1.2, 'åº”ç”¨': 1.2
        }
        
        logger.info("ğŸ”„ è½»é‡çº§é‡æ’åºå™¨å·²åˆå§‹åŒ–")
    
    def rerank_results(self, query: str, results: List[RetrievalResult], 
                      top_k: int = None) -> List[RetrievalResult]:
        """å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº"""
        try:
            if not results:
                logger.warning("ç»“æœä¸ºç©ºï¼Œè·³è¿‡é‡æ’åº")
                return results
            
            if top_k is None:
                top_k = len(results)
            
            logger.info(f"å¼€å§‹å¯¹ {len(results)} ä¸ªç»“æœè¿›è¡Œè½»é‡çº§é‡æ’åº")
            
            # åˆ†ææŸ¥è¯¢
            query_features = self._analyze_query(query)
            
            # è®¡ç®—é‡æ’åºåˆ†æ•°
            for result in results:
                content_features = self._analyze_content(result.content)
                
                # è®¡ç®—å¤šç»´åº¦ç›¸ä¼¼åº¦
                similarity_scores = self._calculate_similarity(query_features, content_features)
                
                # ç»“åˆåˆå§‹åˆ†æ•°å’Œç›¸ä¼¼åº¦åˆ†æ•°
                final_score = self._combine_scores(result.initial_score, similarity_scores)
                
                result.rerank_score = final_score
                result.relevance_explanation = f"è½»é‡é‡æ’åº: {final_score:.4f} (è¯æ±‡:{similarity_scores['lexical']:.3f}, è¯­ä¹‰:{similarity_scores['semantic']:.3f})"
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            reranked_results = sorted(results, key=lambda x: x.get_final_score(), reverse=True)
            
            logger.info(f"è½»é‡çº§é‡æ’åºå®Œæˆï¼Œè¿”å›å‰ {min(top_k, len(reranked_results))} ä¸ªç»“æœ")
            return reranked_results[:top_k]
            
        except Exception as e:
            logger.error(f"è½»é‡çº§é‡æ’åºå¤±è´¥: {e}")
            return results[:top_k] if top_k else results
    
    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢ç‰¹å¾"""
        words = self._tokenize(query)
        
        return {
            'words': words,
            'important_words': [w for w in words if w in self.important_words],
            'word_count': len(words),
            'char_count': len(query),
            'has_question': any(q in query for q in ['ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å“ªä¸ª', '?', 'ï¼Ÿ']),
            'has_comparison': any(c in query for c in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'åŒºåˆ«', 'å·®å¼‚']),
            'word_weights': {w: self.important_words.get(w, 1.0) for w in words}
        }
    
    def _analyze_content(self, content: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹ç‰¹å¾"""
        words = self._tokenize(content)
        
        return {
            'words': words,
            'important_words': [w for w in words if w in self.important_words],
            'word_count': len(words),
            'char_count': len(content),
            'word_freq': Counter(words),
            'word_weights': {w: self.important_words.get(w, 1.0) for w in words}
        }
    
    def _tokenize(self, text: str) -> List[str]:
        """ç®€å•çš„ä¸­æ–‡åˆ†è¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s]', ' ', text)
        # åˆ†å‰²å•è¯
        words = text.split()
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        words = [w.lower() for w in words if len(w) > 1 and w.lower() not in self.stop_words]
        return words
    
    def _calculate_similarity(self, query_features: Dict, content_features: Dict) -> Dict[str, float]:
        """è®¡ç®—å¤šç»´åº¦ç›¸ä¼¼åº¦"""
        # 1. è¯æ±‡é‡å ç›¸ä¼¼åº¦
        query_words = set(query_features['words'])
        content_words = set(content_features['words'])
        
        if not query_words or not content_words:
            lexical_sim = 0.0
        else:
            intersection = query_words.intersection(content_words)
            union = query_words.union(content_words)
            lexical_sim = len(intersection) / len(union) if union else 0.0
        
        # 2. é‡è¦è¯æ±‡ç›¸ä¼¼åº¦
        query_important = set(query_features['important_words'])
        content_important = set(content_features['important_words'])
        
        if query_important and content_important:
            important_intersection = query_important.intersection(content_important)
            important_sim = len(important_intersection) / len(query_important)
        else:
            important_sim = 0.0
        
        # 3. åŠ æƒè¯æ±‡ç›¸ä¼¼åº¦
        weighted_sim = self._calculate_weighted_similarity(
            query_features['word_weights'],
            content_features['word_weights']
        )
        
        # 4. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…±ç°è¯æ±‡ï¼‰
        semantic_sim = self._calculate_semantic_similarity(query_features, content_features)
        
        return {
            'lexical': lexical_sim,
            'important': important_sim,
            'weighted': weighted_sim,
            'semantic': semantic_sim
        }
    
    def _calculate_weighted_similarity(self, query_weights: Dict, content_weights: Dict) -> float:
        """è®¡ç®—åŠ æƒè¯æ±‡ç›¸ä¼¼åº¦"""
        if not query_weights or not content_weights:
            return 0.0
        
        common_words = set(query_weights.keys()).intersection(set(content_weights.keys()))
        
        if not common_words:
            return 0.0
        
        weighted_score = sum(query_weights[word] * content_weights[word] for word in common_words)
        max_possible_score = sum(query_weights[word] ** 2 for word in query_weights.keys())
        
        return weighted_score / max_possible_score if max_possible_score > 0 else 0.0
    
    def _calculate_semantic_similarity(self, query_features: Dict, content_features: Dict) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºç®€å•çš„å…±ç°åˆ†æï¼‰"""
        query_words = query_features['words']
        content_words = content_features['words']
        
        if not query_words or not content_words:
            return 0.0
        
        # è®¡ç®—è¯æ±‡å¯†åº¦
        query_density = len(set(query_words)) / len(query_words)
        content_density = len(set(content_words)) / len(content_words)
        
        # è®¡ç®—é•¿åº¦ç›¸ä¼¼åº¦
        length_ratio = min(len(query_words), len(content_words)) / max(len(query_words), len(content_words))
        
        # ç»“åˆå¯†åº¦å’Œé•¿åº¦
        semantic_score = (query_density + content_density) / 2 * length_ratio
        
        return min(semantic_score, 1.0)
    
    def _combine_scores(self, initial_score: float, similarity_scores: Dict[str, float]) -> float:
        """ç»“åˆåˆå§‹åˆ†æ•°å’Œç›¸ä¼¼åº¦åˆ†æ•°"""
        # æƒé‡é…ç½®
        weights = {
            'initial': 0.4,
            'lexical': 0.25,
            'important': 0.2,
            'weighted': 0.1,
            'semantic': 0.05
        }
        
        combined_score = (
            weights['initial'] * initial_score +
            weights['lexical'] * similarity_scores['lexical'] +
            weights['important'] * similarity_scores['important'] +
            weights['weighted'] * similarity_scores['weighted'] +
            weights['semantic'] * similarity_scores['semantic']
        )
        
        return min(combined_score, 1.0)

class QueryComplexityAnalyzer:
    """æŸ¥è¯¢å¤æ‚åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.complexity_patterns = {
            'simple_indicators': ['ä»€ä¹ˆ', 'æ˜¯', 'æœ‰', 'å—'],
            'medium_indicators': ['å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›'],
            'complex_indicators': ['æ¯”è¾ƒ', 'åˆ†æ', 'è¯„ä¼°', 'ç»¼åˆ', 'å…³ç³»', 'å½±å“', 'ä¼˜ç¼ºç‚¹', 'å·®å¼‚']
        }
        logger.info("ğŸ” æŸ¥è¯¢å¤æ‚åº¦åˆ†æå™¨å·²åˆå§‹åŒ–")
    
    def analyze_query_complexity(self, query: str) -> QueryComplexity:
        """åˆ†ææŸ¥è¯¢å¤æ‚åº¦"""
        try:
            query = query.strip()
            
            # åŸºç¡€ç‰¹å¾åˆ†æ
            word_count = len(query.split())
            char_count = len(query)
            sentence_count = len([s for s in re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', query) if s.strip()])
            
            # å¤æ‚åº¦æŒ‡æ ‡æ£€æµ‹
            simple_count = sum(1 for indicator in self.complexity_patterns['simple_indicators'] if indicator in query)
            medium_count = sum(1 for indicator in self.complexity_patterns['medium_indicators'] if indicator in query)
            complex_count = sum(1 for indicator in self.complexity_patterns['complex_indicators'] if indicator in query)
            
            # è¯­è¨€ç‰¹å¾
            has_punctuation = bool(re.search(r'[ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ]', query))
            has_logical_words = bool(re.search(r'(å’Œ|ä¸|æˆ–|ä½†æ˜¯|ç„¶è€Œ|å› æ­¤)', query))
            has_multiple_concepts = word_count > 8
            
            # å¤æ‚åº¦è¯„åˆ†
            complexity_score = 0
            
            # é•¿åº¦å› å­
            if char_count > 60:
                complexity_score += 3
            elif char_count > 30:
                complexity_score += 2
            elif char_count > 15:
                complexity_score += 1
            
            # è¯æ±‡å¤æ‚åº¦
            if word_count > 12:
                complexity_score += 3
            elif word_count > 6:
                complexity_score += 2
            elif word_count > 3:
                complexity_score += 1
            
            # è¯­è¨€ç‰¹å¾
            complexity_score += complex_count * 2
            complexity_score += medium_count * 1
            complexity_score += sentence_count - 1  # å¤šå¥å­å¢åŠ å¤æ‚åº¦
            
            if has_logical_words:
                complexity_score += 2
            if has_punctuation:
                complexity_score += 1
            if has_multiple_concepts:
                complexity_score += 1
            
            # ç¡®å®šå¤æ‚åº¦çº§åˆ«
            if complexity_score >= 8:
                complexity_level = "complex"
                requires_hierarchical = True
                suggested_strategy = "hierarchical_macro_to_micro"
            elif complexity_score >= 4:
                complexity_level = "medium"
                requires_hierarchical = True
                suggested_strategy = "enhanced_retrieval"
            else:
                complexity_level = "simple"
                requires_hierarchical = False
                suggested_strategy = "direct_retrieval"
            
            # ç½®ä¿¡åº¦è®¡ç®—
            confidence = min(0.95, 0.6 + complexity_score * 0.05)
            
            analysis_details = {
                "word_count": word_count,
                "char_count": char_count,
                "sentence_count": sentence_count,
                "complexity_score": complexity_score,
                "simple_indicators": simple_count,
                "medium_indicators": medium_count,
                "complex_indicators": complex_count,
                "has_punctuation": has_punctuation,
                "has_logical_words": has_logical_words,
                "has_multiple_concepts": has_multiple_concepts
            }
            
            result = QueryComplexity(
                complexity_level=complexity_level,
                requires_hierarchical=requires_hierarchical,
                suggested_strategy=suggested_strategy,
                confidence=confidence,
                analysis_details=analysis_details
            )
            
            logger.info(f"æŸ¥è¯¢å¤æ‚åº¦åˆ†æå®Œæˆ: {complexity_level} (ç½®ä¿¡åº¦: {confidence:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤æ‚åº¦åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return QueryComplexity(
                complexity_level="medium",
                requires_hierarchical=True,
                suggested_strategy="enhanced_retrieval",
                confidence=0.5,
                analysis_details={"error": str(e)}
            )

class DocumentSummarizer:
    """æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨ - è½»é‡ç‰ˆ"""
    
    def __init__(self, llm_manager=None):
        self.llm_manager = llm_manager
        self.summaries_cache = {}
        self._init_cache_storage()
        logger.info("ğŸ“„ è½»é‡çº§æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨å·²åˆå§‹åŒ–")
    
    def _init_cache_storage(self):
        """åˆå§‹åŒ–æ‘˜è¦ç¼“å­˜å­˜å‚¨"""
        try:
            self.cache_db_path = StorageConfig.MODELS_DATA_DIR / "document_summaries_lite.db"
            self.cache_db_path.parent.mkdir(parents=True, exist_ok=True)
            
            conn = sqlite3.connect(str(self.cache_db_path))
            cursor = conn.cursor()
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS document_summaries (
                    document_id TEXT PRIMARY KEY,
                    content_hash TEXT NOT NULL,
                    summary TEXT NOT NULL,
                    summary_type TEXT DEFAULT 'extractive',
                    created_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                )
            """)
            
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_content_hash ON document_summaries (content_hash)")
            
            conn.commit()
            conn.close()
            
            logger.info("è½»é‡çº§æ–‡æ¡£æ‘˜è¦ç¼“å­˜æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ‘˜è¦ç¼“å­˜å¤±è´¥: {e}")
    
    def generate_document_summary(self, document_content: str, document_id: str, 
                                max_length: int = 200) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        try:
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(document_content.encode()).hexdigest()
            
            # æ£€æŸ¥ç¼“å­˜
            cached_summary = self._get_cached_summary(document_id, content_hash)
            if cached_summary:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ–‡æ¡£æ‘˜è¦: {document_id}")
                return cached_summary
            
            # ç”Ÿæˆæ–°æ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨æŠ½å–å¼ï¼‰
            summary = self._generate_extractive_summary(document_content, max_length)
            
            # ç¼“å­˜æ‘˜è¦
            self._cache_summary(document_id, content_hash, summary)
            
            logger.info(f"æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå®Œæˆ: {document_id}")
            return summary
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æ¡£æ‘˜è¦å¤±è´¥: {e}")
            return self._generate_fallback_summary(document_content, max_length)
    
    def _generate_extractive_summary(self, content: str, max_length: int) -> str:
        """ç”ŸæˆæŠ½å–å¼æ‘˜è¦"""
        try:
            # åˆ†å¥
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
            sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]
            
            if not sentences:
                return content[:max_length] + "..." if len(content) > max_length else content
            
            # è®¡ç®—å¥å­é‡è¦æ€§
            sentence_scores = []
            for i, sentence in enumerate(sentences):
                score = self._calculate_sentence_importance(sentence, content)
                sentence_scores.append((score, i, sentence))
            
            # æŒ‰é‡è¦æ€§æ’åº
            sentence_scores.sort(reverse=True)
            
            # é€‰æ‹©æœ€é‡è¦çš„å¥å­
            summary = ""
            selected_sentences = []
            
            for score, idx, sentence in sentence_scores:
                if len(summary + sentence) <= max_length:
                    selected_sentences.append((idx, sentence))
                    summary += sentence + "ã€‚"
                else:
                    break
            
            # æŒ‰åŸæ–‡é¡ºåºæ’åˆ—
            selected_sentences.sort(key=lambda x: x[0])
            final_summary = "".join([s[1] + "ã€‚" for s in selected_sentences])
            
            return final_summary.strip() if final_summary else content[:max_length] + "..."
            
        except Exception as e:
            logger.error(f"æŠ½å–å¼æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return content[:max_length] + "..." if len(content) > max_length else content
    
    def _calculate_sentence_importance(self, sentence: str, full_content: str) -> float:
        """è®¡ç®—å¥å­é‡è¦æ€§"""
        try:
            score = 0.0
            
            # é•¿åº¦å› å­ï¼ˆä¸­ç­‰é•¿åº¦çš„å¥å­æ›´é‡è¦ï¼‰
            length = len(sentence)
            if 20 <= length <= 100:
                score += 1.0
            elif 10 <= length <= 150:
                score += 0.5
            
            # å…³é”®è¯å› å­
            important_words = ['äººå·¥æ™ºèƒ½', 'AI', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'ç®—æ³•', 'æ¨¡å‹', 'æ•°æ®', 'æŠ€æœ¯', 'ç³»ç»Ÿ', 'æ–¹æ³•', 'åº”ç”¨']
            for word in important_words:
                if word in sentence:
                    score += 2.0
            
            # ä½ç½®å› å­ï¼ˆå¼€å¤´å’Œç»“å°¾çš„å¥å­æ›´é‡è¦ï¼‰
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', full_content)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            if sentence in sentences:
                idx = sentences.index(sentence)
                total = len(sentences)
                if idx < total * 0.2 or idx > total * 0.8:  # å‰20%æˆ–å20%
                    score += 1.0
            
            # æ•°å­—å’Œä¸“ä¸šæœ¯è¯­å› å­
            if re.search(r'\d+', sentence):
                score += 0.5
            
            return score
            
        except Exception as e:
            logger.error(f"è®¡ç®—å¥å­é‡è¦æ€§å¤±è´¥: {e}")
            return 0.0
    
    def _generate_fallback_summary(self, content: str, max_length: int) -> str:
        """ç”Ÿæˆå¤‡ç”¨æ‘˜è¦"""
        return content[:max_length] + "..." if len(content) > max_length else content
    
    def _get_cached_summary(self, document_id: str, content_hash: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„æ‘˜è¦"""
        try:
            conn = sqlite3.connect(str(self.cache_db_path))
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT summary FROM document_summaries 
                WHERE document_id = ? AND content_hash = ?
            """, (document_id, content_hash))
            
            result = cursor.fetchone()
            conn.close()
            
            return result[0] if result else None
            
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜æ‘˜è¦å¤±è´¥: {e}")
            return None
    
    def _cache_summary(self, document_id: str, content_hash: str, summary: str):
        """ç¼“å­˜æ‘˜è¦"""
        try:
            conn = sqlite3.connect(str(self.cache_db_path))
            cursor = conn.cursor()
            
            current_time = datetime.now().isoformat()
            
            cursor.execute("""
                INSERT OR REPLACE INTO document_summaries 
                (document_id, content_hash, summary, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?)
            """, (document_id, content_hash, summary, current_time, current_time))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ç¼“å­˜æ‘˜è¦å¤±è´¥: {e}")

class HierarchicalRetriever:
    """åˆ†å±‚æ£€ç´¢å™¨ - è½»é‡ç‰ˆ"""
    
    def __init__(self, eternal_archive, memory_nebula, document_summarizer):
        self.eternal_archive = eternal_archive
        self.memory_nebula = memory_nebula
        self.document_summarizer = document_summarizer
        logger.info("ğŸ—ºï¸ è½»é‡çº§åˆ†å±‚æ£€ç´¢å™¨å·²åˆå§‹åŒ–")
    
    def hierarchical_retrieve(self, query: str, complexity: QueryComplexity, 
                            top_k: int = 10) -> List[RetrievalResult]:
        """æ‰§è¡Œåˆ†å±‚æ£€ç´¢"""
        try:
            logger.info(f"å¼€å§‹åˆ†å±‚æ£€ç´¢: {complexity.suggested_strategy}")
            
            if complexity.suggested_strategy == "hierarchical_macro_to_micro":
                return self._macro_to_micro_retrieval(query, top_k)
            elif complexity.suggested_strategy == "enhanced_retrieval":
                return self._enhanced_retrieval(query, top_k)
            else:
                return self._direct_retrieval(query, top_k)
                
        except Exception as e:
            logger.error(f"åˆ†å±‚æ£€ç´¢å¤±è´¥: {e}")
            return self._direct_retrieval(query, top_k)
    
    def _macro_to_micro_retrieval(self, query: str, top_k: int) -> List[RetrievalResult]:
        """å®è§‚åˆ°å¾®è§‚çš„åˆ†å±‚æ£€ç´¢"""
        try:
            logger.info("æ‰§è¡Œå®è§‚åˆ°å¾®è§‚æ£€ç´¢ç­–ç•¥")
            
            # ç”±äºç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨å¢å¼ºæ£€ç´¢
            return self._enhanced_retrieval(query, top_k)
            
        except Exception as e:
            logger.error(f"å®è§‚åˆ°å¾®è§‚æ£€ç´¢å¤±è´¥: {e}")
            return self._direct_retrieval(query, top_k)
    
    def _enhanced_retrieval(self, query: str, top_k: int) -> List[RetrievalResult]:
        """å¢å¼ºæ£€ç´¢ç­–ç•¥"""
        try:
            logger.info("æ‰§è¡Œå¢å¼ºæ£€ç´¢ç­–ç•¥")
            
            # å‘é‡æ£€ç´¢
            vector_results = self._direct_retrieval(query, top_k * 2)
            
            # å°è¯•å›¾è°±æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                graph_results = self._retrieve_from_knowledge_graph(query, top_k // 2)
                all_results = vector_results + graph_results
            except Exception as e:
                logger.warning(f"å›¾è°±æ£€ç´¢å¤±è´¥: {e}")
                all_results = vector_results
            
            # å»é‡å’Œæ’åº
            unique_results = self._deduplicate_results(all_results)
            
            logger.info(f"å¢å¼ºæ£€ç´¢å®Œæˆ: {len(unique_results)} ä¸ªç»“æœ")
            return unique_results[:top_k]
            
        except Exception as e:
            logger.error(f"å¢å¼ºæ£€ç´¢å¤±è´¥: {e}")
            return self._direct_retrieval(query, top_k)
    
    def _direct_retrieval(self, query: str, top_k: int) -> List[RetrievalResult]:
        """ç›´æ¥æ£€ç´¢ç­–ç•¥"""
        try:
            logger.info("æ‰§è¡Œç›´æ¥æ£€ç´¢ç­–ç•¥")
            
            # ä½¿ç”¨æ°¸æ’å½’æ¡£è¿›è¡Œå‘é‡æ£€ç´¢
            search_results = self.eternal_archive.search_knowledge_atoms(query, top_k)
            
            results = []
            for result in search_results:
                retrieval_result = RetrievalResult(
                    content=result["content"],
                    metadata=result["metadata"],
                    initial_score=result["similarity_score"],
                    retrieval_level="atom",
                    source_document=result["metadata"].get("document_id", ""),
                    relevance_explanation=f"å‘é‡ç›¸ä¼¼åº¦: {result['similarity_score']:.4f}"
                )
                results.append(retrieval_result)
            
            logger.info(f"ç›´æ¥æ£€ç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"ç›´æ¥æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _retrieve_from_knowledge_graph(self, query: str, top_k: int) -> List[RetrievalResult]:
        """ä»çŸ¥è¯†å›¾è°±æ£€ç´¢"""
        try:
            # æå–æŸ¥è¯¢ä¸­çš„å®ä½“
            entities = self._extract_entities_from_query(query)
            
            results = []
            for entity in entities:
                graph_result = self.memory_nebula.query_related_knowledge(entity, max_depth=2)
                
                if graph_result["success"]:
                    for related in graph_result.get("related_entities", []):
                        retrieval_result = RetrievalResult(
                            content=f"å®ä½“å…³è”: {entity} -> {related['entity']}",
                            metadata={"entity": entity, "related_entity": related["entity"]},
                            initial_score=0.8,
                            retrieval_level="graph",
                            source_document="knowledge_graph",
                            relevance_explanation=f"å›¾è°±å…³è”æ·±åº¦: {related['depth']}"
                        )
                        results.append(retrieval_result)
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _extract_entities_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å®ä½“"""
        # ç®€å•çš„å®ä½“æå–
        words = query.split()
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸'}
        entities = [word for word in words if len(word) > 2 and word not in stop_words]
        return entities[:3]  # é™åˆ¶å®ä½“æ•°é‡
    
    def _deduplicate_results(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """å»é‡æ£€ç´¢ç»“æœ"""
        seen_content = set()
        unique_results = []
        
        for result in results:
            content_hash = hashlib.md5(result.content.encode()).hexdigest()
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        unique_results.sort(key=lambda x: x.get_final_score(), reverse=True)
        return unique_results

class ShieldsOfOrderLite:
    """ç§©åºä¹‹ç›¾è½»é‡ç‰ˆä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, eternal_archive, memory_nebula, llm_manager=None):
        self.eternal_archive = eternal_archive
        self.memory_nebula = memory_nebula
        self.llm_manager = llm_manager
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.lite_reranker = LiteReRanker()
        self.document_summarizer = DocumentSummarizer(llm_manager)
        self.query_complexity_analyzer = QueryComplexityAnalyzer()
        self.hierarchical_retriever = HierarchicalRetriever(
            eternal_archive, memory_nebula, self.document_summarizer
        )
        
        logger.info("ğŸ›¡ï¸ ç§©åºä¹‹ç›¾è½»é‡ç‰ˆå·²æ¿€æ´»")
    
    def protected_retrieve(self, query: str, top_k: int = 10, 
                          enable_reranking: bool = True) -> Dict[str, Any]:
        """å—ä¿æŠ¤çš„æ£€ç´¢ - è½»é‡ç‰ˆç§©åºä¹‹ç›¾æµç¨‹"""
        try:
            start_time = datetime.now()
            logger.info(f"å¼€å§‹è½»é‡ç‰ˆå—ä¿æŠ¤æ£€ç´¢: {query[:50]}...")
            
            # ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢å¤æ‚åº¦åˆ†æ
            complexity = self.query_complexity_analyzer.analyze_query_complexity(query)
            
            # ç¬¬äºŒæ­¥ï¼šåˆ†å±‚æ£€ç´¢ï¼ˆæ˜Ÿå›¾å¯¼èˆªï¼‰
            initial_results = self.hierarchical_retriever.hierarchical_retrieve(
                query, complexity, top_k * 2  # è·å–æ›´å¤šç»“æœç”¨äºé‡æ’åº
            )
            
            if not initial_results:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ",
                    "query": query,
                    "results": [],
                    "complexity_analysis": complexity.__dict__
                }
            
            # ç¬¬ä¸‰æ­¥ï¼šè½»é‡çº§é‡æ’åº
            if enable_reranking and len(initial_results) > 1:
                final_results = self.lite_reranker.rerank_results(
                    query, initial_results, top_k
                )
            else:
                final_results = initial_results[:top_k]
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "success": True,
                "message": f"è½»é‡ç‰ˆç§©åºä¹‹ç›¾æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_results)} ä¸ªé«˜è´¨é‡ç»“æœ",
                "query": query,
                "results": [self._format_result(r) for r in final_results],
                "complexity_analysis": complexity.__dict__,
                "processing_time": processing_time,
                "reranking_enabled": enable_reranking,
                "retrieval_strategy": complexity.suggested_strategy,
                "shields_status": "active_lite"
            }
            
            logger.info(f"è½»é‡ç‰ˆå—ä¿æŠ¤æ£€ç´¢å®Œæˆ: {len(final_results)} ä¸ªç»“æœï¼Œè€—æ—¶ {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"è½»é‡ç‰ˆå—ä¿æŠ¤æ£€ç´¢å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ£€ç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "query": query,
                "results": [],
                "error": str(e)
            }
    
    def _format_result(self, result: RetrievalResult) -> Dict[str, Any]:
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœ"""
        return {
            "content": result.content,
            "metadata": result.metadata,
            "initial_score": result.initial_score,
            "rerank_score": result.rerank_score,
            "final_score": result.get_final_score(),
            "retrieval_level": result.retrieval_level,
            "source_document": result.source_document,
            "relevance_explanation": result.relevance_explanation
        }
    
    def get_shields_status(self) -> Dict[str, Any]:
        """è·å–è½»é‡ç‰ˆç§©åºä¹‹ç›¾çŠ¶æ€"""
        try:
            return {
                "status": "active",
                "shields_version": "2.0.0-Genesis-Chapter3-Lite",
                "components": {
                    "lite_reranker": {
                        "status": "operational",
                        "type": "rule_based"
                    },
                    "document_summarizer": {
                        "status": "operational",
                        "type": "extractive",
                        "llm_enabled": self.document_summarizer.llm_manager is not None
                    },
                    "query_complexity_analyzer": {
                        "status": "operational",
                        "type": "rule_based"
                    },
                    "hierarchical_retriever": {
                        "status": "operational",
                        "type": "multi_strategy"
                    }
                },
                "capabilities": [
                    "è½»é‡çº§äºŒçº§ç²¾ç‚¼",
                    "æ˜Ÿå›¾å¯¼èˆªç­–ç•¥",
                    "æŸ¥è¯¢å¤æ‚åº¦åˆ†æ",
                    "åˆ†å±‚æ£€ç´¢",
                    "é˜²è…çƒ‚æœºåˆ¶"
                ],
                "advantages": [
                    "æ— ä¾èµ–å†²çª",
                    "å¿«é€Ÿå¯åŠ¨",
                    "ä½èµ„æºæ¶ˆè€—",
                    "é«˜å¯é æ€§"
                ],
                "last_check": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–è½»é‡ç‰ˆç§©åºä¹‹ç›¾çŠ¶æ€å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }