"""
ğŸŒŒ è®°å¿†æ˜Ÿå›¾ (Memory Nebula)
===========================

å®ç°"å¤§å®ªç« "ç¬¬äºŒç« ï¼šçŸ¥è¯†çš„"è¿æ¥"
- å…³ç³»æå–æµç¨‹
- çŸ¥è¯†å›¾è°±æ„å»º
- è®°å¿†æ˜Ÿå›¾ç®¡ç†

Author: N.S.S-Novena-Garfield Project
Version: 2.0.0 - "Genesis" Chapter 2
"""

import json
import networkx as nx
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import hashlib
import sqlite3
from pathlib import Path

from utils.logger import logger
from config import StorageConfig

@dataclass
class KnowledgeTriplet:
    """çŸ¥è¯†ä¸‰å…ƒç»„"""
    subject: str          # ä¸»è¯­
    relation: str         # å…³ç³»
    object: str          # å®¾è¯­
    sentence_index: int   # å¥å­ç´¢å¼•
    sentence_content: str # å¥å­å†…å®¹
    document_id: str     # æ–‡æ¡£ID
    paragraph_id: str    # æ®µè½ID
    confidence: float    # ç½®ä¿¡åº¦
    source: str          # æ¥æº
    created_at: str      # åˆ›å»ºæ—¶é—´
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "subject": self.subject,
            "relation": self.relation,
            "object": self.object,
            "sentence_index": self.sentence_index,
            "sentence_content": self.sentence_content,
            "document_id": self.document_id,
            "paragraph_id": self.paragraph_id,
            "confidence": self.confidence,
            "source": self.source,
            "created_at": self.created_at
        }
    
    def get_triplet_id(self) -> str:
        """ç”Ÿæˆä¸‰å…ƒç»„å”¯ä¸€ID"""
        triplet_str = f"{self.subject}|{self.relation}|{self.object}"
        return hashlib.md5(triplet_str.encode()).hexdigest()

class RelationExtractor:
    """å…³ç³»æå–å™¨ - ç”Ÿç‰©ä¿¡æ¯å…³ç³»æå–å®˜"""
    
    def __init__(self, llm_manager=None):
        self.llm_manager = llm_manager
        logger.info("ğŸ”¬ å…³ç³»æå–å™¨å·²åˆå§‹åŒ–")
    
    def _build_extraction_prompt(self, sentence_list: List[str]) -> str:
        """æ„å»ºå…³ç³»æå–çš„æç¤ºè¯"""
        sentences_json = []
        for i, sentence in enumerate(sentence_list):
            sentences_json.append({
                "index": i,
                "content": sentence.strip()
            })
        
        prompt = f"""ä½ æ˜¯ä¸€å'ç”Ÿç‰©ä¿¡æ¯å…³ç³»æå–å®˜'ã€‚ä½ çš„ä»»åŠ¡ï¼Œæ˜¯ä»æˆ‘ä¸‹é¢ï¼Œæä¾›ç»™ä½ çš„è¿™ä¸ª'å¥å­åˆ—è¡¨'ä¸­ï¼Œä¸ºæ¯ä¸€ä¸ªå¥å­ï¼Œéƒ½ï¼Œæå–å‡ºæ‰€æœ‰å¯èƒ½çš„'çŸ¥è¯†ä¸‰å…ƒç»„ (ä¸»è¯­, å…³ç³», å®¾è¯­)'ã€‚

è¯·ï¼Œä»¥ä¸€ä¸ª'JSONæ•°ç»„'çš„æ ¼å¼ï¼Œè¿”å›ä½ çš„ç»“æœã€‚æ•°ç»„çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œéƒ½åº”è¯¥ï¼Œå¯¹åº”ä¸€ä¸ªå¥å­ï¼Œå¹¶åŒ…å«'å¥å­ç´¢å¼•'å’Œå®ƒæ‰€å¯¹åº”çš„'ä¸‰å…ƒç»„åˆ—è¡¨'ã€‚

è¾“å…¥å¥å­åˆ—è¡¨ï¼š
{json.dumps(sentences_json, ensure_ascii=False, indent=2)}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
[
  {{
    "sentence_index": 0,
    "sentence_content": "å¥å­å†…å®¹",
    "triplets": [
      {{
        "subject": "ä¸»è¯­",
        "relation": "å…³ç³»",
        "object": "å®¾è¯­",
        "confidence": 0.95
      }}
    ]
  }}
]

æ³¨æ„äº‹é¡¹ï¼š
1. æ¯ä¸ªå¥å­è‡³å°‘æå–1ä¸ªä¸‰å…ƒç»„ï¼Œæœ€å¤šæå–5ä¸ª
2. ä¸»è¯­å’Œå®¾è¯­åº”è¯¥æ˜¯å…·ä½“çš„å®ä½“æˆ–æ¦‚å¿µ
3. å…³ç³»åº”è¯¥æ˜¯åŠ¨è¯æˆ–åŠ¨è¯çŸ­è¯­
4. confidenceè¡¨ç¤ºæå–çš„ç½®ä¿¡åº¦(0-1ä¹‹é—´)
5. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šæ–‡å­—"""

        return prompt
    
    def extract_relations_from_paragraph(self, sentence_list: List[str], 
                                       document_id: str, paragraph_id: str) -> List[KnowledgeTriplet]:
        """ä»æ®µè½çš„å¥å­åˆ—è¡¨ä¸­æå–å…³ç³»"""
        try:
            if not sentence_list:
                logger.warning("å¥å­åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡å…³ç³»æå–")
                return []
            
            logger.info(f"å¼€å§‹æå–æ®µè½ {paragraph_id} çš„å…³ç³»ï¼ŒåŒ…å« {len(sentence_list)} ä¸ªå¥å­")
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_extraction_prompt(sentence_list)
            
            # è°ƒç”¨LLMè¿›è¡Œå…³ç³»æå–
            if self.llm_manager:
                try:
                    response = self.llm_manager.generate_response(prompt)
                    logger.debug(f"LLMå“åº”: {response[:200]}...")
                    
                    # è§£æJSONå“åº”
                    triplets = self._parse_extraction_response(
                        response, sentence_list, document_id, paragraph_id
                    )
                    
                    logger.info(f"æˆåŠŸæå– {len(triplets)} ä¸ªçŸ¥è¯†ä¸‰å…ƒç»„")
                    return triplets
                    
                except Exception as e:
                    logger.error(f"LLMå…³ç³»æå–å¤±è´¥: {e}")
                    return self._fallback_extraction(sentence_list, document_id, paragraph_id)
            else:
                logger.warning("LLMç®¡ç†å™¨æœªé…ç½®ï¼Œä½¿ç”¨å¤‡ç”¨æå–æ–¹æ³•")
                return self._fallback_extraction(sentence_list, document_id, paragraph_id)
                
        except Exception as e:
            logger.error(f"å…³ç³»æå–å¤±è´¥: {e}")
            return []
    
    def _parse_extraction_response(self, response: str, sentence_list: List[str],
                                 document_id: str, paragraph_id: str) -> List[KnowledgeTriplet]:
        """è§£æLLMçš„æå–å“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            response = response.strip()
            
            # æŸ¥æ‰¾JSONæ•°ç»„çš„å¼€å§‹å’Œç»“æŸ
            start_idx = response.find('[')
            end_idx = response.rfind(']') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("å“åº”ä¸­æœªæ‰¾åˆ°JSONæ•°ç»„")
            
            json_str = response[start_idx:end_idx]
            extraction_results = json.loads(json_str)
            
            triplets = []
            current_time = datetime.now().isoformat()
            
            for result in extraction_results:
                sentence_index = result.get("sentence_index", 0)
                sentence_content = result.get("sentence_content", "")
                
                # ç¡®ä¿å¥å­å†…å®¹æ­£ç¡®
                if sentence_index < len(sentence_list):
                    sentence_content = sentence_list[sentence_index]
                
                for triplet_data in result.get("triplets", []):
                    triplet = KnowledgeTriplet(
                        subject=triplet_data.get("subject", "").strip(),
                        relation=triplet_data.get("relation", "").strip(),
                        object=triplet_data.get("object", "").strip(),
                        sentence_index=sentence_index,
                        sentence_content=sentence_content,
                        document_id=document_id,
                        paragraph_id=paragraph_id,
                        confidence=float(triplet_data.get("confidence", 0.8)),
                        source="llm_extraction",
                        created_at=current_time
                    )
                    
                    # éªŒè¯ä¸‰å…ƒç»„çš„æœ‰æ•ˆæ€§
                    if triplet.subject and triplet.relation and triplet.object:
                        triplets.append(triplet)
            
            return triplets
            
        except Exception as e:
            logger.error(f"è§£ææå–å“åº”å¤±è´¥: {e}")
            return self._fallback_extraction(sentence_list, document_id, paragraph_id)
    
    def _fallback_extraction(self, sentence_list: List[str], 
                           document_id: str, paragraph_id: str) -> List[KnowledgeTriplet]:
        """å¤‡ç”¨å…³ç³»æå–æ–¹æ³•ï¼ˆåŸºäºè§„åˆ™ï¼‰"""
        try:
            logger.info("ä½¿ç”¨å¤‡ç”¨å…³ç³»æå–æ–¹æ³•")
            triplets = []
            current_time = datetime.now().isoformat()
            
            # ç®€å•çš„è§„åˆ™æå–ï¼ˆç¤ºä¾‹ï¼‰
            for i, sentence in enumerate(sentence_list):
                sentence = sentence.strip()
                if len(sentence) > 10:  # åªå¤„ç†æœ‰æ„ä¹‰çš„å¥å­
                    words = sentence.split()
                    if len(words) >= 3:
                        # ç®€å•çš„è§„åˆ™ï¼šå¯»æ‰¾å¸¸è§çš„å…³ç³»è¯
                        relation_words = ["æ˜¯", "åŒ…å«", "å±äº", "ä½¿ç”¨", "åº”ç”¨", "ç ”ç©¶", "å‘å±•", "æå‡", "æ¨åŠ¨"]
                        
                        found_relation = None
                        relation_index = -1
                        
                        for j, word in enumerate(words):
                            if any(rel in word for rel in relation_words):
                                found_relation = word
                                relation_index = j
                                break
                        
                        if found_relation and relation_index > 0 and relation_index < len(words) - 1:
                            # æå–ä¸»è¯­ï¼ˆå…³ç³»è¯å‰çš„è¯ï¼‰
                            subject_words = words[:relation_index]
                            subject = "".join(subject_words) if subject_words else words[0]
                            
                            # æå–å®¾è¯­ï¼ˆå…³ç³»è¯åçš„è¯ï¼‰
                            object_words = words[relation_index + 1:]
                            obj = "".join(object_words) if object_words else words[-1]
                            
                            # æ¸…ç†æ ‡ç‚¹ç¬¦å·
                            subject = subject.rstrip('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
                            obj = obj.rstrip('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
                            
                            if subject and obj and subject != obj:
                                triplet = KnowledgeTriplet(
                                    subject=subject,
                                    relation=found_relation,
                                    object=obj,
                                    sentence_index=i,
                                    sentence_content=sentence,
                                    document_id=document_id,
                                    paragraph_id=paragraph_id,
                                    confidence=0.6,  # ä¸­ç­‰ç½®ä¿¡åº¦
                                    source="rule_based",
                                    created_at=current_time
                                )
                                triplets.append(triplet)
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³ç³»è¯ï¼Œä½¿ç”¨ç®€å•çš„ä¸»è°“å®¾ç»“æ„
                            if len(words) >= 3:
                                triplet = KnowledgeTriplet(
                                    subject=words[0].rstrip('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š'),
                                    relation="ç›¸å…³äº",
                                    object=words[-1].rstrip('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š'),
                                    sentence_index=i,
                                    sentence_content=sentence,
                                    document_id=document_id,
                                    paragraph_id=paragraph_id,
                                    confidence=0.4,  # è¾ƒä½çš„ç½®ä¿¡åº¦
                                    source="rule_based",
                                    created_at=current_time
                                )
                                triplets.append(triplet)
            
            logger.info(f"å¤‡ç”¨æ–¹æ³•æå–äº† {len(triplets)} ä¸ªä¸‰å…ƒç»„")
            return triplets
            
        except Exception as e:
            logger.error(f"å¤‡ç”¨å…³ç³»æå–å¤±è´¥: {e}")
            return []

class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±ç®¡ç†å™¨"""
    
    def __init__(self):
        self.graph = nx.MultiDiGraph()  # æ”¯æŒå¤šé‡è¾¹çš„æœ‰å‘å›¾
        self.triplet_storage = TripletStorage()
        logger.info("ğŸŒ çŸ¥è¯†å›¾è°±ç®¡ç†å™¨å·²åˆå§‹åŒ–")
    
    def add_triplets(self, triplets: List[KnowledgeTriplet]) -> Dict[str, Any]:
        """æ·»åŠ çŸ¥è¯†ä¸‰å…ƒç»„åˆ°å›¾è°±"""
        try:
            if not triplets:
                return {"added": 0, "updated": 0, "total_nodes": 0, "total_edges": 0}
            
            added_count = 0
            updated_count = 0
            
            for triplet in triplets:
                # å­˜å‚¨åˆ°æ•°æ®åº“
                stored = self.triplet_storage.store_triplet(triplet)
                
                if stored:
                    # æ·»åŠ åˆ°NetworkXå›¾
                    self._add_triplet_to_graph(triplet)
                    added_count += 1
                else:
                    # æ›´æ–°æƒé‡
                    self._update_triplet_weight(triplet)
                    updated_count += 1
            
            stats = {
                "added": added_count,
                "updated": updated_count,
                "total_nodes": self.graph.number_of_nodes(),
                "total_edges": self.graph.number_of_edges()
            }
            
            logger.info(f"å›¾è°±æ›´æ–°å®Œæˆ: æ–°å¢ {added_count}, æ›´æ–° {updated_count}")
            return stats
            
        except Exception as e:
            logger.error(f"æ·»åŠ ä¸‰å…ƒç»„åˆ°å›¾è°±å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _add_triplet_to_graph(self, triplet: KnowledgeTriplet):
        """å°†ä¸‰å…ƒç»„æ·»åŠ åˆ°NetworkXå›¾"""
        try:
            # æ·»åŠ èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if not self.graph.has_node(triplet.subject):
                self.graph.add_node(triplet.subject, type="entity")
            
            if not self.graph.has_node(triplet.object):
                self.graph.add_node(triplet.object, type="entity")
            
            # æ·»åŠ è¾¹
            edge_data = {
                "relation": triplet.relation,
                "weight": 1.0,
                "confidence": triplet.confidence,
                "sources": [triplet.source],
                "documents": [triplet.document_id],
                "created_at": triplet.created_at,
                "triplet_id": triplet.get_triplet_id()
            }
            
            self.graph.add_edge(triplet.subject, triplet.object, **edge_data)
            
        except Exception as e:
            logger.error(f"æ·»åŠ ä¸‰å…ƒç»„åˆ°å›¾å¤±è´¥: {e}")
    
    def _update_triplet_weight(self, triplet: KnowledgeTriplet):
        """æ›´æ–°å·²å­˜åœ¨ä¸‰å…ƒç»„çš„æƒé‡"""
        try:
            # æŸ¥æ‰¾ç°æœ‰è¾¹
            if self.graph.has_edge(triplet.subject, triplet.object):
                edges = self.graph[triplet.subject][triplet.object]
                
                # æŸ¥æ‰¾ç›¸åŒå…³ç³»çš„è¾¹
                for key, edge_data in edges.items():
                    if edge_data.get("relation") == triplet.relation:
                        # å¢åŠ æƒé‡
                        edge_data["weight"] = edge_data.get("weight", 1.0) + 1.0
                        
                        # æ·»åŠ æ–°çš„æ¥æº
                        sources = edge_data.get("sources", [])
                        if triplet.source not in sources:
                            sources.append(triplet.source)
                        edge_data["sources"] = sources
                        
                        # æ·»åŠ æ–°çš„æ–‡æ¡£
                        documents = edge_data.get("documents", [])
                        if triplet.document_id not in documents:
                            documents.append(triplet.document_id)
                        edge_data["documents"] = documents
                        
                        logger.debug(f"æ›´æ–°ä¸‰å…ƒç»„æƒé‡: {triplet.subject} -> {triplet.object}")
                        break
                        
        except Exception as e:
            logger.error(f"æ›´æ–°ä¸‰å…ƒç»„æƒé‡å¤±è´¥: {e}")
    
    def find_related_entities(self, entity: str, max_depth: int = 2) -> Dict[str, Any]:
        """æŸ¥æ‰¾ä¸å®ä½“ç›¸å…³çš„å…¶ä»–å®ä½“"""
        try:
            if not self.graph.has_node(entity):
                return {"entity": entity, "related": [], "paths": []}
            
            # ä½¿ç”¨BFSæŸ¥æ‰¾ç›¸å…³å®ä½“
            related_entities = []
            visited = set()
            queue = [(entity, 0, [])]  # (èŠ‚ç‚¹, æ·±åº¦, è·¯å¾„)
            
            while queue:
                current_entity, depth, path = queue.pop(0)
                
                if current_entity in visited or depth > max_depth:
                    continue
                
                visited.add(current_entity)
                
                if current_entity != entity:
                    related_entities.append({
                        "entity": current_entity,
                        "depth": depth,
                        "path": path + [current_entity]
                    })
                
                # æ·»åŠ é‚»å±…èŠ‚ç‚¹
                if depth < max_depth:
                    for neighbor in self.graph.neighbors(current_entity):
                        if neighbor not in visited:
                            queue.append((neighbor, depth + 1, path + [current_entity]))
            
            return {
                "entity": entity,
                "related": related_entities[:20],  # é™åˆ¶è¿”å›æ•°é‡
                "total_found": len(related_entities)
            }
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾ç›¸å…³å®ä½“å¤±è´¥: {e}")
            return {"entity": entity, "error": str(e)}
    
    def get_graph_stats(self) -> Dict[str, Any]:
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                "total_nodes": self.graph.number_of_nodes(),
                "total_edges": self.graph.number_of_edges(),
                "density": nx.density(self.graph),
                "is_connected": nx.is_weakly_connected(self.graph) if self.graph.number_of_nodes() > 0 else False
            }
            
            # è·å–åº¦æ•°ç»Ÿè®¡
            if self.graph.number_of_nodes() > 0:
                degrees = dict(self.graph.degree())
                stats["avg_degree"] = sum(degrees.values()) / len(degrees)
                stats["max_degree"] = max(degrees.values()) if degrees else 0
                stats["min_degree"] = min(degrees.values()) if degrees else 0
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–å›¾è°±ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}

class TripletStorage:
    """ä¸‰å…ƒç»„å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.db_path = StorageConfig.MODELS_DATA_DIR / "knowledge_graph.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()
        logger.info("ğŸ’¾ ä¸‰å…ƒç»„å­˜å‚¨ç®¡ç†å™¨å·²åˆå§‹åŒ–")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            
            # åˆ›å»ºä¸‰å…ƒç»„è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS knowledge_triplets (
                    triplet_id TEXT PRIMARY KEY,
                    subject TEXT NOT NULL,
                    relation TEXT NOT NULL,
                    object TEXT NOT NULL,
                    sentence_index INTEGER,
                    sentence_content TEXT,
                    document_id TEXT,
                    paragraph_id TEXT,
                    confidence REAL,
                    source TEXT,
                    weight REAL DEFAULT 1.0,
                    created_at TEXT,
                    updated_at TEXT
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_subject ON knowledge_triplets (subject)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_object ON knowledge_triplets (object)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_relation ON knowledge_triplets (relation)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_document ON knowledge_triplets (document_id)")
            
            conn.commit()
            conn.close()
            
            logger.info("ä¸‰å…ƒç»„æ•°æ®åº“è¡¨åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–ä¸‰å…ƒç»„æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def store_triplet(self, triplet: KnowledgeTriplet) -> bool:
        """å­˜å‚¨ä¸‰å…ƒç»„"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            
            triplet_id = triplet.get_triplet_id()
            current_time = datetime.now().isoformat()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("SELECT weight FROM knowledge_triplets WHERE triplet_id = ?", (triplet_id,))
            existing = cursor.fetchone()
            
            if existing:
                # æ›´æ–°æƒé‡
                new_weight = existing[0] + 1.0
                cursor.execute("""
                    UPDATE knowledge_triplets 
                    SET weight = ?, updated_at = ?
                    WHERE triplet_id = ?
                """, (new_weight, current_time, triplet_id))
                
                conn.commit()
                conn.close()
                return False  # è¡¨ç¤ºæ›´æ–°è€Œéæ–°å¢
            else:
                # æ’å…¥æ–°è®°å½•
                cursor.execute("""
                    INSERT INTO knowledge_triplets 
                    (triplet_id, subject, relation, object, sentence_index, sentence_content,
                     document_id, paragraph_id, confidence, source, weight, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    triplet_id, triplet.subject, triplet.relation, triplet.object,
                    triplet.sentence_index, triplet.sentence_content, triplet.document_id,
                    triplet.paragraph_id, triplet.confidence, triplet.source, 1.0,
                    triplet.created_at, current_time
                ))
                
                conn.commit()
                conn.close()
                return True  # è¡¨ç¤ºæ–°å¢
                
        except Exception as e:
            logger.error(f"å­˜å‚¨ä¸‰å…ƒç»„å¤±è´¥: {e}")
            return False
    
    def get_triplets_by_document(self, document_id: str) -> List[Dict[str, Any]]:
        """è·å–æ–‡æ¡£çš„æ‰€æœ‰ä¸‰å…ƒç»„"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT * FROM knowledge_triplets 
                WHERE document_id = ?
                ORDER BY weight DESC
            """, (document_id,))
            
            columns = [desc[0] for desc in cursor.description]
            triplets = []
            
            for row in cursor.fetchall():
                triplet_dict = dict(zip(columns, row))
                triplets.append(triplet_dict)
            
            conn.close()
            return triplets
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£ä¸‰å…ƒç»„å¤±è´¥: {e}")
            return []

class MemoryNebula:
    """è®°å¿†æ˜Ÿå›¾ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, llm_manager=None):
        self.relation_extractor = RelationExtractor(llm_manager)
        self.knowledge_graph = KnowledgeGraph()
        logger.info("ğŸŒŒ è®°å¿†æ˜Ÿå›¾å·²å¯åŠ¨")
    
    def process_paragraph_relations(self, knowledge_atoms: List, document_id: str) -> Dict[str, Any]:
        """å¤„ç†æ®µè½å…³ç³»æå–"""
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£ {document_id} çš„å…³ç³»æå–")
            
            # æŒ‰æ®µè½åˆ†ç»„çŸ¥è¯†åŸå­
            paragraph_groups = {}
            for atom in knowledge_atoms:
                para_id = atom.paragraph_id
                if para_id not in paragraph_groups:
                    paragraph_groups[para_id] = []
                paragraph_groups[para_id].append(atom)
            
            total_triplets = []
            processed_paragraphs = 0
            
            # å¤„ç†æ¯ä¸ªæ®µè½
            for para_id, atoms in paragraph_groups.items():
                logger.info(f"å¤„ç†æ®µè½ {para_id}ï¼ŒåŒ…å« {len(atoms)} ä¸ªçŸ¥è¯†åŸå­")
                
                # æå–å¥å­åˆ—è¡¨
                sentence_list = [atom.content for atom in atoms]
                
                # æå–å…³ç³»
                triplets = self.relation_extractor.extract_relations_from_paragraph(
                    sentence_list, document_id, para_id
                )
                
                if triplets:
                    # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                    graph_stats = self.knowledge_graph.add_triplets(triplets)
                    total_triplets.extend(triplets)
                    
                    logger.info(f"æ®µè½ {para_id} æå–äº† {len(triplets)} ä¸ªä¸‰å…ƒç»„")
                
                processed_paragraphs += 1
            
            result = {
                "success": True,
                "document_id": document_id,
                "processed_paragraphs": processed_paragraphs,
                "total_triplets": len(total_triplets),
                "graph_stats": self.knowledge_graph.get_graph_stats(),
                "processing_time": datetime.now().isoformat()
            }
            
            logger.info(f"æ–‡æ¡£ {document_id} å…³ç³»æå–å®Œæˆ: {len(total_triplets)} ä¸ªä¸‰å…ƒç»„")
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ®µè½å…³ç³»å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "document_id": document_id
            }
    
    def query_related_knowledge(self, entity: str, max_depth: int = 2) -> Dict[str, Any]:
        """æŸ¥è¯¢ç›¸å…³çŸ¥è¯†"""
        try:
            logger.info(f"æŸ¥è¯¢å®ä½“ '{entity}' çš„ç›¸å…³çŸ¥è¯†")
            
            related_info = self.knowledge_graph.find_related_entities(entity, max_depth)
            
            return {
                "success": True,
                "query_entity": entity,
                "related_entities": related_info.get("related", []),
                "total_found": related_info.get("total_found", 0),
                "query_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢ç›¸å…³çŸ¥è¯†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "query_entity": entity
            }
    
    def get_nebula_status(self) -> Dict[str, Any]:
        """è·å–è®°å¿†æ˜Ÿå›¾çŠ¶æ€"""
        try:
            graph_stats = self.knowledge_graph.get_graph_stats()
            
            return {
                "status": "operational",
                "nebula_version": "2.0.0-Genesis-Chapter2",
                "capabilities": [
                    "å…³ç³»æå–",
                    "çŸ¥è¯†å›¾è°±æ„å»º",
                    "å®ä½“å…³è”æŸ¥è¯¢",
                    "æƒé‡ç®¡ç†"
                ],
                "graph_statistics": graph_stats,
                "last_check": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–è®°å¿†æ˜Ÿå›¾çŠ¶æ€å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }