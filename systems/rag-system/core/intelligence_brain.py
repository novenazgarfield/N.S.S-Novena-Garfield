"""
ğŸ§  ä¸­å¤®æƒ…æŠ¥å¤§è„‘ (Central Intelligence Brain)
==============================================

æŒ‰ç…§"å¤§å®ªç« "æ„å»ºçš„æ–°ä¸€ä»£RAGæ ¸å¿ƒç³»ç»Ÿ
- æ‹¥æœ‰é•¿æœŸè®°å¿†ã€æ·±åº¦ç†è§£ã€è‡ªæˆ‘ä¿®å¤ä¸ç²¾å‡†æ§åˆ¶èƒ½åŠ›
- å®ç°"ä¸‰ä½ä¸€ä½“"æ™ºèƒ½åˆ†å—æ¶æ„
- æ”¯æŒçŸ¥è¯†çš„æ°¸æ’çƒ™å°ä¸è‡ªåŠ¨å½’æ¡£

Author: N.S.S-Novena-Garfield Project
Version: 2.0.0 - "Genesis"
"""

import uuid
import sqlite3
import chromadb
import nltk
import spacy
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
from datetime import datetime
import json
import hashlib

from config import StorageConfig, DocumentConfig
from utils.logger import logger
from core.memory_nebula import MemoryNebula
from core.shields_of_order import ShieldsOfOrder
from core.fire_control_system import FireControlSystem, SearchScope, AttentionTarget
from core.chronicle_healing import (
    chronicle_self_healing as ai_self_healing, 
    intelligence_brain_self_healing,
    FailureSeverity,
    SystemSource,
    configure_chronicle_healing,
    check_chronicle_federation_health
)

# ç¡®ä¿NLTKæ•°æ®å¯ç”¨
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class KnowledgeAtom:
    """çŸ¥è¯†åŸå­ - å¥å­çº§åˆ«çš„æœ€å°çŸ¥è¯†å•å…ƒ"""
    
    def __init__(self, content: str, document_id: str, paragraph_id: str, sentence_id: str):
        self.content = content
        self.document_id = document_id
        self.paragraph_id = paragraph_id
        self.sentence_id = sentence_id
        self.atom_id = self._generate_atom_id()
        self.created_at = datetime.now().isoformat()
        
    def _generate_atom_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„åŸå­ID"""
        content_hash = hashlib.md5(self.content.encode()).hexdigest()[:8]
        return f"{self.document_id}_{self.paragraph_id}_{self.sentence_id}_{content_hash}"
    
    def get_metadata(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´çš„å…ƒæ•°æ®é“­ç‰Œ"""
        return {
            "atom_id": self.atom_id,
            "document_id": self.document_id,
            "paragraph_id": self.paragraph_id,
            "sentence_id": self.sentence_id,
            "created_at": self.created_at,
            "content_length": len(self.content),
            "content_hash": hashlib.md5(self.content.encode()).hexdigest()
        }

class TrinityChunker:
    """ä¸‰ä½ä¸€ä½“æ™ºèƒ½åˆ†å—å™¨"""
    
    def __init__(self):
        self.nlp = None
        self._load_nlp_model()
    
    def _load_nlp_model(self):
        """åŠ è½½spaCyæ¨¡å‹"""
        try:
            # å°è¯•åŠ è½½ä¸­æ–‡æ¨¡å‹
            self.nlp = spacy.load("zh_core_web_sm")
            logger.info("å·²åŠ è½½ä¸­æ–‡spaCyæ¨¡å‹")
        except OSError:
            try:
                # å°è¯•åŠ è½½è‹±æ–‡æ¨¡å‹
                self.nlp = spacy.load("en_core_web_sm")
                logger.info("å·²åŠ è½½è‹±æ–‡spaCyæ¨¡å‹")
            except OSError:
                logger.warning("æœªæ‰¾åˆ°spaCyæ¨¡å‹ï¼Œå°†ä½¿ç”¨NLTKè¿›è¡Œå¥å­åˆ†å‰²")
                self.nlp = None
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """ç¬¬ä¸€å±‚ï¼šå°†æ–‡æœ¬ç²¾å‡†åˆ†å‰²ä¸ºå¥å­åŸå­"""
        try:
            if self.nlp:
                # ä½¿ç”¨spaCyè¿›è¡Œå¥å­åˆ†å‰²
                doc = self.nlp(text)
                sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
            else:
                # ä½¿ç”¨NLTKè¿›è¡Œå¥å­åˆ†å‰²
                sentences = nltk.sent_tokenize(text)
                sentences = [sent.strip() for sent in sentences if sent.strip()]
            
            logger.debug(f"æ–‡æœ¬åˆ†å‰²ä¸º {len(sentences)} ä¸ªå¥å­")
            return sentences
            
        except Exception as e:
            logger.error(f"å¥å­åˆ†å‰²å¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šæŒ‰å¥å·åˆ†å‰²
            sentences = [s.strip() for s in text.split('.') if s.strip()]
            return sentences
    
    def _group_into_paragraphs(self, text: str) -> List[str]:
        """ç¬¬äºŒå±‚ï¼šæ ¹æ®æ¢è¡Œç¬¦å°†å¥å­èšåˆä¸ºæ®µè½åˆ†å­"""
        try:
            # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
            paragraphs = text.split('\n\n')
            # æ¸…ç†ç©ºæ®µè½
            paragraphs = [p.strip() for p in paragraphs if p.strip()]
            
            # å¦‚æœæ²¡æœ‰åŒæ¢è¡Œç¬¦ï¼ŒæŒ‰å•æ¢è¡Œç¬¦åˆ†å‰²
            if len(paragraphs) == 1:
                paragraphs = text.split('\n')
                paragraphs = [p.strip() for p in paragraphs if p.strip()]
            
            logger.debug(f"æ–‡æœ¬åˆ†å‰²ä¸º {len(paragraphs)} ä¸ªæ®µè½")
            return paragraphs
            
        except Exception as e:
            logger.error(f"æ®µè½åˆ†å‰²å¤±è´¥: {e}")
            return [text]  # é™çº§å¤„ç†ï¼šæ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªæ®µè½
    
    def trinity_chunk(self, document_content: str, document_id: str) -> List[KnowledgeAtom]:
        """æ‰§è¡Œä¸‰ä½ä¸€ä½“åˆ†å—å¤„ç†"""
        try:
            logger.info(f"å¼€å§‹å¯¹æ–‡æ¡£ {document_id} æ‰§è¡Œä¸‰ä½ä¸€ä½“åˆ†å—...")
            
            knowledge_atoms = []
            
            # ç¬¬äºŒå±‚ï¼šæ®µè½åˆ†å­
            paragraphs = self._group_into_paragraphs(document_content)
            
            for para_idx, paragraph in enumerate(paragraphs):
                paragraph_id = f"para_{para_idx:04d}"
                
                # ç¬¬ä¸€å±‚ï¼šå¥å­åŸå­
                sentences = self._split_into_sentences(paragraph)
                
                for sent_idx, sentence in enumerate(sentences):
                    sentence_id = f"sent_{sent_idx:04d}"
                    
                    # åˆ›å»ºçŸ¥è¯†åŸå­
                    atom = KnowledgeAtom(
                        content=sentence,
                        document_id=document_id,
                        paragraph_id=paragraph_id,
                        sentence_id=sentence_id
                    )
                    
                    knowledge_atoms.append(atom)
            
            logger.info(f"ä¸‰ä½ä¸€ä½“åˆ†å—å®Œæˆ: {len(knowledge_atoms)} ä¸ªçŸ¥è¯†åŸå­")
            return knowledge_atoms
            
        except Exception as e:
            logger.error(f"ä¸‰ä½ä¸€ä½“åˆ†å—å¤±è´¥: {e}")
            raise

class EternalArchive:
    """æ°¸æ’å½’æ¡£ç³»ç»Ÿ - å®ç°çŸ¥è¯†çš„æ°¸æ’çƒ™å°"""
    
    def __init__(self):
        self.chroma_client = None
        self.sqlite_conn = None
        self.vector_collection = None
        self._initialize_storage()
    
    def _initialize_storage(self):
        """åˆå§‹åŒ–æŒä¹…åŒ–å­˜å‚¨ç³»ç»Ÿ"""
        try:
            # åˆå§‹åŒ–ChromaDBï¼ˆå‘é‡æ•°æ®åº“ï¼‰
            chroma_path = StorageConfig.MODELS_DATA_DIR / "chroma_db"
            chroma_path.mkdir(parents=True, exist_ok=True)
            
            self.chroma_client = chromadb.PersistentClient(path=str(chroma_path))
            self.vector_collection = self.chroma_client.get_or_create_collection(
                name="knowledge_atoms",
                metadata={"description": "çŸ¥è¯†åŸå­å‘é‡å­˜å‚¨"}
            )
            
            # åˆå§‹åŒ–SQLiteï¼ˆå…³ç³»å‹æ•°æ®åº“ï¼‰
            sqlite_path = StorageConfig.MODELS_DATA_DIR / "intelligence_brain.db"
            self.sqlite_conn = sqlite3.connect(str(sqlite_path), check_same_thread=False)
            self._create_tables()
            
            logger.info("æ°¸æ’å½’æ¡£ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ°¸æ’å½’æ¡£ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _create_tables(self):
        """åˆ›å»ºSQLiteè¡¨ç»“æ„"""
        try:
            cursor = self.sqlite_conn.cursor()
            
            # æ–‡æ¡£ç´¢å¼•è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    document_id TEXT PRIMARY KEY,
                    filename TEXT NOT NULL,
                    file_path TEXT,
                    file_size INTEGER,
                    file_hash TEXT,
                    upload_time TEXT NOT NULL,
                    processed_time TEXT NOT NULL,
                    total_atoms INTEGER DEFAULT 0,
                    metadata TEXT
                )
            """)
            
            # çŸ¥è¯†åŸå­ç´¢å¼•è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS knowledge_atoms (
                    atom_id TEXT PRIMARY KEY,
                    document_id TEXT NOT NULL,
                    paragraph_id TEXT NOT NULL,
                    sentence_id TEXT NOT NULL,
                    content TEXT NOT NULL,
                    content_length INTEGER,
                    content_hash TEXT,
                    created_at TEXT NOT NULL,
                    FOREIGN KEY (document_id) REFERENCES documents (document_id)
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_document_id ON knowledge_atoms (document_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_content_hash ON knowledge_atoms (content_hash)")
            
            self.sqlite_conn.commit()
            logger.info("SQLiteè¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆ›å»ºSQLiteè¡¨å¤±è´¥: {e}")
            raise
    
    def archive_document(self, document_id: str, filename: str, file_path: str = None, 
                        file_size: int = None, metadata: Dict = None) -> bool:
        """å½’æ¡£æ–‡æ¡£ä¿¡æ¯åˆ°å…³ç³»å‹æ•°æ®åº“"""
        try:
            cursor = self.sqlite_conn.cursor()
            
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆå¦‚æœæœ‰æ–‡ä»¶è·¯å¾„ï¼‰
            file_hash = None
            if file_path and Path(file_path).exists():
                with open(file_path, 'rb') as f:
                    file_hash = hashlib.md5(f.read()).hexdigest()
            
            cursor.execute("""
                INSERT OR REPLACE INTO documents 
                (document_id, filename, file_path, file_size, file_hash, upload_time, processed_time, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                document_id,
                filename,
                file_path,
                file_size,
                file_hash,
                datetime.now().isoformat(),
                datetime.now().isoformat(),
                json.dumps(metadata) if metadata else None
            ))
            
            self.sqlite_conn.commit()
            logger.info(f"æ–‡æ¡£ {document_id} å·²å½’æ¡£åˆ°å…³ç³»å‹æ•°æ®åº“")
            return True
            
        except Exception as e:
            logger.error(f"å½’æ¡£æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def archive_knowledge_atoms(self, knowledge_atoms: List[KnowledgeAtom]) -> bool:
        """å½’æ¡£çŸ¥è¯†åŸå­åˆ°å‘é‡æ•°æ®åº“å’Œå…³ç³»å‹æ•°æ®åº“"""
        try:
            if not knowledge_atoms:
                logger.warning("æ²¡æœ‰çŸ¥è¯†åŸå­éœ€è¦å½’æ¡£")
                return False
            
            # å‡†å¤‡æ•°æ®
            atom_ids = []
            contents = []
            metadatas = []
            
            # SQLiteæ‰¹é‡æ’å…¥æ•°æ®
            cursor = self.sqlite_conn.cursor()
            sqlite_data = []
            
            for atom in knowledge_atoms:
                atom_ids.append(atom.atom_id)
                contents.append(atom.content)
                metadatas.append(atom.get_metadata())
                
                sqlite_data.append((
                    atom.atom_id,
                    atom.document_id,
                    atom.paragraph_id,
                    atom.sentence_id,
                    atom.content,
                    len(atom.content),
                    hashlib.md5(atom.content.encode()).hexdigest(),
                    atom.created_at
                ))
            
            # å½’æ¡£åˆ°ChromaDBï¼ˆå‘é‡æ•°æ®åº“ï¼‰
            self.vector_collection.add(
                ids=atom_ids,
                documents=contents,
                metadatas=metadatas
            )
            
            # å½’æ¡£åˆ°SQLiteï¼ˆå…³ç³»å‹æ•°æ®åº“ï¼‰
            cursor.executemany("""
                INSERT OR REPLACE INTO knowledge_atoms 
                (atom_id, document_id, paragraph_id, sentence_id, content, content_length, content_hash, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, sqlite_data)
            
            # æ›´æ–°æ–‡æ¡£çš„åŸå­è®¡æ•°
            document_id = knowledge_atoms[0].document_id
            cursor.execute("""
                UPDATE documents SET total_atoms = (
                    SELECT COUNT(*) FROM knowledge_atoms WHERE document_id = ?
                ) WHERE document_id = ?
            """, (document_id, document_id))
            
            self.sqlite_conn.commit()
            
            logger.info(f"æˆåŠŸå½’æ¡£ {len(knowledge_atoms)} ä¸ªçŸ¥è¯†åŸå­")
            return True
            
        except Exception as e:
            logger.error(f"å½’æ¡£çŸ¥è¯†åŸå­å¤±è´¥: {e}")
            return False
    
    def search_knowledge_atoms(self, query: str, top_k: int = 10, 
                             document_id: str = None) -> List[Dict[str, Any]]:
        """æœç´¢çŸ¥è¯†åŸå­"""
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clause = {}
            if document_id:
                where_clause["document_id"] = document_id
            
            # åœ¨ChromaDBä¸­æœç´¢
            results = self.vector_collection.query(
                query_texts=[query],
                n_results=top_k,
                where=where_clause if where_clause else None
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            search_results = []
            if results['documents'] and results['documents'][0]:
                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0],
                    results['distances'][0]
                )):
                    search_results.append({
                        "content": doc,
                        "metadata": metadata,
                        "similarity_score": 1 - distance,  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                        "rank": i + 1
                    })
            
            logger.info(f"æœç´¢å®Œæˆï¼Œè¿”å› {len(search_results)} ä¸ªç»“æœ")
            return search_results
            
        except Exception as e:
            logger.error(f"æœç´¢çŸ¥è¯†åŸå­å¤±è´¥: {e}")
            return []
    
    def get_document_stats(self) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            cursor = self.sqlite_conn.cursor()
            
            # æ–‡æ¡£ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) FROM documents")
            total_documents = cursor.fetchone()[0]
            
            # çŸ¥è¯†åŸå­ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) FROM knowledge_atoms")
            total_atoms = cursor.fetchone()[0]
            
            # å‘é‡é›†åˆç»Ÿè®¡
            vector_count = self.vector_collection.count()
            
            return {
                "total_documents": total_documents,
                "total_knowledge_atoms": total_atoms,
                "vector_count": vector_count,
                "storage_status": "operational"
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}

class IntelligenceBrain:
    """ä¸­å¤®æƒ…æŠ¥å¤§è„‘ - ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, llm_manager=None):
        self.trinity_chunker = TrinityChunker()
        self.eternal_archive = EternalArchive()
        self.memory_nebula = MemoryNebula(llm_manager)
        self.shields_of_order = ShieldsOfOrder(self.eternal_archive, self.memory_nebula, llm_manager)
        self.fire_control_system = FireControlSystem()
        # é…ç½®Chronicleè”é‚¦æ²»ç–—ç³»ç»Ÿ
        configure_chronicle_healing(
            chronicle_url="http://localhost:3000",
            max_retries=3,
            enable_fallback=True,
            default_source=SystemSource.INTELLIGENCE_BRAIN
        )
        logger.info("ğŸ§  ä¸­å¤®æƒ…æŠ¥å¤§è„‘å·²å¯åŠ¨ - é›†æˆè®°å¿†æ˜Ÿå›¾ã€ç§©åºä¹‹ç›¾ã€ç«æ§ç³»ç»Ÿï¼Œè¿æ¥Chronicleä¸­å¤®åŒ»é™¢")
    
    @intelligence_brain_self_healing(severity=FailureSeverity.MEDIUM, max_retries=2)
    def ingest_document(self, document_content: str, filename: str, 
                       file_path: str = None, metadata: Dict = None) -> Dict[str, Any]:
        """æ‘„å–æ–‡æ¡£å¹¶æ‰§è¡Œå®Œæ•´çš„çŸ¥è¯†å¤„ç†æµç¨‹"""
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            document_id = f"doc_{uuid.uuid4().hex[:12]}"
            
            logger.info(f"å¼€å§‹æ‘„å–æ–‡æ¡£: {filename} (ID: {document_id})")
            
            # ç¬¬ä¸€æ­¥ï¼šä¸‰ä½ä¸€ä½“æ™ºèƒ½åˆ†å—
            knowledge_atoms = self.trinity_chunker.trinity_chunk(document_content, document_id)
            
            if not knowledge_atoms:
                return {
                    "success": False,
                    "message": "æ–‡æ¡£åˆ†å—å¤±è´¥ï¼Œæœªç”ŸæˆçŸ¥è¯†åŸå­",
                    "document_id": document_id
                }
            
            # ç¬¬äºŒæ­¥ï¼šå½’æ¡£æ–‡æ¡£ä¿¡æ¯
            doc_archived = self.eternal_archive.archive_document(
                document_id=document_id,
                filename=filename,
                file_path=file_path,
                file_size=len(document_content),
                metadata=metadata
            )
            
            if not doc_archived:
                logger.warning(f"æ–‡æ¡£ {document_id} å½’æ¡£å¤±è´¥")
            
            # ç¬¬ä¸‰æ­¥ï¼šå½’æ¡£çŸ¥è¯†åŸå­ï¼ˆè‡ªåŠ¨å½’æ¡£ï¼‰
            atoms_archived = self.eternal_archive.archive_knowledge_atoms(knowledge_atoms)
            
            if not atoms_archived:
                return {
                    "success": False,
                    "message": "çŸ¥è¯†åŸå­å½’æ¡£å¤±è´¥",
                    "document_id": document_id
                }
            
            # ç¬¬å››æ­¥ï¼šæ„å»ºè®°å¿†æ˜Ÿå›¾ï¼ˆå…³ç³»æå–å’ŒçŸ¥è¯†å›¾è°±æ„å»ºï¼‰
            nebula_result = self.memory_nebula.process_paragraph_relations(knowledge_atoms, document_id)
            
            # è¿”å›å¤„ç†ç»“æœ
            result = {
                "success": True,
                "message": f"æ–‡æ¡£æ‘„å–æˆåŠŸï¼ç”Ÿæˆäº† {len(knowledge_atoms)} ä¸ªçŸ¥è¯†åŸå­",
                "document_id": document_id,
                "filename": filename,
                "knowledge_atoms_count": len(knowledge_atoms),
                "knowledge_triplets_count": nebula_result.get("total_triplets", 0),
                "memory_nebula_stats": nebula_result.get("graph_stats", {}),
                "processing_time": datetime.now().isoformat()
            }
            
            logger.info(f"æ–‡æ¡£æ‘„å–å®Œæˆ: {filename} -> {len(knowledge_atoms)} ä¸ªçŸ¥è¯†åŸå­")
            return result
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ‘„å–å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ–‡æ¡£æ‘„å–å¤±è´¥: {str(e)}",
                "document_id": document_id if 'document_id' in locals() else None
            }
    
    def query_intelligence(self, query: str, top_k: int = 10, 
                          document_id: str = None) -> Dict[str, Any]:
        """æŸ¥è¯¢ä¸­å¤®æƒ…æŠ¥å¤§è„‘"""
        try:
            logger.info(f"æ”¶åˆ°æ™ºèƒ½æŸ¥è¯¢: {query[:50]}...")
            
            # æœç´¢ç›¸å…³çŸ¥è¯†åŸå­
            search_results = self.eternal_archive.search_knowledge_atoms(
                query=query,
                top_k=top_k,
                document_id=document_id
            )
            
            if not search_results:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°ç›¸å…³çš„çŸ¥è¯†åŸå­",
                    "query": query,
                    "results": []
                }
            
            # æ„å»ºæ™ºèƒ½å›ç­”ä¸Šä¸‹æ–‡
            context_parts = []
            for result in search_results:
                context_parts.append(f"[ç›¸ä¼¼åº¦: {result['similarity_score']:.3f}] {result['content']}")
            
            context = "\n\n".join(context_parts)
            
            return {
                "success": True,
                "message": f"æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³çŸ¥è¯†åŸå­",
                "query": query,
                "results": search_results,
                "context": context,
                "query_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "query": query,
                "results": []
            }
    
    def query_knowledge_graph(self, entity: str, max_depth: int = 2) -> Dict[str, Any]:
        """åŸºäºçŸ¥è¯†å›¾è°±çš„å…³è”æŸ¥è¯¢"""
        try:
            logger.info(f"æ‰§è¡ŒçŸ¥è¯†å›¾è°±æŸ¥è¯¢: {entity}")
            
            # ä½¿ç”¨è®°å¿†æ˜Ÿå›¾è¿›è¡Œå…³è”æŸ¥è¯¢
            nebula_result = self.memory_nebula.query_related_knowledge(entity, max_depth)
            
            if nebula_result["success"]:
                # åŒæ—¶è¿›è¡Œå‘é‡æ£€ç´¢ä½œä¸ºè¡¥å……
                vector_results = self.eternal_archive.search_knowledge_atoms(entity, top_k=5)
                
                return {
                    "success": True,
                    "query_entity": entity,
                    "graph_relations": nebula_result.get("related_entities", []),
                    "vector_matches": vector_results,
                    "total_graph_relations": nebula_result.get("total_found", 0),
                    "query_time": datetime.now().isoformat()
                }
            else:
                return nebula_result
                
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "query_entity": entity
            }
    
    def protected_query_intelligence(self, query: str, top_k: int = 10, 
                                   enable_reranking: bool = True) -> Dict[str, Any]:
        """å—ä¿æŠ¤çš„æ™ºèƒ½æŸ¥è¯¢ - ä½¿ç”¨ç§©åºä¹‹ç›¾"""
        try:
            logger.info(f"æ‰§è¡Œå—ä¿æŠ¤æŸ¥è¯¢: {query[:50]}...")
            
            # ä½¿ç”¨ç§©åºä¹‹ç›¾è¿›è¡Œå—ä¿æŠ¤æ£€ç´¢
            shields_result = self.shields_of_order.protected_retrieve(
                query=query,
                top_k=top_k,
                enable_reranking=enable_reranking
            )
            
            if shields_result["success"]:
                return {
                    "success": True,
                    "message": "ç§©åºä¹‹ç›¾ä¿æŠ¤ä¸‹çš„æŸ¥è¯¢å®Œæˆ",
                    "query": query,
                    "results": shields_result["results"],
                    "complexity_analysis": shields_result.get("complexity_analysis", {}),
                    "processing_time": shields_result.get("processing_time", 0),
                    "retrieval_strategy": shields_result.get("retrieval_strategy", "unknown"),
                    "reranking_enabled": enable_reranking,
                    "shields_protection": True,
                    "query_time": datetime.now().isoformat()
                }
            else:
                return shields_result
                
        except Exception as e:
            logger.error(f"å—ä¿æŠ¤æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "shields_protection": False
            }
    
    @intelligence_brain_self_healing(severity=FailureSeverity.HIGH, max_retries=3)
    def fire_controlled_query(self, query: str, search_scope: str = None, 
                            target_id: str = None, top_k: int = 10,
                            enable_reranking: bool = True) -> Dict[str, Any]:
        """ç«æ§ç³»ç»Ÿæ§åˆ¶çš„æŸ¥è¯¢ - AIæ³¨æ„åŠ›ç²¾ç¡®æ§åˆ¶"""
        try:
            logger.info(f"ğŸ¯ æ‰§è¡Œç«æ§æŸ¥è¯¢: {query[:50]}... (èŒƒå›´: {search_scope})")
            
            # è®¾ç½®æ³¨æ„åŠ›ç›®æ ‡
            if search_scope:
                try:
                    scope_enum = SearchScope(search_scope)
                    target_result = self.fire_control_system.set_attention_target(
                        scope=scope_enum,
                        target_id=target_id
                    )
                    
                    if not target_result["success"]:
                        return target_result
                        
                except ValueError:
                    return {
                        "success": False,
                        "message": f"æ— æ•ˆçš„æœç´¢èŒƒå›´: {search_scope}",
                        "available_scopes": [s["value"] for s in self.fire_control_system.get_available_scopes()]
                    }
            
            # è·å–æ£€ç´¢ç­–ç•¥
            strategy = self.fire_control_system.get_retrieval_strategy(query)
            
            # æ ¹æ®ç­–ç•¥æ‰§è¡Œæ£€ç´¢
            if strategy["scope"] == SearchScope.CURRENT_CHAT.value:
                results = self._execute_chat_retrieval(query, strategy, top_k)
            elif strategy["scope"] == SearchScope.CURRENT_DOCUMENT.value:
                results = self._execute_document_retrieval(query, strategy, top_k)
            elif strategy["scope"] == SearchScope.FULL_DATABASE.value:
                results = self._execute_database_retrieval(query, strategy, top_k, enable_reranking)
            elif strategy["scope"] == SearchScope.GOD_MODE.value:
                results = self._execute_god_mode_retrieval(query, strategy, top_k)
            else:
                # å¤‡ç”¨ç­–ç•¥
                results = self._execute_database_retrieval(query, strategy, top_k, enable_reranking)
            
            return {
                "success": True,
                "message": f"ç«æ§ç³»ç»ŸæŸ¥è¯¢å®Œæˆ - æ³¨æ„åŠ›èšç„¦äº{self.fire_control_system._get_scope_description(SearchScope(strategy['scope']))}",
                "query": query,
                "search_scope": strategy["scope"],
                "target_id": target_id,
                "results": results.get("results", []),
                "fire_control_strategy": strategy,
                "processing_time": results.get("processing_time", 0),
                "fire_control_active": True,
                "query_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ç«æ§æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "search_scope": search_scope,
                "fire_control_active": False
            }
    
    def _execute_chat_retrieval(self, query: str, strategy: Dict, top_k: int) -> Dict[str, Any]:
        """æ‰§è¡ŒèŠå¤©èŒƒå›´æ£€ç´¢"""
        try:
            # ä»èŠå¤©å†å²ä¸­æ£€ç´¢
            chat_history = self.fire_control_system.chat_history
            
            if not chat_history:
                return {
                    "results": [],
                    "message": "å½“å‰èŠå¤©å†å²ä¸ºç©º",
                    "processing_time": 0
                }
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆå¯ä»¥åç»­ä¼˜åŒ–ä¸ºå‘é‡æœç´¢ï¼‰
            query_words = set(query.lower().split())
            scored_messages = []
            
            for msg in chat_history[-strategy["filters"]["limit"]:]:
                content = msg.get("content", "").lower()
                content_words = set(content.split())
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                overlap = len(query_words.intersection(content_words))
                if overlap > 0:
                    score = overlap / len(query_words.union(content_words))
                    scored_messages.append({
                        "content": msg.get("content", ""),
                        "similarity_score": score,
                        "metadata": {
                            "timestamp": msg.get("timestamp"),
                            "role": msg.get("role"),
                            "source": "chat_history"
                        }
                    })
            
            # æ’åºå¹¶è¿”å›å‰top_kä¸ª
            scored_messages.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            return {
                "results": scored_messages[:top_k],
                "processing_time": 0.1,
                "total_searched": len(chat_history)
            }
            
        except Exception as e:
            logger.error(f"èŠå¤©æ£€ç´¢å¤±è´¥: {e}")
            return {"results": [], "error": str(e)}
    
    def _execute_document_retrieval(self, query: str, strategy: Dict, top_k: int) -> Dict[str, Any]:
        """æ‰§è¡Œæ–‡æ¡£èŒƒå›´æ£€ç´¢"""
        try:
            document_id = strategy.get("target_id")
            if not document_id:
                return {
                    "results": [],
                    "message": "æœªæŒ‡å®šç›®æ ‡æ–‡æ¡£",
                    "processing_time": 0
                }
            
            # ä½¿ç”¨æ°¸æ’å½’æ¡£è¿›è¡Œæ–‡æ¡£å†…æ£€ç´¢
            # æ·»åŠ æ–‡æ¡£IDè¿‡æ»¤å™¨
            results = self.eternal_archive.search_knowledge_atoms(
                query, 
                top_k=top_k,
                filters={"document_id": document_id}
            )
            
            return {
                "results": results,
                "processing_time": 0.5,
                "document_id": document_id
            }
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return {"results": [], "error": str(e)}
    
    def _execute_database_retrieval(self, query: str, strategy: Dict, top_k: int, 
                                  enable_reranking: bool) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨æ•°æ®åº“æ£€ç´¢"""
        try:
            if enable_reranking and strategy.get("use_shields"):
                # ä½¿ç”¨ç§©åºä¹‹ç›¾è¿›è¡Œå—ä¿æŠ¤æ£€ç´¢
                return self.shields_of_order.protected_retrieve(
                    query=query,
                    top_k=top_k,
                    enable_reranking=enable_reranking
                )
            else:
                # ç›´æ¥ä½¿ç”¨æ°¸æ’å½’æ¡£
                results = self.eternal_archive.search_knowledge_atoms(query, top_k=top_k)
                return {
                    "results": results,
                    "processing_time": 1.0
                }
                
        except Exception as e:
            logger.error(f"æ•°æ®åº“æ£€ç´¢å¤±è´¥: {e}")
            return {"results": [], "error": str(e)}
    
    def _execute_god_mode_retrieval(self, query: str, strategy: Dict, top_k: int) -> Dict[str, Any]:
        """æ‰§è¡Œç¥ä¹‹æ¡£ä½æ£€ç´¢ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
        try:
            logger.info("ğŸ”¥ ç¥ä¹‹æ¡£ä½æ£€ç´¢ - å½“å‰æ–‡æ¡£å¯¹æ¯”å…¨æ•°æ®åº“")
            
            # è¿™æ˜¯é¢„ç•™çš„æ¥å£ï¼Œæœªæ¥å®ç°æ–‡æ¡£å¯¹æ¯”å…¨æ•°æ®åº“çš„ç»ˆææ´å¯Ÿ
            return {
                "results": [],
                "message": "ç¥ä¹‹æ¡£ä½åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...",
                "processing_time": 0,
                "god_mode_preview": True,
                "future_capabilities": [
                    "å½“å‰æ–‡æ¡£ä¸å…¨æ•°æ®åº“çš„æ·±åº¦å¯¹æ¯”åˆ†æ",
                    "è‡ªåŠ¨å‘ç°æ–‡æ¡£ä¸­çš„ç‹¬ç‰¹è§è§£",
                    "è·¨æ–‡æ¡£å…³è”æ€§åˆ†æ",
                    "æ™ºèƒ½æ´å¯Ÿç”Ÿæˆ"
                ]
            }
            
        except Exception as e:
            logger.error(f"ç¥ä¹‹æ¡£ä½æ£€ç´¢å¤±è´¥: {e}")
            return {"results": [], "error": str(e)}
    
    async def check_chronicle_federation_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥Chronicleè”é‚¦å¥åº·çŠ¶æ€"""
        try:
            logger.info("ğŸ¥ æ£€æŸ¥Chronicleè”é‚¦å¥åº·çŠ¶æ€...")
            
            # ä½¿ç”¨Chronicleè”é‚¦å¥åº·æ£€æŸ¥
            health_status = await check_chronicle_federation_health()
            
            return {
                "success": True,
                "message": "Chronicleè”é‚¦å¥åº·æ£€æŸ¥å®Œæˆ",
                "federation_status": health_status,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Chronicleè”é‚¦å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "federation_status": "error"
            }
    
    async def get_chronicle_healing_statistics(self) -> Dict[str, Any]:
        """è·å–Chronicleè”é‚¦æ²»ç–—ç»Ÿè®¡"""
        try:
            from core.chronicle_client import get_chronicle_client
            client = get_chronicle_client()
            health_report = await client.get_health_report(source="intelligence_brain")
            
            return {
                "success": True,
                "message": "Chronicleæ²»ç–—ç»Ÿè®¡è·å–æˆåŠŸ",
                "healing_statistics": health_report,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"è·å–Chronicleæ²»ç–—ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e), "success": False}
    
    def get_brain_status(self) -> Dict[str, Any]:
        """è·å–å¤§è„‘çŠ¶æ€"""
        try:
            stats = self.eternal_archive.get_document_stats()
            nebula_status = self.memory_nebula.get_nebula_status()
            shields_status = self.shields_of_order.get_shields_status()
            fire_control_status = self.fire_control_system.get_fire_control_status()
            
            return {
                "status": "operational",
                "brain_version": "2.0.0-Chronicle-Federation",
                "architecture": "Trinity Smart Chunking + Memory Nebula + Shields of Order + Fire Control System + Chronicle Federation",
                "capabilities": [
                    "é•¿æœŸè®°å¿†",
                    "æ·±åº¦ç†è§£", 
                    "è‡ªæˆ‘ä¿®å¤",
                    "ç²¾å‡†æ§åˆ¶",
                    "å…³ç³»æå–",
                    "çŸ¥è¯†å›¾è°±æ„å»º",
                    "å®ä½“å…³è”æŸ¥è¯¢",
                    "äºŒçº§ç²¾ç‚¼æœºåˆ¶",
                    "æ˜Ÿå›¾å¯¼èˆªç­–ç•¥",
                    "é˜²è…çƒ‚æœºåˆ¶",
                    "AIæ³¨æ„åŠ›æ§åˆ¶",
                    "ä¸‰æ®µå¼æ‹¨ç›˜",
                    "ç«æ§ç³»ç»Ÿ",
                    "Chronicleè”é‚¦æ²»ç–—",
                    "ä¸­å¤®åŒ»é™¢æ±‚æ•‘",
                    "æ•…éšœè®°å½•å§”æ‰˜",
                    "æ²»ç–—æ–¹æ¡ˆè·å–",
                    "è”é‚¦å…ç–«ç³»ç»Ÿ"
                ],
                "statistics": stats,
                "memory_nebula": nebula_status,
                "shields_of_order": shields_status,
                "fire_control_system": fire_control_status,
                "chronicle_federation": "connected",
                "last_check": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–å¤§è„‘çŠ¶æ€å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }