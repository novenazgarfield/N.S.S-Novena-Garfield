"""
ğŸ›¡ï¸ ç§©åºä¹‹ç›¾ (Shields of Order)
===============================

å®ç°"å¤§å®ªç« "ç¬¬ä¸‰ç« ï¼šçŸ¥è¯†çš„"å®ˆæŠ¤"
- äºŒçº§ç²¾ç‚¼æœºåˆ¶ (Re-Ranking)
- æ˜Ÿå›¾å¯¼èˆªç­–ç•¥ (Hierarchical Retrieval)
- é˜²è…çƒ‚æœºåˆ¶ (Anti-Entropy)

Author: N.S.S-Novena-Garfield Project
Version: 2.0.0 - "Genesis" Chapter 3
"""

import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import hashlib
import sqlite3
from pathlib import Path
import json
import re

from utils.logger import logger
from config import StorageConfig

from sentence_transformers import SentenceTransformer, CrossEncoder

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    content: str
    metadata: Dict[str, Any]
    initial_score: float
    rerank_score: Optional[float] = None
    retrieval_level: str = "atom"  # atom, paragraph, document
    source_document: str = ""
    relevance_explanation: str = ""
    
    def get_final_score(self) -> float:
        """è·å–æœ€ç»ˆè¯„åˆ†"""
        return self.rerank_score if self.rerank_score is not None else self.initial_score

@dataclass
class QueryComplexity:
    """æŸ¥è¯¢å¤æ‚åº¦åˆ†æ"""
    complexity_level: str  # simple, medium, complex
    requires_hierarchical: bool
    suggested_strategy: str
    confidence: float
    analysis_details: Dict[str, Any]

class CrossEncoderReRanker:
    """Cross-Encoderé‡æ’åºå™¨ - äºŒçº§ç²¾ç‚¼æœºåˆ¶"""
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.model_name = model_name
        self.cross_encoder = None
        self._load_model()
        logger.info("ğŸ”„ Cross-Encoderé‡æ’åºå™¨å·²åˆå§‹åŒ–")
    
    def _load_model(self):
        """åŠ è½½Cross-Encoderæ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½Cross-Encoderæ¨¡å‹: {self.model_name}")
            self.cross_encoder = CrossEncoder(self.model_name)
            logger.info("Cross-Encoderæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"Cross-Encoderæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
            try:
                backup_model = "cross-encoder/ms-marco-TinyBERT-L-2-v2"
                logger.info(f"å°è¯•åŠ è½½å¤‡ç”¨æ¨¡å‹: {backup_model}")
                self.cross_encoder = CrossEncoder(backup_model)
                self.model_name = backup_model
                logger.info("å¤‡ç”¨Cross-Encoderæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e2:
                logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                self.cross_encoder = None
    
    def rerank_results(self, query: str, results: List[RetrievalResult], 
                      top_k: int = None) -> List[RetrievalResult]:
        """å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº"""
        try:
            if not self.cross_encoder or not results:
                logger.warning("Cross-EncoderæœªåŠ è½½æˆ–ç»“æœä¸ºç©ºï¼Œä½¿ç”¨å¤‡ç”¨é‡æ’åºæ–¹æ³•")
                return self._fallback_rerank(query, results, top_k)
            
            if top_k is None:
                top_k = len(results)
            
            logger.info(f"å¼€å§‹å¯¹ {len(results)} ä¸ªç»“æœè¿›è¡ŒCross-Encoderé‡æ’åº")
            
            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
            query_doc_pairs = []
            for result in results:
                query_doc_pairs.append([query, result.content])
            
            # è®¡ç®—Cross-Encoderåˆ†æ•°
            rerank_scores = self.cross_encoder.predict(query_doc_pairs)
            
            # æ›´æ–°ç»“æœçš„é‡æ’åºåˆ†æ•°
            for i, result in enumerate(results):
                result.rerank_score = float(rerank_scores[i])
                result.relevance_explanation = f"Cross-Encoderè¯„åˆ†: {result.rerank_score:.4f}"
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            reranked_results = sorted(results, key=lambda x: x.get_final_score(), reverse=True)
            
            logger.info(f"é‡æ’åºå®Œæˆï¼Œè¿”å›å‰ {min(top_k, len(reranked_results))} ä¸ªç»“æœ")
            return reranked_results[:top_k]
            
        except Exception as e:
            logger.error(f"é‡æ’åºå¤±è´¥: {e}")
            return self._fallback_rerank(query, results, top_k)
    
    def _fallback_rerank(self, query: str, results: List[RetrievalResult], 
                        top_k: int = None) -> List[RetrievalResult]:
        """å¤‡ç”¨é‡æ’åºæ–¹æ³•ï¼ˆåŸºäºç®€å•çš„æ–‡æœ¬åŒ¹é…ï¼‰"""
        try:
            if top_k is None:
                top_k = len(results)
            
            logger.info("ä½¿ç”¨å¤‡ç”¨é‡æ’åºæ–¹æ³•")
            
            query_words = set(query.lower().split())
            
            # è®¡ç®—ç®€å•çš„åŒ¹é…åˆ†æ•°
            for result in results:
                content_words = set(result.content.lower().split())
                
                # è®¡ç®—è¯æ±‡é‡å åº¦
                overlap = len(query_words.intersection(content_words))
                total_words = len(query_words.union(content_words))
                
                if total_words > 0:
                    overlap_score = overlap / total_words
                else:
                    overlap_score = 0
                
                # ç»“åˆåˆå§‹åˆ†æ•°å’Œé‡å åˆ†æ•°
                result.rerank_score = (result.initial_score * 0.7) + (overlap_score * 0.3)
                result.relevance_explanation = f"å¤‡ç”¨é‡æ’åº: {result.rerank_score:.4f} (è¯æ±‡é‡å : {overlap_score:.3f})"
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            reranked_results = sorted(results, key=lambda x: x.get_final_score(), reverse=True)
            
            logger.info(f"å¤‡ç”¨é‡æ’åºå®Œæˆï¼Œè¿”å›å‰ {min(top_k, len(reranked_results))} ä¸ªç»“æœ")
            return reranked_results[:top_k]
            
        except Exception as e:
            logger.error(f"å¤‡ç”¨é‡æ’åºå¤±è´¥: {e}")
            return results[:top_k] if top_k else results

class DocumentSummarizer:
    """æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨"""
    
    def __init__(self, llm_manager=None):
        self.llm_manager = llm_manager
        self.summaries_cache = {}
        self._init_cache_storage()
        logger.info("ğŸ“„ æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨å·²åˆå§‹åŒ–")
    
    def _init_cache_storage(self):
        """åˆå§‹åŒ–æ‘˜è¦ç¼“å­˜å­˜å‚¨"""
        try:
            self.cache_db_path = StorageConfig.MODELS_DATA_DIR / "document_summaries.db"
            self.cache_db_path.parent.mkdir(parents=True, exist_ok=True)
            
            conn = sqlite3.connect(str(self.cache_db_path))
            cursor = conn.cursor()
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS document_summaries (
                    document_id TEXT PRIMARY KEY,
                    content_hash TEXT NOT NULL,
                    summary TEXT NOT NULL,
                    summary_type TEXT DEFAULT 'auto',
                    created_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                )
            """)
            
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_content_hash ON document_summaries (content_hash)")
            
            conn.commit()
            conn.close()
            
            logger.info("æ–‡æ¡£æ‘˜è¦ç¼“å­˜æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ‘˜è¦ç¼“å­˜å¤±è´¥: {e}")
    
    def generate_document_summary(self, document_content: str, document_id: str, 
                                max_length: int = 200) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        try:
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(document_content.encode()).hexdigest()
            
            # æ£€æŸ¥ç¼“å­˜
            cached_summary = self._get_cached_summary(document_id, content_hash)
            if cached_summary:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ–‡æ¡£æ‘˜è¦: {document_id}")
                return cached_summary
            
            # ç”Ÿæˆæ–°æ‘˜è¦
            if self.llm_manager:
                summary = self._generate_llm_summary(document_content, max_length)
            else:
                summary = self._generate_extractive_summary(document_content, max_length)
            
            # ç¼“å­˜æ‘˜è¦
            self._cache_summary(document_id, content_hash, summary)
            
            logger.info(f"æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå®Œæˆ: {document_id}")
            return summary
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æ¡£æ‘˜è¦å¤±è´¥: {e}")
            return self._generate_fallback_summary(document_content, max_length)
    
    def _generate_llm_summary(self, content: str, max_length: int) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦"""
        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œé•¿åº¦ä¸è¶…è¿‡{max_length}å­—ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{content[:2000]}  # é™åˆ¶è¾“å…¥é•¿åº¦

è¦æ±‚ï¼š
1. æå–æ–‡æ¡£çš„æ ¸å¿ƒä¸»é¢˜å’Œå…³é”®ä¿¡æ¯
2. ä¿æŒæ‘˜è¦çš„å®Œæ•´æ€§å’Œå¯è¯»æ€§
3. çªå‡ºæ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹å’Œç»“è®º
4. ä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€

æ‘˜è¦ï¼š"""
            
            summary = self.llm_manager.generate_response(prompt)
            
            # æ¸…ç†å’Œæˆªæ–­æ‘˜è¦
            summary = summary.strip()
            if len(summary) > max_length:
                summary = summary[:max_length] + "..."
            
            return summary
            
        except Exception as e:
            logger.error(f"LLMæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_extractive_summary(content, max_length)
    
    def _generate_extractive_summary(self, content: str, max_length: int) -> str:
        """ç”ŸæˆæŠ½å–å¼æ‘˜è¦"""
        try:
            # ç®€å•çš„æŠ½å–å¼æ‘˜è¦ï¼šå–å‰å‡ å¥è¯
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            summary = ""
            for sentence in sentences:
                if len(summary + sentence) <= max_length:
                    summary += sentence + "ã€‚"
                else:
                    break
            
            return summary.strip() if summary else content[:max_length] + "..."
            
        except Exception as e:
            logger.error(f"æŠ½å–å¼æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return content[:max_length] + "..."
    
    def _generate_fallback_summary(self, content: str, max_length: int) -> str:
        """ç”Ÿæˆå¤‡ç”¨æ‘˜è¦"""
        return content[:max_length] + "..." if len(content) > max_length else content
    
    def _get_cached_summary(self, document_id: str, content_hash: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„æ‘˜è¦"""
        try:
            conn = sqlite3.connect(str(self.cache_db_path))
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT summary FROM document_summaries 
                WHERE document_id = ? AND content_hash = ?
            """, (document_id, content_hash))
            
            result = cursor.fetchone()
            conn.close()
            
            return result[0] if result else None
            
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜æ‘˜è¦å¤±è´¥: {e}")
            return None
    
    def _cache_summary(self, document_id: str, content_hash: str, summary: str):
        """ç¼“å­˜æ‘˜è¦"""
        try:
            conn = sqlite3.connect(str(self.cache_db_path))
            cursor = conn.cursor()
            
            current_time = datetime.now().isoformat()
            
            cursor.execute("""
                INSERT OR REPLACE INTO document_summaries 
                (document_id, content_hash, summary, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?)
            """, (document_id, content_hash, summary, current_time, current_time))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ç¼“å­˜æ‘˜è¦å¤±è´¥: {e}")

class QueryComplexityAnalyzer:
    """æŸ¥è¯¢å¤æ‚åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.complexity_patterns = {
            'simple': [
                r'^.{1,20}$',  # å¾ˆçŸ­çš„æŸ¥è¯¢
                r'^\w+$',      # å•ä¸ªè¯
            ],
            'medium': [
                r'^.{21,50}$',  # ä¸­ç­‰é•¿åº¦
                r'\b(ä»€ä¹ˆ|å¦‚ä½•|ä¸ºä»€ä¹ˆ|æ€ä¹ˆ)\b',  # ç®€å•ç–‘é—®è¯
            ],
            'complex': [
                r'^.{51,}$',    # é•¿æŸ¥è¯¢
                r'\b(æ¯”è¾ƒ|åˆ†æ|è¯„ä¼°|ç»¼åˆ|å…³ç³»|å½±å“)\b',  # å¤æ‚æ¦‚å¿µ
                r'[ï¼Œã€‚ï¼›ï¼š]',   # åŒ…å«æ ‡ç‚¹ç¬¦å·
                r'\b(å’Œ|ä¸|æˆ–|ä½†æ˜¯|ç„¶è€Œ|å› æ­¤)\b',  # é€»è¾‘è¿æ¥è¯
            ]
        }
        logger.info("ğŸ” æŸ¥è¯¢å¤æ‚åº¦åˆ†æå™¨å·²åˆå§‹åŒ–")
    
    def analyze_query_complexity(self, query: str) -> QueryComplexity:
        """åˆ†ææŸ¥è¯¢å¤æ‚åº¦"""
        try:
            query = query.strip()
            
            # åŸºç¡€ç‰¹å¾åˆ†æ
            word_count = len(query.split())
            char_count = len(query)
            has_punctuation = bool(re.search(r'[ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ]', query))
            has_logical_words = bool(re.search(r'\b(å’Œ|ä¸|æˆ–|ä½†æ˜¯|ç„¶è€Œ|å› æ­¤|æ¯”è¾ƒ|åˆ†æ)\b', query))
            has_question_words = bool(re.search(r'\b(ä»€ä¹ˆ|å¦‚ä½•|ä¸ºä»€ä¹ˆ|æ€ä¹ˆ|å“ªä¸ª|å“ªäº›)\b', query))
            
            # å¤æ‚åº¦è¯„åˆ†
            complexity_score = 0
            
            # é•¿åº¦å› å­
            if char_count > 50:
                complexity_score += 2
            elif char_count > 20:
                complexity_score += 1
            
            # è¯æ±‡å¤æ‚åº¦
            if word_count > 10:
                complexity_score += 2
            elif word_count > 5:
                complexity_score += 1
            
            # è¯­è¨€ç‰¹å¾
            if has_logical_words:
                complexity_score += 2
            if has_punctuation:
                complexity_score += 1
            if has_question_words:
                complexity_score += 1
            
            # ç¡®å®šå¤æ‚åº¦çº§åˆ«
            if complexity_score >= 5:
                complexity_level = "complex"
                requires_hierarchical = True
                suggested_strategy = "hierarchical_macro_to_micro"
            elif complexity_score >= 2:
                complexity_level = "medium"
                requires_hierarchical = True
                suggested_strategy = "enhanced_retrieval"
            else:
                complexity_level = "simple"
                requires_hierarchical = False
                suggested_strategy = "direct_retrieval"
            
            # ç½®ä¿¡åº¦è®¡ç®—
            confidence = min(0.9, 0.5 + complexity_score * 0.1)
            
            analysis_details = {
                "word_count": word_count,
                "char_count": char_count,
                "complexity_score": complexity_score,
                "has_punctuation": has_punctuation,
                "has_logical_words": has_logical_words,
                "has_question_words": has_question_words
            }
            
            result = QueryComplexity(
                complexity_level=complexity_level,
                requires_hierarchical=requires_hierarchical,
                suggested_strategy=suggested_strategy,
                confidence=confidence,
                analysis_details=analysis_details
            )
            
            logger.info(f"æŸ¥è¯¢å¤æ‚åº¦åˆ†æå®Œæˆ: {complexity_level} (ç½®ä¿¡åº¦: {confidence:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤æ‚åº¦åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return QueryComplexity(
                complexity_level="medium",
                requires_hierarchical=True,
                suggested_strategy="enhanced_retrieval",
                confidence=0.5,
                analysis_details={"error": str(e)}
            )

class HierarchicalRetriever:
    """åˆ†å±‚æ£€ç´¢å™¨ - æ˜Ÿå›¾å¯¼èˆªç­–ç•¥"""
    
    def __init__(self, eternal_archive, memory_nebula, document_summarizer):
        self.eternal_archive = eternal_archive
        self.memory_nebula = memory_nebula
        self.document_summarizer = document_summarizer
        self.document_summaries = {}
        logger.info("ğŸ—ºï¸ åˆ†å±‚æ£€ç´¢å™¨å·²åˆå§‹åŒ–")
    
    def hierarchical_retrieve(self, query: str, complexity: QueryComplexity, 
                            top_k: int = 10) -> List[RetrievalResult]:
        """æ‰§è¡Œåˆ†å±‚æ£€ç´¢"""
        try:
            logger.info(f"å¼€å§‹åˆ†å±‚æ£€ç´¢: {complexity.suggested_strategy}")
            
            if complexity.suggested_strategy == "hierarchical_macro_to_micro":
                return self._macro_to_micro_retrieval(query, top_k)
            elif complexity.suggested_strategy == "enhanced_retrieval":
                return self._enhanced_retrieval(query, top_k)
            else:
                return self._direct_retrieval(query, top_k)
                
        except Exception as e:
            logger.error(f"åˆ†å±‚æ£€ç´¢å¤±è´¥: {e}")
            return self._direct_retrieval(query, top_k)
    
    def _macro_to_micro_retrieval(self, query: str, top_k: int) -> List[RetrievalResult]:
        """å®è§‚åˆ°å¾®è§‚çš„åˆ†å±‚æ£€ç´¢"""
        try:
            logger.info("æ‰§è¡Œå®è§‚åˆ°å¾®è§‚æ£€ç´¢ç­–ç•¥")
            
            # ç¬¬ä¸€æ­¥ï¼šæ–‡æ¡£çº§æ£€ç´¢ï¼ˆå®è§‚å±‚é¢ï¼‰
            document_results = self._retrieve_at_document_level(query, top_k * 2)
            
            if not document_results:
                logger.warning("æ–‡æ¡£çº§æ£€ç´¢æ— ç»“æœï¼Œé™çº§åˆ°ç›´æ¥æ£€ç´¢")
                return self._direct_retrieval(query, top_k)
            
            # ç¬¬äºŒæ­¥ï¼šåœ¨é«˜ç›¸å…³æ€§æ–‡æ¡£å†…è¿›è¡Œå¾®è§‚æ£€ç´¢
            micro_results = []
            for doc_result in document_results[:min(5, len(document_results))]:  # é™åˆ¶æ–‡æ¡£æ•°é‡
                doc_id = doc_result.metadata.get('document_id')
                if doc_id:
                    # åœ¨ç‰¹å®šæ–‡æ¡£å†…æ£€ç´¢å¥å­
                    sentence_results = self._retrieve_sentences_in_document(query, doc_id, top_k // 2)
                    micro_results.extend(sentence_results)
            
            # åˆå¹¶å’Œæ’åºç»“æœ
            all_results = document_results + micro_results
            
            # å»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
            unique_results = self._deduplicate_results(all_results)
            
            logger.info(f"å®è§‚åˆ°å¾®è§‚æ£€ç´¢å®Œæˆ: {len(unique_results)} ä¸ªç»“æœ")
            return unique_results[:top_k]
            
        except Exception as e:
            logger.error(f"å®è§‚åˆ°å¾®è§‚æ£€ç´¢å¤±è´¥: {e}")
            return self._direct_retrieval(query, top_k)
    
    def _enhanced_retrieval(self, query: str, top_k: int) -> List[RetrievalResult]:
        """å¢å¼ºæ£€ç´¢ç­–ç•¥"""
        try:
            logger.info("æ‰§è¡Œå¢å¼ºæ£€ç´¢ç­–ç•¥")
            
            # ç»“åˆå‘é‡æ£€ç´¢å’Œå›¾è°±æ£€ç´¢
            vector_results = self._direct_retrieval(query, top_k)
            
            # å°è¯•å›¾è°±æ£€ç´¢
            try:
                graph_results = self._retrieve_from_knowledge_graph(query, top_k // 2)
                all_results = vector_results + graph_results
            except Exception as e:
                logger.warning(f"å›¾è°±æ£€ç´¢å¤±è´¥: {e}")
                all_results = vector_results
            
            # å»é‡å’Œæ’åº
            unique_results = self._deduplicate_results(all_results)
            
            logger.info(f"å¢å¼ºæ£€ç´¢å®Œæˆ: {len(unique_results)} ä¸ªç»“æœ")
            return unique_results[:top_k]
            
        except Exception as e:
            logger.error(f"å¢å¼ºæ£€ç´¢å¤±è´¥: {e}")
            return self._direct_retrieval(query, top_k)
    
    def _direct_retrieval(self, query: str, top_k: int) -> List[RetrievalResult]:
        """ç›´æ¥æ£€ç´¢ç­–ç•¥"""
        try:
            logger.info("æ‰§è¡Œç›´æ¥æ£€ç´¢ç­–ç•¥")
            
            # ä½¿ç”¨æ°¸æ’å½’æ¡£è¿›è¡Œå‘é‡æ£€ç´¢
            search_results = self.eternal_archive.search_knowledge_atoms(query, top_k)
            
            results = []
            for result in search_results:
                retrieval_result = RetrievalResult(
                    content=result["content"],
                    metadata=result["metadata"],
                    initial_score=result["similarity_score"],
                    retrieval_level="atom",
                    source_document=result["metadata"].get("document_id", ""),
                    relevance_explanation=f"å‘é‡ç›¸ä¼¼åº¦: {result['similarity_score']:.4f}"
                )
                results.append(retrieval_result)
            
            logger.info(f"ç›´æ¥æ£€ç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"ç›´æ¥æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _retrieve_at_document_level(self, query: str, top_k: int) -> List[RetrievalResult]:
        """æ–‡æ¡£çº§æ£€ç´¢"""
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£çš„æ‘˜è¦
            document_stats = self.eternal_archive.get_document_stats()
            # è¿™é‡Œéœ€è¦å®ç°æ–‡æ¡£çº§æ£€ç´¢é€»è¾‘
            # æš‚æ—¶è¿”å›ç©ºç»“æœï¼Œå®é™…å®ç°éœ€è¦æ ¹æ®æ–‡æ¡£æ‘˜è¦è¿›è¡Œæ£€ç´¢
            return []
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£çº§æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _retrieve_sentences_in_document(self, query: str, document_id: str, top_k: int) -> List[RetrievalResult]:
        """åœ¨ç‰¹å®šæ–‡æ¡£å†…æ£€ç´¢å¥å­"""
        try:
            # ä½¿ç”¨æ–‡æ¡£IDè¿‡æ»¤æ£€ç´¢ç»“æœ
            search_results = self.eternal_archive.search_knowledge_atoms(query, top_k * 2, document_id)
            
            results = []
            for result in search_results:
                if result["metadata"].get("document_id") == document_id:
                    retrieval_result = RetrievalResult(
                        content=result["content"],
                        metadata=result["metadata"],
                        initial_score=result["similarity_score"],
                        retrieval_level="atom",
                        source_document=document_id,
                        relevance_explanation=f"æ–‡æ¡£å†…æ£€ç´¢: {result['similarity_score']:.4f}"
                    )
                    results.append(retrieval_result)
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£å†…å¥å­æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _retrieve_from_knowledge_graph(self, query: str, top_k: int) -> List[RetrievalResult]:
        """ä»çŸ¥è¯†å›¾è°±æ£€ç´¢"""
        try:
            # æå–æŸ¥è¯¢ä¸­çš„å®ä½“
            entities = self._extract_entities_from_query(query)
            
            results = []
            for entity in entities:
                graph_result = self.memory_nebula.query_related_knowledge(entity, max_depth=2)
                
                if graph_result["success"]:
                    for related in graph_result.get("related_entities", []):
                        retrieval_result = RetrievalResult(
                            content=f"å®ä½“å…³è”: {entity} -> {related['entity']}",
                            metadata={"entity": entity, "related_entity": related["entity"]},
                            initial_score=0.8,  # å›¾è°±æ£€ç´¢çš„åŸºç¡€åˆ†æ•°
                            retrieval_level="graph",
                            source_document="knowledge_graph",
                            relevance_explanation=f"å›¾è°±å…³è”æ·±åº¦: {related['depth']}"
                        )
                        results.append(retrieval_result)
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _extract_entities_from_query(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å®ä½“"""
        # ç®€å•çš„å®ä½“æå–ï¼ˆå¯ä»¥åç»­ä¼˜åŒ–ï¼‰
        words = query.split()
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        entities = [word for word in words if len(word) > 2 and word not in ['ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ']]
        return entities[:3]  # é™åˆ¶å®ä½“æ•°é‡
    
    def _deduplicate_results(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """å»é‡æ£€ç´¢ç»“æœ"""
        seen_content = set()
        unique_results = []
        
        for result in results:
            content_hash = hashlib.md5(result.content.encode()).hexdigest()
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        unique_results.sort(key=lambda x: x.get_final_score(), reverse=True)
        return unique_results

class ShieldsOfOrder:
    """ç§©åºä¹‹ç›¾ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, eternal_archive, memory_nebula, llm_manager=None):
        self.eternal_archive = eternal_archive
        self.memory_nebula = memory_nebula
        self.llm_manager = llm_manager
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.cross_encoder_reranker = CrossEncoderReRanker()
        self.document_summarizer = DocumentSummarizer(llm_manager)
        self.query_complexity_analyzer = QueryComplexityAnalyzer()
        self.hierarchical_retriever = HierarchicalRetriever(
            eternal_archive, memory_nebula, self.document_summarizer
        )
        
        logger.info("ğŸ›¡ï¸ ç§©åºä¹‹ç›¾å·²æ¿€æ´»")
    
    def protected_retrieve(self, query: str, top_k: int = 10, 
                          enable_reranking: bool = True) -> Dict[str, Any]:
        """å—ä¿æŠ¤çš„æ£€ç´¢ - å®Œæ•´çš„ç§©åºä¹‹ç›¾æµç¨‹"""
        try:
            start_time = datetime.now()
            logger.info(f"å¼€å§‹å—ä¿æŠ¤æ£€ç´¢: {query[:50]}...")
            
            # ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢å¤æ‚åº¦åˆ†æ
            complexity = self.query_complexity_analyzer.analyze_query_complexity(query)
            
            # ç¬¬äºŒæ­¥ï¼šåˆ†å±‚æ£€ç´¢ï¼ˆæ˜Ÿå›¾å¯¼èˆªï¼‰
            initial_results = self.hierarchical_retriever.hierarchical_retrieve(
                query, complexity, top_k * 2  # è·å–æ›´å¤šç»“æœç”¨äºé‡æ’åº
            )
            
            if not initial_results:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ",
                    "query": query,
                    "results": [],
                    "complexity_analysis": complexity.__dict__
                }
            
            # ç¬¬ä¸‰æ­¥ï¼šäºŒçº§ç²¾ç‚¼ï¼ˆRe-Rankingï¼‰
            if enable_reranking and len(initial_results) > 1:
                final_results = self.cross_encoder_reranker.rerank_results(
                    query, initial_results, top_k
                )
            else:
                final_results = initial_results[:top_k]
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "success": True,
                "message": f"ç§©åºä¹‹ç›¾æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_results)} ä¸ªé«˜è´¨é‡ç»“æœ",
                "query": query,
                "results": [self._format_result(r) for r in final_results],
                "complexity_analysis": complexity.__dict__,
                "processing_time": processing_time,
                "reranking_enabled": enable_reranking,
                "retrieval_strategy": complexity.suggested_strategy,
                "shields_status": "active"
            }
            
            logger.info(f"å—ä¿æŠ¤æ£€ç´¢å®Œæˆ: {len(final_results)} ä¸ªç»“æœï¼Œè€—æ—¶ {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"å—ä¿æŠ¤æ£€ç´¢å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ£€ç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "query": query,
                "results": [],
                "error": str(e)
            }
    
    def _format_result(self, result: RetrievalResult) -> Dict[str, Any]:
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœ"""
        return {
            "content": result.content,
            "metadata": result.metadata,
            "initial_score": result.initial_score,
            "rerank_score": result.rerank_score,
            "final_score": result.get_final_score(),
            "retrieval_level": result.retrieval_level,
            "source_document": result.source_document,
            "relevance_explanation": result.relevance_explanation
        }
    
    def get_shields_status(self) -> Dict[str, Any]:
        """è·å–ç§©åºä¹‹ç›¾çŠ¶æ€"""
        try:
            return {
                "status": "active",
                "shields_version": "2.0.0-Genesis-Chapter3",
                "components": {
                    "cross_encoder_reranker": {
                        "status": "operational" if self.cross_encoder_reranker.cross_encoder else "offline",
                        "model": self.cross_encoder_reranker.model_name
                    },
                    "document_summarizer": {
                        "status": "operational",
                        "llm_enabled": self.document_summarizer.llm_manager is not None
                    },
                    "query_complexity_analyzer": {
                        "status": "operational"
                    },
                    "hierarchical_retriever": {
                        "status": "operational"
                    }
                },
                "capabilities": [
                    "äºŒçº§ç²¾ç‚¼æœºåˆ¶",
                    "æ˜Ÿå›¾å¯¼èˆªç­–ç•¥",
                    "æŸ¥è¯¢å¤æ‚åº¦åˆ†æ",
                    "åˆ†å±‚æ£€ç´¢",
                    "é˜²è…çƒ‚æœºåˆ¶"
                ],
                "last_check": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–ç§©åºä¹‹ç›¾çŠ¶æ€å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }