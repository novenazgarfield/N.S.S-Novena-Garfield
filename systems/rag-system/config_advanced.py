"""
é«˜çº§é…ç½® - æ”¯æŒå¤šAPIå’Œåˆ†å¸ƒå¼è®¡ç®—
"""
import os
from pathlib import Path
from typing import Dict, Any, Optional
import json

# åŸºç¡€è·¯å¾„é…ç½®
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR.parent.parent / "data"
SHARED_DIR = BASE_DIR.parent.parent / "shared"

class APIConfig:
    """APIé…ç½®ç®¡ç†"""
    
    # APIç±»å‹æšä¸¾
    API_TYPES = {
        "local": "æœ¬åœ°æ¨¡å‹",
        "modelscope": "é­”æ­API", 
        "openai": "OpenAI API",
        "anthropic": "Claude API",
        "zhipu": "æ™ºè°±API",
        "baidu": "ç™¾åº¦API",
        "alibaba": "é˜¿é‡Œäº‘API"
    }
    
    # é»˜è®¤APIé…ç½®
    DEFAULT_CONFIGS = {
        "local": {
            "model_path": "/models/deepseek-7b-chat-q5.gguf",
            "device": "cuda:0",  # 3090
            "n_ctx": 4096,
            "n_gpu_layers": 30
        },
        "modelscope": {
            "api_key": os.getenv("MODELSCOPE_API_KEY", ""),
            "base_url": "https://dashscope.aliyuncs.com/api/v1",
            "model": "qwen2.5-72b-instruct",
            "max_tokens": 2000
        },
        "openai": {
            "api_key": os.getenv("OPENAI_API_KEY", ""),
            "base_url": "https://api.openai.com/v1",
            "model": "gpt-4o-mini",
            "max_tokens": 2000
        },
        "zhipu": {
            "api_key": os.getenv("ZHIPU_API_KEY", ""),
            "base_url": "https://open.bigmodel.cn/api/paas/v4",
            "model": "glm-4-flash",
            "max_tokens": 2000
        }
    }
    
    @classmethod
    def get_config(cls, api_type: str) -> Dict[str, Any]:
        """è·å–æŒ‡å®šAPIçš„é…ç½®"""
        return cls.DEFAULT_CONFIGS.get(api_type, {})
    
    @classmethod
    def set_config(cls, api_type: str, config: Dict[str, Any]):
        """è®¾ç½®æŒ‡å®šAPIçš„é…ç½®"""
        cls.DEFAULT_CONFIGS[api_type] = config

class DistributedConfig:
    """åˆ†å¸ƒå¼è®¡ç®—é…ç½®"""
    
    # è®¾å¤‡é…ç½®
    DEVICES = {
        "gpu_3090": {
            "device": "cuda:0",
            "memory": "24GB",
            "role": "llm_inference",  # ä¸»è¦è´Ÿè´£å¤§æ¨¡å‹æ¨ç†
            "priority": 1
        },
        "gpu_4060": {
            "device": "cuda:1", 
            "memory": "8GB",
            "role": "embedding_processing",  # è´Ÿè´£åµŒå…¥è®¡ç®—å’Œå…¶ä»–å¤„ç†
            "priority": 2
        }
    }
    
    # ä»»åŠ¡åˆ†é…ç­–ç•¥
    TASK_ALLOCATION = {
        "llm_inference": "gpu_3090",      # LLMæ¨ç† -> 3090
        "embedding": "gpu_4060",          # åµŒå…¥è®¡ç®— -> 4060
        "vector_search": "gpu_4060",      # å‘é‡æœç´¢ -> 4060
        "document_processing": "cpu",     # æ–‡æ¡£å¤„ç† -> CPU
        "memory_management": "cpu"        # è®°å¿†ç®¡ç† -> CPU
    }
    
    @classmethod
    def get_device_for_task(cls, task: str) -> str:
        """è·å–ä»»åŠ¡å¯¹åº”çš„è®¾å¤‡"""
        device_name = cls.TASK_ALLOCATION.get(task, "cpu")
        if device_name in cls.DEVICES:
            return cls.DEVICES[device_name]["device"]
        return device_name

class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    
    # å½“å‰ä½¿ç”¨çš„APIç±»å‹
    CURRENT_API = os.getenv("RAG_API_TYPE", "local")
    
    # åµŒå…¥æ¨¡å‹é…ç½®ï¼ˆåœ¨4060ä¸Šè¿è¡Œï¼‰
    EMBEDDING_MODEL_PATH = "BAAI/bge-large-zh-v1.5"  # ä¸­æ–‡æ•ˆæœæ›´å¥½
    EMBEDDING_DEVICE = DistributedConfig.get_device_for_task("embedding")
    
    # LLMé…ç½®ï¼ˆåœ¨3090ä¸Šè¿è¡Œï¼‰
    LLM_DEVICE = DistributedConfig.get_device_for_task("llm_inference")
    
    @classmethod
    def get_current_config(cls) -> Dict[str, Any]:
        """è·å–å½“å‰APIé…ç½®"""
        return APIConfig.get_config(cls.CURRENT_API)
    
    @classmethod
    def switch_api(cls, api_type: str):
        """åˆ‡æ¢APIç±»å‹"""
        if api_type in APIConfig.API_TYPES:
            cls.CURRENT_API = api_type
            os.environ["RAG_API_TYPE"] = api_type
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„APIç±»å‹: {api_type}")

class StorageConfig:
    """å­˜å‚¨é…ç½®"""
    # åŸºç¡€ç›®å½•
    BASE_DIR = Path(__file__).parent
    DATA_DIR = BASE_DIR.parent.parent / "data"
    SHARED_DIR = BASE_DIR.parent.parent / "shared"
    
    # æ•°æ®ç›®å½•
    RAW_DATA_DIR = DATA_DIR / "raw" / "rag"
    PROCESSED_DATA_DIR = DATA_DIR / "processed" / "rag"
    MODELS_DATA_DIR = DATA_DIR / "models" / "rag"
    RESULTS_DATA_DIR = DATA_DIR / "results" / "rag"
    
    # è®°å¿†ç³»ç»Ÿè·¯å¾„
    MEMORY_DIR = DATA_DIR / "processed" / "rag" / "memory"
    PERMANENT_MEMORY_DIR = MEMORY_DIR / "permanent"
    TEMPORARY_MEMORY_DIR = MEMORY_DIR / "temporary"
    
    # æ•°æ®åº“è·¯å¾„
    DATABASE_DIR = DATA_DIR / "processed" / "rag" / "database"
    CHAT_DB_PATH = DATABASE_DIR / "chat_logs.db"
    
    # æœ¬åœ°æ–‡çŒ®åº“è·¯å¾„
    LIBRARY_DIR = RAW_DATA_DIR / "library"
    
    # æ—¥å¿—è·¯å¾„
    LOG_DIR = DATA_DIR / "processed" / "rag" / "logs"
    LOG_FILE = LOG_DIR / "rag_system.log"

class DocumentConfig:
    """æ–‡æ¡£å¤„ç†é…ç½®"""
    SUPPORTED_EXTENSIONS = [".pdf", ".docx", ".pptx", ".xlsx", ".xls", ".csv", ".txt", ".md", ".html", ".caj"]
    CHUNK_SIZE = 300
    CHUNK_OVERLAP = 50
    MAX_RETRIEVED_CHUNKS = 15
    MAX_SIMILAR_HISTORY = 5

class SystemConfig:
    """ç³»ç»Ÿé…ç½®"""
    DEBUG = True
    LOG_LEVEL = "INFO"
    PAGE_TITLE = "ğŸ§¬ ç»¼åˆç§‘ç ”å·¥ä½œç«™ - RAGæ™ºèƒ½é—®ç­”"
    PAGE_ICON = "ğŸ¤–"

def init_config():
    """åˆå§‹åŒ–é…ç½®"""
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    directories = [
        StorageConfig.RAW_DATA_DIR,
        StorageConfig.PROCESSED_DATA_DIR,
        StorageConfig.MODELS_DATA_DIR,
        StorageConfig.RESULTS_DATA_DIR,
        StorageConfig.MEMORY_DIR,
        StorageConfig.PERMANENT_MEMORY_DIR,
        StorageConfig.TEMPORARY_MEMORY_DIR,
        StorageConfig.DATABASE_DIR,
        StorageConfig.LIBRARY_DIR,
        StorageConfig.LOG_DIR
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
    
    print("RAGç³»ç»Ÿé«˜çº§é…ç½®åˆå§‹åŒ–å®Œæˆ")
    print(f"å½“å‰APIç±»å‹: {ModelConfig.CURRENT_API}")
    print(f"LLMè®¾å¤‡: {ModelConfig.LLM_DEVICE}")
    print(f"åµŒå…¥è®¾å¤‡: {ModelConfig.EMBEDDING_DEVICE}")

def save_config_to_file(config_path: Optional[str] = None):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    if config_path is None:
        config_path = StorageConfig.DATA_DIR / "config.json"
    
    config_data = {
        "api_configs": APIConfig.DEFAULT_CONFIGS,
        "current_api": ModelConfig.CURRENT_API,
        "distributed_config": DistributedConfig.DEVICES,
        "task_allocation": DistributedConfig.TASK_ALLOCATION
    }
    
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config_data, f, indent=2, ensure_ascii=False)
    
    print(f"é…ç½®å·²ä¿å­˜åˆ°: {config_path}")

def load_config_from_file(config_path: Optional[str] = None):
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    if config_path is None:
        config_path = StorageConfig.DATA_DIR / "config.json"
    
    if not Path(config_path).exists():
        print("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return
    
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        
        # æ›´æ–°é…ç½®
        if "api_configs" in config_data:
            APIConfig.DEFAULT_CONFIGS.update(config_data["api_configs"])
        
        if "current_api" in config_data:
            ModelConfig.CURRENT_API = config_data["current_api"]
        
        print(f"é…ç½®å·²ä»æ–‡ä»¶åŠ è½½: {config_path}")
        
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

# è‡ªåŠ¨åˆå§‹åŒ–
if __name__ != "__main__":
    init_config()