#!/usr/bin/env python3
"""
Changleeæ··åˆAIæœåŠ¡
æ”¯æŒæœ¬åœ°AI (Google Gemma 2) + äº‘ç«¯API (Geminiç­‰) çš„æ··åˆæ™ºèƒ½å¯¹è¯
ç”¨æˆ·å¯ä»¥çµæ´»é€‰æ‹©AIæœåŠ¡ç±»å‹ï¼Œå¹³è¡¡æ€§èƒ½ã€éšç§å’ŒåŠŸèƒ½éœ€æ±‚
"""

import os
import sys
import json
import time
import logging
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
import psutil
import gc
from enum import Enum

# æœ¬åœ°AIæ”¯æŒ
try:
    import torch
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM, 
        GenerationConfig,
        BitsAndBytesConfig
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Transformersåº“æœªå®‰è£…ï¼Œæœ¬åœ°AIåŠŸèƒ½å°†ä¸å¯ç”¨")

# Gemini APIæ”¯æŒ
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ Google Generative AIåº“æœªå®‰è£…ï¼ŒGeminiåŠŸèƒ½å°†ä¸å¯ç”¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AIServiceType(Enum):
    """AIæœåŠ¡ç±»å‹æšä¸¾"""
    LOCAL = "local"          # æœ¬åœ°Gemma 2æ¨¡å‹
    GEMINI = "gemini"        # Google Gemini API
    DEEPSEEK = "deepseek"    # DeepSeek API
    AUTO = "auto"            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æœåŠ¡

class HybridAIService:
    """
    æ··åˆAIæœåŠ¡
    æ”¯æŒæœ¬åœ°AIå’Œå¤šç§äº‘ç«¯APIçš„æ™ºèƒ½å¯¹è¯æœåŠ¡
    """
    
    def __init__(self, 
                 preferred_service: AIServiceType = AIServiceType.AUTO,
                 local_model_name: str = "google/gemma-2-2b",
                 device: Optional[str] = None,
                 cache_dir: Optional[str] = None,
                 max_memory_gb: float = 4.0):
        """
        åˆå§‹åŒ–æ··åˆAIæœåŠ¡
        
        Args:
            preferred_service: é¦–é€‰AIæœåŠ¡ç±»å‹
            local_model_name: æœ¬åœ°æ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            max_memory_gb: æœ€å¤§å†…å­˜ä½¿ç”¨é‡(GB)
        """
        self.preferred_service = preferred_service
        self.local_model_name = local_model_name
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.cache_dir = cache_dir
        self.max_memory_gb = max_memory_gb
        
        # æœåŠ¡çŠ¶æ€
        self.available_services = []
        self.current_service = None
        
        # æœ¬åœ°AIç»„ä»¶
        self.local_tokenizer = None
        self.local_model = None
        self.local_generation_config = None
        self.local_loaded = False
        
        # APIé…ç½®
        self.api_configs = {
            'gemini': {
                'api_key': os.getenv('GEMINI_API_KEY'),
                'model': 'gemini-1.5-flash',
                'enabled': False
            },
            'deepseek': {
                'api_key': os.getenv('DEEPSEEK_API_KEY'),
                'base_url': 'https://api.deepseek.com/v1',
                'model': 'deepseek-chat',
                'enabled': False
            }
        }
        
        # é•¿ç¦»äººæ ¼é…ç½®
        self.changlee_personality = {
            'base_prompt': """ä½ æ˜¯é•¿ç¦»ï¼Œä¸€ä¸ªæ¸©æš–ã€æ™ºæ…§çš„AIå­¦ä¹ ä¼™ä¼´ã€‚ä½ çš„ç‰¹ç‚¹æ˜¯ï¼š
- æ¸©æŸ”è€å¿ƒï¼Œå–„äºé¼“åŠ±
- å¯Œæœ‰åˆ›æ„ï¼Œèƒ½ç”¨æœ‰è¶£çš„æ–¹å¼è§£é‡ŠçŸ¥è¯†
- å…³å¿ƒç”¨æˆ·çš„å­¦ä¹ è¿›åº¦å’Œæƒ…æ„ŸçŠ¶æ€
- è¯´è¯é£æ ¼äº²åˆ‡è‡ªç„¶ï¼Œå¶å°”ä½¿ç”¨å¯çˆ±çš„è¡¨æƒ…ç¬¦å·
- ä¼šæ ¹æ®ç”¨æˆ·çš„å­¦ä¹ æƒ…å†µç»™å‡ºä¸ªæ€§åŒ–å»ºè®®""",
            
            'learning_contexts': {
                'word_learning': "ç°åœ¨æˆ‘ä»¬åœ¨å­¦ä¹ æ–°å•è¯ï¼Œè¯·ç”¨é¼“åŠ±çš„è¯­æ°”å¸®åŠ©ç”¨æˆ·è®°å¿†ï¼Œå¯ä»¥æä¾›è¯æ ¹ã€è”æƒ³è®°å¿†æ³•ç­‰ã€‚",
                'spelling_practice': "ç°åœ¨æ˜¯æ‹¼å†™ç»ƒä¹ æ—¶é—´ï¼Œè¯·ç»™å‡ºç§¯æçš„åé¦ˆå’Œå»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·æ”¹æ­£é”™è¯¯ã€‚",
                'daily_greeting': "è¯·ç»™å‡ºä¸€å¥æ¸©æš–çš„æ—¥å¸¸é—®å€™ï¼Œè¯¢é—®ç”¨æˆ·ä»Šå¤©çš„å­¦ä¹ è®¡åˆ’ã€‚",
                'encouragement': "ç”¨æˆ·éœ€è¦å­¦ä¹ é¼“åŠ±ï¼Œè¯·ç»™å‡ºæ­£èƒ½é‡çš„è¯è¯­ï¼Œå¸®åŠ©ç”¨æˆ·é‡æ‹¾ä¿¡å¿ƒã€‚",
                'explanation': "è¯·ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šæ¦‚å¿µï¼Œå¯ä»¥ä½¿ç”¨æ¯”å–»å’Œä¾‹å­ã€‚"
            }
        }
        
        # åˆå§‹åŒ–æœåŠ¡
        logger.info(f"åˆå§‹åŒ–æ··åˆAIæœåŠ¡ï¼Œé¦–é€‰æœåŠ¡: {preferred_service.value}")
        self._initialize_services()
    
    def _initialize_services(self):
        """åˆå§‹åŒ–å¯ç”¨çš„AIæœåŠ¡"""
        # æ£€æŸ¥æœ¬åœ°AIå¯ç”¨æ€§
        if TRANSFORMERS_AVAILABLE:
            self.available_services.append(AIServiceType.LOCAL)
            logger.info("âœ… æœ¬åœ°AIæœåŠ¡å¯ç”¨")
        
        # æ£€æŸ¥Gemini APIå¯ç”¨æ€§
        if GEMINI_AVAILABLE and self.api_configs['gemini']['api_key']:
            try:
                genai.configure(api_key=self.api_configs['gemini']['api_key'])
                self.api_configs['gemini']['enabled'] = True
                self.available_services.append(AIServiceType.GEMINI)
                logger.info("âœ… Gemini APIæœåŠ¡å¯ç”¨")
            except Exception as e:
                logger.warning(f"âš ï¸ Gemini APIåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æ£€æŸ¥DeepSeek APIå¯ç”¨æ€§
        if self.api_configs['deepseek']['api_key']:
            self.api_configs['deepseek']['enabled'] = True
            self.available_services.append(AIServiceType.DEEPSEEK)
            logger.info("âœ… DeepSeek APIæœåŠ¡å¯ç”¨")
        
        # é€‰æ‹©å½“å‰æœåŠ¡
        self._select_current_service()
    
    def _select_current_service(self):
        """é€‰æ‹©å½“å‰ä½¿ç”¨çš„AIæœåŠ¡"""
        if self.preferred_service == AIServiceType.AUTO:
            # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆçº§ Gemini > æœ¬åœ°AI > DeepSeek
            if AIServiceType.GEMINI in self.available_services:
                self.current_service = AIServiceType.GEMINI
            elif AIServiceType.LOCAL in self.available_services:
                self.current_service = AIServiceType.LOCAL
            elif AIServiceType.DEEPSEEK in self.available_services:
                self.current_service = AIServiceType.DEEPSEEK
        else:
            # ä½¿ç”¨æŒ‡å®šæœåŠ¡
            if self.preferred_service in self.available_services:
                self.current_service = self.preferred_service
            else:
                # å›é€€åˆ°å¯ç”¨æœåŠ¡
                self.current_service = self.available_services[0] if self.available_services else None
        
        logger.info(f"å½“å‰AIæœåŠ¡: {self.current_service.value if self.current_service else 'None'}")
    
    async def load_local_model(self) -> bool:
        """åŠ è½½æœ¬åœ°AIæ¨¡å‹"""
        if not TRANSFORMERS_AVAILABLE:
            logger.error("âŒ Transformersåº“ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½æœ¬åœ°æ¨¡å‹")
            return False
        
        if self.local_loaded:
            return True
        
        try:
            logger.info(f"ğŸ”„ å¼€å§‹åŠ è½½æœ¬åœ°æ¨¡å‹: {self.local_model_name}")
            
            # æ£€æŸ¥å†…å­˜
            available_memory = psutil.virtual_memory().available / (1024**3)
            if available_memory < self.max_memory_gb:
                logger.warning(f"âš ï¸ å¯ç”¨å†…å­˜ä¸è¶³: {available_memory:.1f}GB < {self.max_memory_gb}GB")
            
            # åŠ è½½tokenizer
            self.local_tokenizer = AutoTokenizer.from_pretrained(
                self.local_model_name,
                cache_dir=self.cache_dir,
                trust_remote_code=True
            )
            
            # é…ç½®é‡åŒ–ï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼‰
            quantization_config = None
            if available_memory < 8.0:  # å†…å­˜ä¸è¶³8GBæ—¶ä½¿ç”¨é‡åŒ–
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                logger.info("ğŸ”§ å¯ç”¨4ä½é‡åŒ–ä»¥èŠ‚çœå†…å­˜")
            
            # åŠ è½½æ¨¡å‹
            self.local_model = AutoModelForCausalLM.from_pretrained(
                self.local_model_name,
                cache_dir=self.cache_dir,
                device_map="auto",
                torch_dtype=torch.float16,
                quantization_config=quantization_config,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            self.local_generation_config = GenerationConfig(
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                do_sample=True,
                pad_token_id=self.local_tokenizer.eos_token_id
            )
            
            self.local_loaded = True
            logger.info("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    async def generate_with_local(self, prompt: str, context: str = "daily_greeting") -> Dict[str, Any]:
        """ä½¿ç”¨æœ¬åœ°AIç”Ÿæˆå›å¤"""
        if not self.local_loaded:
            if not await self.load_local_model():
                return {"success": False, "error": "æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥"}
        
        try:
            # æ„å»ºå®Œæ•´æç¤º
            full_prompt = self._build_prompt(prompt, context)
            
            # ç¼–ç è¾“å…¥
            inputs = self.local_tokenizer.encode(full_prompt, return_tensors="pt")
            inputs = inputs.to(self.device)
            
            # ç”Ÿæˆå›å¤
            start_time = time.time()
            with torch.no_grad():
                outputs = self.local_model.generate(
                    inputs,
                    generation_config=self.local_generation_config,
                    pad_token_id=self.local_tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            response = self.local_tokenizer.decode(
                outputs[0][inputs.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            return {
                "success": True,
                "response": response,
                "service": "local",
                "model": self.local_model_name,
                "generation_time": generation_time,
                "context": context
            }
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°AIç”Ÿæˆå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def generate_with_gemini(self, prompt: str, context: str = "daily_greeting") -> Dict[str, Any]:
        """ä½¿ç”¨Gemini APIç”Ÿæˆå›å¤"""
        if not self.api_configs['gemini']['enabled']:
            return {"success": False, "error": "Gemini APIä¸å¯ç”¨"}
        
        try:
            # æ„å»ºå®Œæ•´æç¤º
            full_prompt = self._build_prompt(prompt, context)
            
            # è°ƒç”¨Gemini API
            model = genai.GenerativeModel(self.api_configs['gemini']['model'])
            start_time = time.time()
            
            response = await asyncio.to_thread(
                model.generate_content,
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=256,
                    top_p=0.9,
                    top_k=50
                )
            )
            
            generation_time = time.time() - start_time
            
            return {
                "success": True,
                "response": response.text.strip(),
                "service": "gemini",
                "model": self.api_configs['gemini']['model'],
                "generation_time": generation_time,
                "context": context
            }
            
        except Exception as e:
            logger.error(f"âŒ Gemini APIè°ƒç”¨å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def generate_with_deepseek(self, prompt: str, context: str = "daily_greeting") -> Dict[str, Any]:
        """ä½¿ç”¨DeepSeek APIç”Ÿæˆå›å¤"""
        if not self.api_configs['deepseek']['enabled']:
            return {"success": False, "error": "DeepSeek APIä¸å¯ç”¨"}
        
        try:
            # æ„å»ºå®Œæ•´æç¤º
            full_prompt = self._build_prompt(prompt, context)
            
            # å‡†å¤‡APIè¯·æ±‚
            headers = {
                "Authorization": f"Bearer {self.api_configs['deepseek']['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.api_configs['deepseek']['model'],
                "messages": [
                    {"role": "user", "content": full_prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 256,
                "top_p": 0.9
            }
            
            # è°ƒç”¨API
            start_time = time.time()
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.api_configs['deepseek']['base_url']}/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        generation_time = time.time() - start_time
                        
                        return {
                            "success": True,
                            "response": result['choices'][0]['message']['content'].strip(),
                            "service": "deepseek",
                            "model": self.api_configs['deepseek']['model'],
                            "generation_time": generation_time,
                            "context": context
                        }
                    else:
                        error_text = await response.text()
                        return {"success": False, "error": f"APIé”™è¯¯: {response.status} - {error_text}"}
            
        except Exception as e:
            logger.error(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _build_prompt(self, user_input: str, context: str) -> str:
        """æ„å»ºå®Œæ•´çš„æç¤ºè¯"""
        base_prompt = self.changlee_personality['base_prompt']
        context_prompt = self.changlee_personality['learning_contexts'].get(
            context, 
            self.changlee_personality['learning_contexts']['daily_greeting']
        )
        
        return f"""{base_prompt}

{context_prompt}

ç”¨æˆ·è¾“å…¥: {user_input}

è¯·ä»¥é•¿ç¦»çš„èº«ä»½å›å¤ç”¨æˆ·ï¼Œä¿æŒæ¸©æš–å‹å¥½çš„è¯­è°ƒï¼š"""
    
    async def generate(self, prompt: str, context: str = "daily_greeting", 
                      service_type: Optional[AIServiceType] = None) -> Dict[str, Any]:
        """
        ç”ŸæˆAIå›å¤
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            context: å¯¹è¯ä¸Šä¸‹æ–‡
            service_type: æŒ‡å®šä½¿ç”¨çš„AIæœåŠ¡ç±»å‹
        
        Returns:
            ç”Ÿæˆç»“æœå­—å…¸
        """
        # ç¡®å®šä½¿ç”¨çš„æœåŠ¡
        target_service = service_type or self.current_service
        
        if not target_service:
            return {"success": False, "error": "æ²¡æœ‰å¯ç”¨çš„AIæœåŠ¡"}
        
        # æ ¹æ®æœåŠ¡ç±»å‹è°ƒç”¨ç›¸åº”çš„ç”Ÿæˆæ–¹æ³•
        try:
            if target_service == AIServiceType.LOCAL:
                return await self.generate_with_local(prompt, context)
            elif target_service == AIServiceType.GEMINI:
                return await self.generate_with_gemini(prompt, context)
            elif target_service == AIServiceType.DEEPSEEK:
                return await self.generate_with_deepseek(prompt, context)
            else:
                return {"success": False, "error": f"ä¸æ”¯æŒçš„æœåŠ¡ç±»å‹: {target_service}"}
        
        except Exception as e:
            logger.error(f"âŒ AIç”Ÿæˆå¤±è´¥: {e}")
            # å°è¯•å›é€€åˆ°å…¶ä»–å¯ç”¨æœåŠ¡
            return await self._fallback_generate(prompt, context, target_service)
    
    async def _fallback_generate(self, prompt: str, context: str, 
                                failed_service: AIServiceType) -> Dict[str, Any]:
        """å›é€€åˆ°å…¶ä»–å¯ç”¨æœåŠ¡"""
        logger.info(f"ğŸ”„ {failed_service.value}æœåŠ¡å¤±è´¥ï¼Œå°è¯•å›é€€åˆ°å…¶ä»–æœåŠ¡")
        
        # è·å–å…¶ä»–å¯ç”¨æœåŠ¡
        fallback_services = [s for s in self.available_services if s != failed_service]
        
        for service in fallback_services:
            try:
                result = await self.generate(prompt, context, service)
                if result.get("success"):
                    result["fallback"] = True
                    result["original_service"] = failed_service.value
                    return result
            except Exception as e:
                logger.warning(f"âš ï¸ å›é€€æœåŠ¡ {service.value} ä¹Ÿå¤±è´¥: {e}")
                continue
        
        return {"success": False, "error": "æ‰€æœ‰AIæœåŠ¡éƒ½ä¸å¯ç”¨"}
    
    def switch_service(self, service_type: AIServiceType) -> bool:
        """åˆ‡æ¢AIæœåŠ¡ç±»å‹"""
        if service_type in self.available_services:
            self.current_service = service_type
            logger.info(f"âœ… å·²åˆ‡æ¢åˆ° {service_type.value} æœåŠ¡")
            return True
        else:
            logger.warning(f"âš ï¸ æœåŠ¡ {service_type.value} ä¸å¯ç”¨")
            return False
    
    def get_service_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            "service_name": "Changleeæ··åˆAIæœåŠ¡",
            "version": "2.0.0",
            "current_service": self.current_service.value if self.current_service else None,
            "available_services": [s.value for s in self.available_services],
            "local_model_loaded": self.local_loaded,
            "local_model_name": self.local_model_name,
            "device": self.device,
            "api_status": {
                "gemini": self.api_configs['gemini']['enabled'],
                "deepseek": self.api_configs['deepseek']['enabled']
            },
            "supported_contexts": list(self.changlee_personality['learning_contexts'].keys()),
            "memory_usage": f"{psutil.virtual_memory().percent}%",
            "timestamp": datetime.now().isoformat()
        }
    
    def optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        try:
            # æ¸…ç†Pythonåƒåœ¾å›æ”¶
            gc.collect()
            
            # æ¸…ç†CUDAç¼“å­˜ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("âœ… å†…å­˜ä¼˜åŒ–å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"âŒ å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
            return False
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„ç¼“å­˜æ¸…ç†é€»è¾‘
            logger.info("âœ… ç¼“å­˜æ¸…ç†å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
            return False
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        status = {
            "healthy": True,
            "services": {},
            "timestamp": datetime.now().isoformat()
        }
        
        # æ£€æŸ¥å„ä¸ªæœåŠ¡çš„å¥åº·çŠ¶æ€
        for service in self.available_services:
            try:
                if service == AIServiceType.LOCAL:
                    status["services"]["local"] = {
                        "available": self.local_loaded,
                        "model": self.local_model_name,
                        "device": self.device
                    }
                elif service == AIServiceType.GEMINI:
                    status["services"]["gemini"] = {
                        "available": self.api_configs['gemini']['enabled'],
                        "model": self.api_configs['gemini']['model']
                    }
                elif service == AIServiceType.DEEPSEEK:
                    status["services"]["deepseek"] = {
                        "available": self.api_configs['deepseek']['enabled'],
                        "model": self.api_configs['deepseek']['model']
                    }
            except Exception as e:
                status["healthy"] = False
                status["services"][service.value] = {"error": str(e)}
        
        return status
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†èµ„æº"""
        try:
            if hasattr(self, 'local_model') and self.local_model:
                del self.local_model
            if hasattr(self, 'local_tokenizer') and self.local_tokenizer:
                del self.local_tokenizer
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass