#!/usr/bin/env python3
"""
ğŸ”„ Changleeæ¡Œé¢å® ç‰©ç³»ç»Ÿä¼˜åŒ–å™¨
============================

ä¼˜åŒ–Changleeç³»ç»Ÿæ€§èƒ½å’Œç¨³å®šæ€§
- Electronæ€§èƒ½ä¼˜åŒ–
- ä¾èµ–ç®¡ç†ç»Ÿä¸€
- æœåŠ¡æ¨¡å—è§£è€¦
- é”™è¯¯æ¢å¤æœºåˆ¶
- Chronicleé›†æˆä¼˜åŒ–

ä¿æŒæ‰€æœ‰åŸæœ‰åŠŸèƒ½ä¸å˜
"""

import os
import json
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import re

class ChangleeOptimizer:
    """Changleeç³»ç»Ÿä¼˜åŒ–å™¨"""
    
    def __init__(self, changlee_dir: Path = None):
        self.changlee_dir = changlee_dir or Path(__file__).parent
        self.src_dir = self.changlee_dir / "src"
        self.main_package = self.changlee_dir / "package.json"
        self.renderer_package = self.changlee_dir / "src" / "renderer" / "package.json"
        
        self.optimization_log = []
        self.performance_metrics = {}
    
    def log_action(self, action: str, details: str = "", level: str = "INFO"):
        """è®°å½•ä¼˜åŒ–æ“ä½œ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "details": details,
            "level": level
        }
        self.optimization_log.append(log_entry)
        
        emoji = "âœ…" if level == "INFO" else "âš ï¸" if level == "WARN" else "âŒ"
        print(f"{emoji} {action}: {details}")
    
    def analyze_package_structure(self) -> Dict[str, Any]:
        """åˆ†æpackage.jsonç»“æ„"""
        analysis = {
            "main_package": {},
            "renderer_package": {},
            "dependency_conflicts": [],
            "optimization_suggestions": []
        }
        
        # åˆ†æä¸»package.json
        if self.main_package.exists():
            with open(self.main_package, 'r', encoding='utf-8') as f:
                main_pkg = json.load(f)
                analysis["main_package"] = {
                    "dependencies": len(main_pkg.get("dependencies", {})),
                    "devDependencies": len(main_pkg.get("devDependencies", {})),
                    "scripts": len(main_pkg.get("scripts", {})),
                    "size": self.main_package.stat().st_size
                }
        
        # åˆ†ærenderer package.json
        if self.renderer_package.exists():
            with open(self.renderer_package, 'r', encoding='utf-8') as f:
                renderer_pkg = json.load(f)
                analysis["renderer_package"] = {
                    "dependencies": len(renderer_pkg.get("dependencies", {})),
                    "devDependencies": len(renderer_pkg.get("devDependencies", {})),
                    "scripts": len(renderer_pkg.get("scripts", {})),
                    "size": self.renderer_package.stat().st_size
                }
        
        # æ£€æŸ¥ä¾èµ–å†²çª
        analysis["dependency_conflicts"] = self._find_dependency_conflicts()
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        analysis["optimization_suggestions"] = self._generate_package_suggestions(analysis)
        
        return analysis
    
    def _find_dependency_conflicts(self) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾ä¾èµ–å†²çª"""
        conflicts = []
        
        if not (self.main_package.exists() and self.renderer_package.exists()):
            return conflicts
        
        try:
            with open(self.main_package, 'r', encoding='utf-8') as f:
                main_pkg = json.load(f)
            
            with open(self.renderer_package, 'r', encoding='utf-8') as f:
                renderer_pkg = json.load(f)
            
            main_deps = main_pkg.get("dependencies", {})
            renderer_deps = renderer_pkg.get("dependencies", {})
            
            # æŸ¥æ‰¾ç‰ˆæœ¬å†²çª
            for pkg_name in set(main_deps.keys()) & set(renderer_deps.keys()):
                main_version = main_deps[pkg_name]
                renderer_version = renderer_deps[pkg_name]
                
                if main_version != renderer_version:
                    conflicts.append({
                        "package": pkg_name,
                        "main_version": main_version,
                        "renderer_version": renderer_version,
                        "type": "version_conflict"
                    })
            
        except Exception as e:
            self.log_action("ä¾èµ–åˆ†æå¤±è´¥", str(e), "ERROR")
        
        return conflicts
    
    def _generate_package_suggestions(self, analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆpackageä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # æ£€æŸ¥è„šæœ¬æ•°é‡
        main_scripts = analysis["main_package"].get("scripts", 0)
        if main_scripts > 10:
            suggestions.append(f"ä¸»package.jsonæœ‰{main_scripts}ä¸ªè„šæœ¬ï¼Œå»ºè®®æ•´ç†å’Œåˆ†ç±»")
        
        # æ£€æŸ¥ä¾èµ–æ•°é‡
        main_deps = analysis["main_package"].get("dependencies", 0)
        renderer_deps = analysis["renderer_package"].get("dependencies", 0)
        
        if main_deps + renderer_deps > 50:
            suggestions.append("ä¾èµ–åŒ…è¾ƒå¤šï¼Œå»ºè®®å®¡æŸ¥æ˜¯å¦æœ‰æœªä½¿ç”¨çš„ä¾èµ–")
        
        # æ£€æŸ¥å†²çª
        if analysis["dependency_conflicts"]:
            suggestions.append(f"å‘ç°{len(analysis['dependency_conflicts'])}ä¸ªä¾èµ–ç‰ˆæœ¬å†²çª")
        
        return suggestions
    
    def optimize_electron_performance(self):
        """ä¼˜åŒ–Electronæ€§èƒ½"""
        self.log_action("å¼€å§‹Electronæ€§èƒ½ä¼˜åŒ–", "åˆ†æå’Œä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        
        # æ£€æŸ¥main.jsæ–‡ä»¶
        main_js_path = self.src_dir / "main" / "main.js"
        if main_js_path.exists():
            self._optimize_main_process(main_js_path)
        
        # æ£€æŸ¥preloadè„šæœ¬
        preload_files = list(self.src_dir.glob("**/preload*.js"))
        for preload_file in preload_files:
            self._optimize_preload_script(preload_file)
        
        # ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–é…ç½®
        self._create_performance_config()
    
    def _optimize_main_process(self, main_js_path: Path):
        """ä¼˜åŒ–ä¸»è¿›ç¨‹"""
        try:
            content = main_js_path.read_text(encoding='utf-8')
            original_size = len(content)
            
            optimizations = []
            
            # æ£€æŸ¥å†…å­˜ç®¡ç†
            if "webSecurity: false" in content:
                optimizations.append("å»ºè®®å¯ç”¨webSecurityä»¥æå‡å®‰å…¨æ€§")
            
            if "nodeIntegration: true" in content:
                optimizations.append("å»ºè®®ç¦ç”¨nodeIntegrationï¼Œä½¿ç”¨preloadè„šæœ¬")
            
            # æ£€æŸ¥çª—å£ç®¡ç†
            if "show: true" in content and "ready-to-show" not in content:
                optimizations.append("å»ºè®®ä½¿ç”¨ready-to-showäº‹ä»¶ä¼˜åŒ–çª—å£æ˜¾ç¤º")
            
            self.log_action(
                "ä¸»è¿›ç¨‹åˆ†æå®Œæˆ",
                f"æ–‡ä»¶å¤§å°: {original_size}å­—èŠ‚, å‘ç°{len(optimizations)}ä¸ªä¼˜åŒ–ç‚¹"
            )
            
            if optimizations:
                self.performance_metrics["main_process_optimizations"] = optimizations
            
        except Exception as e:
            self.log_action("ä¸»è¿›ç¨‹ä¼˜åŒ–å¤±è´¥", str(e), "ERROR")
    
    def _optimize_preload_script(self, preload_path: Path):
        """ä¼˜åŒ–preloadè„šæœ¬"""
        try:
            content = preload_path.read_text(encoding='utf-8')
            
            # æ£€æŸ¥APIæš´éœ²
            api_count = len(re.findall(r'contextBridge\.exposeInMainWorld', content))
            
            self.log_action(
                f"Preloadè„šæœ¬åˆ†æ",
                f"{preload_path.name}: æš´éœ²{api_count}ä¸ªAPI"
            )
            
        except Exception as e:
            self.log_action(f"Preloadè„šæœ¬åˆ†æå¤±è´¥", str(e), "ERROR")
    
    def _create_performance_config(self):
        """åˆ›å»ºæ€§èƒ½ä¼˜åŒ–é…ç½®"""
        config = {
            "electron_performance": {
                "memory_optimization": {
                    "enable_memory_info": True,
                    "gc_interval": 60000,
                    "max_memory_usage": "512MB"
                },
                "window_optimization": {
                    "use_ready_to_show": True,
                    "enable_background_throttling": True,
                    "web_security": True
                },
                "renderer_optimization": {
                    "node_integration": False,
                    "context_isolation": True,
                    "enable_remote_module": False
                }
            },
            "created_at": datetime.now().isoformat()
        }
        
        config_path = self.changlee_dir / "performance_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        self.log_action("æ€§èƒ½é…ç½®åˆ›å»º", f"é…ç½®å·²ä¿å­˜åˆ° {config_path}")
    
    def optimize_service_modules(self):
        """ä¼˜åŒ–æœåŠ¡æ¨¡å—è§£è€¦"""
        self.log_action("å¼€å§‹æœåŠ¡æ¨¡å—è§£è€¦ä¼˜åŒ–", "åˆ†ææœåŠ¡ä¾èµ–å…³ç³»")
        
        # æŸ¥æ‰¾æœåŠ¡æ–‡ä»¶
        service_files = []
        for pattern in ["*Service.js", "*service.js", "*Service.ts", "*service.ts"]:
            service_files.extend(self.src_dir.glob(f"**/{pattern}"))
        
        if not service_files:
            self.log_action("æœªæ‰¾åˆ°æœåŠ¡æ–‡ä»¶", "è·³è¿‡æœåŠ¡æ¨¡å—ä¼˜åŒ–")
            return
        
        service_analysis = {}
        
        for service_file in service_files:
            analysis = self._analyze_service_file(service_file)
            service_analysis[service_file.name] = analysis
        
        # ç”Ÿæˆè§£è€¦å»ºè®®
        decoupling_suggestions = self._generate_decoupling_suggestions(service_analysis)
        
        self.log_action(
            "æœåŠ¡æ¨¡å—åˆ†æå®Œæˆ",
            f"åˆ†æäº†{len(service_files)}ä¸ªæœåŠ¡æ–‡ä»¶ï¼Œç”Ÿæˆ{len(decoupling_suggestions)}ä¸ªå»ºè®®"
        )
        
        self.performance_metrics["service_analysis"] = service_analysis
        self.performance_metrics["decoupling_suggestions"] = decoupling_suggestions
    
    def _analyze_service_file(self, service_file: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæœåŠ¡æ–‡ä»¶"""
        try:
            content = service_file.read_text(encoding='utf-8')
            
            # åˆ†æå¯¼å…¥ä¾èµ–
            imports = re.findall(r'(?:import|require)\s*\([\'"]([^\'"]+)[\'"]', content)
            
            # åˆ†æç±»å’Œå‡½æ•°å®šä¹‰
            classes = re.findall(r'class\s+(\w+)', content)
            functions = re.findall(r'(?:function\s+(\w+)|const\s+(\w+)\s*=.*?=>)', content)
            
            # åˆ†æäº‹ä»¶ç›‘å¬
            event_listeners = len(re.findall(r'addEventListener|on\w+\s*=', content))
            
            return {
                "size": len(content),
                "imports": len(imports),
                "classes": len(classes),
                "functions": len([f for f in functions if f]),
                "event_listeners": event_listeners,
                "dependencies": imports[:10]  # åªä¿ç•™å‰10ä¸ªä¾èµ–
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def _generate_decoupling_suggestions(self, service_analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè§£è€¦å»ºè®®"""
        suggestions = []
        
        # åˆ†æé«˜è€¦åˆæœåŠ¡
        high_coupling_services = []
        for service_name, analysis in service_analysis.items():
            if analysis.get("imports", 0) > 10:
                high_coupling_services.append(service_name)
        
        if high_coupling_services:
            suggestions.append(f"é«˜è€¦åˆæœåŠ¡: {', '.join(high_coupling_services)} - å»ºè®®æ‹†åˆ†ä¾èµ–")
        
        # åˆ†æå¤§å‹æœåŠ¡
        large_services = []
        for service_name, analysis in service_analysis.items():
            if analysis.get("size", 0) > 5000:  # 5KBä»¥ä¸Š
                large_services.append(service_name)
        
        if large_services:
            suggestions.append(f"å¤§å‹æœåŠ¡: {', '.join(large_services)} - å»ºè®®æ‹†åˆ†åŠŸèƒ½")
        
        return suggestions
    
    def add_error_recovery_mechanism(self):
        """æ·»åŠ é”™è¯¯æ¢å¤æœºåˆ¶"""
        self.log_action("æ·»åŠ é”™è¯¯æ¢å¤æœºåˆ¶", "åˆ›å»ºé”™è¯¯å¤„ç†å’Œæ¢å¤ç³»ç»Ÿ")
        
        # åˆ›å»ºé”™è¯¯æ¢å¤é…ç½®
        error_recovery_config = {
            "error_handling": {
                "auto_restart": True,
                "max_restart_attempts": 3,
                "restart_delay": 5000,
                "log_errors": True
            },
            "crash_recovery": {
                "save_state_on_crash": True,
                "restore_state_on_restart": True,
                "backup_interval": 300000  # 5åˆ†é’Ÿ
            },
            "memory_management": {
                "monitor_memory": True,
                "memory_threshold": 512,  # MB
                "gc_on_threshold": True
            }
        }
        
        config_path = self.changlee_dir / "error_recovery_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(error_recovery_config, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºé”™è¯¯æ¢å¤è„šæœ¬
        self._create_error_recovery_script()
        
        self.log_action("é”™è¯¯æ¢å¤æœºåˆ¶åˆ›å»ºå®Œæˆ", f"é…ç½®ä¿å­˜åˆ° {config_path}")
    
    def _create_error_recovery_script(self):
        """åˆ›å»ºé”™è¯¯æ¢å¤è„šæœ¬"""
        recovery_script = '''/**
 * Changleeé”™è¯¯æ¢å¤æœºåˆ¶
 * è‡ªåŠ¨å¤„ç†åº”ç”¨å´©æºƒå’Œé”™è¯¯æ¢å¤
 */

class ErrorRecoveryManager {
    constructor() {
        this.restartAttempts = 0;
        this.maxRestartAttempts = 3;
        this.restartDelay = 5000;
        this.isRecovering = false;
        
        this.setupErrorHandlers();
        this.setupMemoryMonitoring();
    }
    
    setupErrorHandlers() {
        // ä¸»è¿›ç¨‹é”™è¯¯å¤„ç†
        process.on('uncaughtException', (error) => {
            console.error('Uncaught Exception:', error);
            this.handleCrash(error);
        });
        
        process.on('unhandledRejection', (reason, promise) => {
            console.error('Unhandled Rejection at:', promise, 'reason:', reason);
            this.handleCrash(reason);
        });
    }
    
    setupMemoryMonitoring() {
        setInterval(() => {
            const memoryUsage = process.memoryUsage();
            const heapUsedMB = memoryUsage.heapUsed / 1024 / 1024;
            
            if (heapUsedMB > 512) { // 512MBé˜ˆå€¼
                console.warn(`High memory usage: ${heapUsedMB.toFixed(2)}MB`);
                this.triggerGarbageCollection();
            }
        }, 60000); // æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    }
    
    handleCrash(error) {
        if (this.isRecovering) return;
        
        this.isRecovering = true;
        
        // ä¿å­˜åº”ç”¨çŠ¶æ€
        this.saveApplicationState();
        
        // è®°å½•é”™è¯¯
        this.logError(error);
        
        // å°è¯•é‡å¯
        if (this.restartAttempts < this.maxRestartAttempts) {
            setTimeout(() => {
                this.attemptRestart();
            }, this.restartDelay);
        } else {
            console.error('Max restart attempts reached. Manual intervention required.');
            this.showCrashDialog();
        }
    }
    
    saveApplicationState() {
        try {
            const state = {
                timestamp: new Date().toISOString(),
                memoryUsage: process.memoryUsage(),
                // æ·»åŠ æ›´å¤šçŠ¶æ€ä¿¡æ¯
            };
            
            require('fs').writeFileSync(
                require('path').join(__dirname, 'crash_state.json'),
                JSON.stringify(state, null, 2)
            );
        } catch (err) {
            console.error('Failed to save application state:', err);
        }
    }
    
    logError(error) {
        const errorLog = {
            timestamp: new Date().toISOString(),
            error: error.toString(),
            stack: error.stack,
            memoryUsage: process.memoryUsage()
        };
        
        try {
            const fs = require('fs');
            const path = require('path');
            const logPath = path.join(__dirname, 'error.log');
            
            fs.appendFileSync(logPath, JSON.stringify(errorLog) + '\\n');
        } catch (err) {
            console.error('Failed to log error:', err);
        }
    }
    
    attemptRestart() {
        this.restartAttempts++;
        console.log(`Attempting restart ${this.restartAttempts}/${this.maxRestartAttempts}`);
        
        // è¿™é‡Œå®ç°é‡å¯é€»è¾‘
        // å…·ä½“å®ç°å–å†³äºåº”ç”¨æ¶æ„
    }
    
    triggerGarbageCollection() {
        if (global.gc) {
            global.gc();
            console.log('Garbage collection triggered');
        }
    }
    
    showCrashDialog() {
        // æ˜¾ç¤ºå´©æºƒå¯¹è¯æ¡†ï¼Œè®©ç”¨æˆ·é€‰æ‹©æ“ä½œ
        const { dialog } = require('electron');
        
        dialog.showErrorBox(
            'Application Crash',
            'Changlee has encountered multiple crashes. Please restart the application manually.'
        );
    }
}

module.exports = ErrorRecoveryManager;
'''
        
        script_path = self.changlee_dir / "src" / "utils" / "errorRecovery.js"
        script_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(recovery_script)
        
        self.log_action("é”™è¯¯æ¢å¤è„šæœ¬åˆ›å»º", f"è„šæœ¬ä¿å­˜åˆ° {script_path}")
    
    def optimize_chronicle_integration(self):
        """ä¼˜åŒ–Chronicleé›†æˆ"""
        self.log_action("ä¼˜åŒ–Chronicleé›†æˆ", "åˆ†æå’Œä¼˜åŒ–Chronicleè¿æ¥")
        
        # æŸ¥æ‰¾Chronicleç›¸å…³æ–‡ä»¶
        chronicle_files = []
        for pattern in ["*chronicle*", "*Chronicle*"]:
            chronicle_files.extend(self.changlee_dir.glob(f"**/{pattern}"))
        
        if not chronicle_files:
            self.log_action("æœªæ‰¾åˆ°Chronicleé›†æˆæ–‡ä»¶", "è·³è¿‡Chronicleä¼˜åŒ–")
            return
        
        # åˆ†æChronicleé›†æˆ
        integration_analysis = {}
        for file_path in chronicle_files:
            if file_path.is_file() and file_path.suffix in ['.js', '.ts', '.json']:
                analysis = self._analyze_chronicle_file(file_path)
                integration_analysis[file_path.name] = analysis
        
        # åˆ›å»ºä¼˜åŒ–åçš„Chronicleé›†æˆé…ç½®
        self._create_optimized_chronicle_config(integration_analysis)
        
        self.log_action(
            "Chronicleé›†æˆä¼˜åŒ–å®Œæˆ",
            f"åˆ†æäº†{len(chronicle_files)}ä¸ªç›¸å…³æ–‡ä»¶"
        )
    
    def _analyze_chronicle_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æChronicleç›¸å…³æ–‡ä»¶"""
        try:
            if file_path.suffix == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                return {
                    "type": "config",
                    "keys": list(content.keys()) if isinstance(content, dict) else [],
                    "size": file_path.stat().st_size
                }
            else:
                content = file_path.read_text(encoding='utf-8')
                
                # åˆ†æAPIè°ƒç”¨
                api_calls = len(re.findall(r'fetch\s*\(|axios\.|http\.|request\(', content))
                
                # åˆ†æäº‹ä»¶å¤„ç†
                event_handlers = len(re.findall(r'addEventListener|on\w+\s*=', content))
                
                return {
                    "type": "script",
                    "size": len(content),
                    "api_calls": api_calls,
                    "event_handlers": event_handlers
                }
                
        except Exception as e:
            return {"error": str(e)}
    
    def _create_optimized_chronicle_config(self, analysis: Dict[str, Any]):
        """åˆ›å»ºä¼˜åŒ–çš„Chronicleé›†æˆé…ç½®"""
        config = {
            "chronicle_integration": {
                "connection": {
                    "timeout": 10000,
                    "retry_attempts": 3,
                    "retry_delay": 2000,
                    "keep_alive": True
                },
                "performance": {
                    "batch_requests": True,
                    "cache_responses": True,
                    "cache_duration": 300000,  # 5åˆ†é’Ÿ
                    "compress_data": True
                },
                "error_handling": {
                    "auto_reconnect": True,
                    "fallback_mode": True,
                    "log_errors": True
                }
            },
            "analysis_results": analysis,
            "optimized_at": datetime.now().isoformat()
        }
        
        config_path = self.changlee_dir / "chronicle_integration_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        self.log_action("Chronicleé›†æˆé…ç½®ä¼˜åŒ–", f"é…ç½®ä¿å­˜åˆ° {config_path}")
    
    def create_optimization_report(self):
        """åˆ›å»ºä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            "optimization_date": datetime.now().isoformat(),
            "changlee_directory": str(self.changlee_dir),
            "actions_performed": self.optimization_log,
            "performance_metrics": self.performance_metrics,
            "summary": {
                "total_actions": len(self.optimization_log),
                "electron_optimized": any("Electron" in action["action"] for action in self.optimization_log),
                "services_analyzed": any("æœåŠ¡" in action["action"] for action in self.optimization_log),
                "error_recovery_added": any("é”™è¯¯æ¢å¤" in action["action"] for action in self.optimization_log),
                "chronicle_optimized": any("Chronicle" in action["action"] for action in self.optimization_log)
            },
            "recommendations": self._generate_final_recommendations()
        }
        
        report_file = self.changlee_dir / "changlee_optimization_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.log_action("ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š", f"æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file}")
        
        return report
    
    def _generate_final_recommendations(self) -> List[str]:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        recommendations = []
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if "service_analysis" in self.performance_metrics:
            service_count = len(self.performance_metrics["service_analysis"])
            if service_count > 5:
                recommendations.append(f"å‘ç°{service_count}ä¸ªæœåŠ¡æ–‡ä»¶ï¼Œå»ºè®®è€ƒè™‘å¾®æœåŠ¡æ¶æ„")
        
        if "decoupling_suggestions" in self.performance_metrics:
            suggestions = self.performance_metrics["decoupling_suggestions"]
            if suggestions:
                recommendations.extend(suggestions)
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å®šæœŸç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ",
            "å®æ–½è‡ªåŠ¨åŒ–æµ‹è¯•ä»¥ç¡®ä¿ç¨³å®šæ€§",
            "è€ƒè™‘ä½¿ç”¨TypeScriptæå‡ä»£ç è´¨é‡",
            "æ·»åŠ æ€§èƒ½ç›‘æ§å’Œåˆ†æå·¥å…·"
        ])
        
        return recommendations
    
    def run_optimization(self):
        """è¿è¡Œå®Œæ•´ä¼˜åŒ–æµç¨‹"""
        print("ğŸ”„ å¼€å§‹Changleeç³»ç»Ÿä¼˜åŒ–...")
        print("=" * 60)
        
        # 1. åˆ†æpackageç»“æ„
        package_analysis = self.analyze_package_structure()
        self.log_action(
            "Packageç»“æ„åˆ†æå®Œæˆ",
            f"ä¸»åŒ…ä¾èµ–: {package_analysis['main_package'].get('dependencies', 0)}, "
            f"æ¸²æŸ“å™¨ä¾èµ–: {package_analysis['renderer_package'].get('dependencies', 0)}"
        )
        
        # 2. Electronæ€§èƒ½ä¼˜åŒ–
        self.optimize_electron_performance()
        
        # 3. æœåŠ¡æ¨¡å—è§£è€¦
        self.optimize_service_modules()
        
        # 4. æ·»åŠ é”™è¯¯æ¢å¤æœºåˆ¶
        self.add_error_recovery_mechanism()
        
        # 5. ä¼˜åŒ–Chronicleé›†æˆ
        self.optimize_chronicle_integration()
        
        # 6. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        report = self.create_optimization_report()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Changleeç³»ç»Ÿä¼˜åŒ–å®Œæˆ!")
        print(f"ğŸ“Š æ‰§è¡Œäº† {report['summary']['total_actions']} ä¸ªä¼˜åŒ–æ“ä½œ")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {self.changlee_dir}/changlee_optimization_report.json")
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Changleeç³»ç»Ÿä¼˜åŒ–å·¥å…·")
    parser.add_argument("--changlee-dir", help="Changleeç³»ç»Ÿç›®å½•è·¯å¾„")
    parser.add_argument("--dry-run", action="store_true", help="ä»…åˆ†æï¼Œä¸æ‰§è¡Œå®é™…ä¼˜åŒ–")
    
    args = parser.parse_args()
    
    changlee_dir = Path(args.changlee_dir) if args.changlee_dir else Path(__file__).parent
    
    if not changlee_dir.exists():
        print(f"âŒ Changleeç›®å½•ä¸å­˜åœ¨: {changlee_dir}")
        return
    
    optimizer = ChangleeOptimizer(changlee_dir)
    
    if args.dry_run:
        print("ğŸ” æ‰§è¡Œåˆ†ææ¨¡å¼ï¼ˆä¸ä¼šä¿®æ”¹æ–‡ä»¶ï¼‰...")
        optimizer.analyze_package_structure()
        optimizer.optimize_service_modules()
    else:
        optimizer.run_optimization()

if __name__ == "__main__":
    main()