#!/usr/bin/env python3
"""
å®Œæ•´RAG APIæœåŠ¡å™¨
æ”¯æŒæ–‡æ¡£ä¸Šä¼ ã€è§£æã€å‘é‡åŒ–å’Œæ™ºèƒ½é—®ç­”
ä½¿ç”¨Gemini AIä½œä¸ºåç«¯
"""

import os
import json
import logging
import hashlib
import tempfile
from datetime import datetime
from typing import List, Dict, Any, Optional
import traceback
import re
from pathlib import Path

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import google.generativeai as genai

# æ–‡æ¡£å¤„ç†åº“
try:
    import PyPDF2
    import docx
    from sentence_transformers import SentenceTransformer
    import numpy as np
    import faiss
    PDF_SUPPORT = True
except ImportError as e:
    logger = logging.getLogger(__name__)
    logger.warning(f"éƒ¨åˆ†æ–‡æ¡£å¤„ç†åº“æœªå®‰è£…: {e}")
    PDF_SUPPORT = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# Gemini APIé…ç½®
GEMINI_API_KEY = "AIzaSyBOlNcGkx43zNOvnDesd_PEhD4Lj9T8Tpo"  # æ‚¨çš„Gemini APIå¯†é’¥
MODEL_NAME = "gemini-2.0-flash-exp"

# å­˜å‚¨é…ç½®
UPLOAD_FOLDER = '/tmp/rag_documents'
VECTOR_DB_PATH = '/tmp/rag_vectors'
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(VECTOR_DB_PATH, exist_ok=True)

# å…¨å±€å­˜å‚¨
chat_history = []
document_store = {}  # æ–‡æ¡£å­˜å‚¨
vector_store = None  # å‘é‡å­˜å‚¨
embeddings_model = None  # åµŒå…¥æ¨¡å‹

# åˆå§‹åŒ–Gemini AI
try:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)
    logger.info(f"âœ… Gemini APIåˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
except Exception as e:
    logger.error(f"âŒ Gemini APIåˆå§‹åŒ–å¤±è´¥: {e}")
    model = None

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆè½»é‡çº§ï¼‰
try:
    embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
    vector_store = faiss.IndexFlatL2(384)  # all-MiniLM-L6-v2çš„ç»´åº¦æ˜¯384
    logger.info("âœ… å‘é‡åŒ–æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸ å‘é‡åŒ–æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¨¡å¼: {e}")
    embeddings_model = None
    vector_store = None

def extract_text_from_file(file_path: str, filename: str) -> str:
    """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        file_ext = Path(filename).suffix.lower()
        
        if file_ext == '.txt':
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        
        elif file_ext == '.pdf':
            try:
                import PyPDF2
                text = ""
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                return text
            except ImportError:
                return "PDFå¤„ç†åº“æœªå®‰è£…ï¼Œè¯·å®‰è£…PyPDF2"
        
        elif file_ext in ['.docx', '.doc']:
            try:
                import docx
                doc = docx.Document(file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                return text
            except ImportError:
                return "Wordæ–‡æ¡£å¤„ç†åº“æœªå®‰è£…ï¼Œè¯·å®‰è£…python-docx"
        
        else:
            # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
                
    except Exception as e:
        logger.error(f"æ–‡ä»¶æ–‡æœ¬æå–å¤±è´¥: {e}")
        return f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """å°†æ–‡æœ¬åˆ†å—"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # å°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦æˆ–ç©ºæ ¼å¤„åˆ†å‰²
        if end < len(text):
            for delimiter in ['\n\n', '\n', '. ', 'ã€‚', ' ']:
                last_delim = text.rfind(delimiter, start, end)
                if last_delim > start:
                    end = last_delim + len(delimiter)
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        start = end - overlap
        if start >= len(text):
            break
    
    return chunks

def add_document_to_vector_store(doc_id: str, text: str):
    """å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨"""
    global vector_store, embeddings_model, document_store
    
    if not embeddings_model or not vector_store:
        logger.warning("å‘é‡åŒ–æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å‘é‡å­˜å‚¨")
        return
    
    try:
        # åˆ†å—
        chunks = chunk_text(text)
        
        # ç”ŸæˆåµŒå…¥
        embeddings = embeddings_model.encode(chunks)
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        vector_store.add(embeddings)
        
        # ä¿å­˜æ–‡æ¡£å—ä¿¡æ¯
        start_idx = vector_store.ntotal - len(chunks)
        document_store[doc_id] = {
            'chunks': chunks,
            'start_idx': start_idx,
            'end_idx': vector_store.ntotal - 1,
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"æ–‡æ¡£ {doc_id} å·²æ·»åŠ åˆ°å‘é‡å­˜å‚¨ï¼Œå…± {len(chunks)} ä¸ªå—")
        
    except Exception as e:
        logger.error(f"å‘é‡å­˜å‚¨å¤±è´¥: {e}")

def search_relevant_documents(query: str, top_k: int = 3) -> List[str]:
    """æœç´¢ç›¸å…³æ–‡æ¡£"""
    global vector_store, embeddings_model, document_store
    
    if not embeddings_model or not vector_store or vector_store.ntotal == 0:
        return []
    
    try:
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = embeddings_model.encode([query])
        
        # æœç´¢
        distances, indices = vector_store.search(query_embedding, min(top_k, vector_store.ntotal))
        
        # è·å–ç›¸å…³æ–‡æ¡£å—
        relevant_chunks = []
        for idx in indices[0]:
            if idx >= 0:  # faissè¿”å›-1è¡¨ç¤ºæ— æ•ˆç´¢å¼•
                # æ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£å—
                for doc_id, doc_info in document_store.items():
                    if doc_info['start_idx'] <= idx <= doc_info['end_idx']:
                        chunk_idx = idx - doc_info['start_idx']
                        if chunk_idx < len(doc_info['chunks']):
                            relevant_chunks.append(doc_info['chunks'][chunk_idx])
                        break
        
        return relevant_chunks
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
        return []

def get_ai_response(message: str) -> str:
    """è·å–AIå“åº”ï¼ˆæ”¯æŒRAGï¼‰"""
    try:
        if not model:
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = search_relevant_documents(message)
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        context = "ä½ æ˜¯NEXUS AIï¼Œä¸€ä¸ªå‹å¥½çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ï¼Œä¿æŒç®€æ´æ˜äº†ä½†ä¿¡æ¯ä¸°å¯Œã€‚"
        
        # å¦‚æœæœ‰ç›¸å…³æ–‡æ¡£ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
        if relevant_docs:
            context += "\n\nå‚è€ƒæ–‡æ¡£å†…å®¹:\n"
            for i, doc in enumerate(relevant_docs, 1):
                context += f"æ–‡æ¡£ç‰‡æ®µ{i}: {doc}\n\n"
            context += "è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä¸€èˆ¬æ€§å›ç­”ã€‚"
        
        # æ·»åŠ èŠå¤©å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘3æ¡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
        recent_history = chat_history[-3:] if len(chat_history) > 3 else chat_history
        if recent_history:
            context += "\n\næœ€è¿‘çš„å¯¹è¯å†å²:\n"
            for item in recent_history:
                context += f"ç”¨æˆ·: {item['user']}\nåŠ©æ‰‹: {item['assistant']}\n"
        
        context += f"\n\nå½“å‰ç”¨æˆ·é—®é¢˜: {message}"
        
        # è°ƒç”¨Gemini API
        response = model.generate_content(context)
        
        if response.text:
            return response.text.strip()
        else:
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·é‡æ–°è¡¨è¿°ã€‚"
            
    except Exception as e:
        logger.error(f"AIå“åº”ç”Ÿæˆå¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        'status': 'ok',
        'backend': 'Gemini AI',
        'model': MODEL_NAME,
        'rag_system_ready': model is not None,
        'vector_store_ready': embeddings_model is not None and vector_store is not None,
        'chat_history_count': len(chat_history),
        'documents_count': len(document_store),
        'vector_count': vector_store.ntotal if vector_store else 0,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤©ç«¯ç‚¹"""
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›æ¶ˆæ¯å†…å®¹'
            }), 400
        
        user_message = data['message'].strip()
        if not user_message:
            return jsonify({
                'success': False,
                'error': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # è·å–AIå“åº”
        ai_response = get_ai_response(user_message)
        
        # ä¿å­˜åˆ°èŠå¤©å†å²
        chat_history.append({
            'user': user_message,
            'assistant': ai_response,
            'timestamp': datetime.now().isoformat()
        })
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(chat_history) > 100:
            chat_history.pop(0)
        
        return jsonify({
            'success': True,
            'response': ai_response,
            'backend': 'Gemini AI',
            'model': MODEL_NAME,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"èŠå¤©å¤„ç†å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}'
        }), 500

@app.route('/api/upload', methods=['POST'])
def upload_document():
    """æ–‡æ¡£ä¸Šä¼ ç«¯ç‚¹"""
    try:
        logger.info(f"æ”¶åˆ°ä¸Šä¼ è¯·æ±‚ï¼Œæ–‡ä»¶åˆ—è¡¨: {list(request.files.keys())}")
        
        if 'file' not in request.files:
            logger.warning("ä¸Šä¼ è¯·æ±‚ä¸­æ²¡æœ‰æ‰¾åˆ°'file'å­—æ®µ")
            return jsonify({
                'success': False,
                'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'
            }), 400
        
        file = request.files['file']
        logger.info(f"å¤„ç†æ–‡ä»¶: {file.filename}, å¤§å°: {file.content_length if hasattr(file, 'content_length') else 'æœªçŸ¥'}")
        
        if file.filename == '':
            logger.warning("æ–‡ä»¶åä¸ºç©º")
            return jsonify({
                'success': False,
                'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'
            }), 400
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file.seek(0, os.SEEK_END)
        file_size = file.tell()
        file.seek(0)
        
        logger.info(f"æ–‡ä»¶ {file.filename} å®é™…å¤§å°: {file_size} å­—èŠ‚ ({file_size / (1024*1024):.2f} MB)")
        
        if file_size > MAX_FILE_SIZE:
            logger.warning(f"æ–‡ä»¶ {file.filename} å¤§å°è¶…é™: {file_size} > {MAX_FILE_SIZE}")
            return jsonify({
                'success': False,
                'error': f'æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({MAX_FILE_SIZE // (1024*1024)}MB)ï¼Œå½“å‰æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.2f}MB'
            }), 400
        
        # ç”Ÿæˆæ–‡ä»¶ID
        file_hash = hashlib.md5(f"{file.filename}{datetime.now()}".encode()).hexdigest()
        file_path = os.path.join(UPLOAD_FOLDER, f"{file_hash}_{file.filename}")
        
        # ä¿å­˜æ–‡ä»¶
        file.save(file_path)
        
        # æå–æ–‡æœ¬
        text_content = extract_text_from_file(file_path, file.filename)
        
        if not text_content or text_content.startswith("æ–‡ä»¶è¯»å–å¤±è´¥"):
            return jsonify({
                'success': False,
                'error': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {text_content}'
            }), 400
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        add_document_to_vector_store(file_hash, text_content)
        
        # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
        doc_info = {
            'id': file_hash,
            'filename': file.filename,
            'size': file_size,
            'upload_time': datetime.now().isoformat(),
            'text_length': len(text_content),
            'chunks_count': len(chunk_text(text_content)) if text_content else 0
        }
        
        return jsonify({
            'success': True,
            'message': 'æ–‡æ¡£ä¸Šä¼ æˆåŠŸ',
            'document': doc_info,
            'preview': text_content[:500] + "..." if len(text_content) > 500 else text_content
        })
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/history', methods=['GET'])
def get_history():
    """è·å–èŠå¤©å†å²"""
    return jsonify({
        'success': True,
        'history': chat_history,
        'count': len(chat_history)
    })

@app.route('/api/clear', methods=['POST'])
def clear_history():
    """æ¸…ç©ºèŠå¤©å†å²"""
    global chat_history
    chat_history = []
    return jsonify({
        'success': True,
        'message': 'èŠå¤©å†å²å·²æ¸…ç©º'
    })

@app.route('/api/stats', methods=['GET'])
def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡"""
    return jsonify({
        'backend': 'Gemini AI',
        'model': MODEL_NAME,
        'chat_history_count': len(chat_history),
        'documents_count': len(document_store),
        'vector_count': vector_store.ntotal if vector_store else 0,
        'system_ready': model is not None,
        'vector_store_ready': embeddings_model is not None and vector_store is not None,
        'supported_formats': ['txt', 'pdf', 'docx', 'doc'],
        'max_file_size_mb': MAX_FILE_SIZE // (1024*1024),
        'uptime': 'N/A',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/documents', methods=['GET'])
def get_documents():
    """è·å–å·²ä¸Šä¼ çš„æ–‡æ¡£åˆ—è¡¨"""
    docs = []
    for doc_id, doc_info in document_store.items():
        docs.append({
            'id': doc_id,
            'chunks_count': len(doc_info['chunks']),
            'timestamp': doc_info['timestamp']
        })
    
    return jsonify({
        'success': True,
        'documents': docs,
        'total_count': len(docs)
    })

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨åœ¨çº¿RAG APIæœåŠ¡å™¨...")
    print("=" * 50)
    print(f"ğŸ¤– åç«¯: Gemini AI")
    print(f"ğŸ§  æ¨¡å‹: {MODEL_NAME}")
    print(f"ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:5000")
    print("ğŸ“¡ APIç«¯ç‚¹:")
    print("  - GET  /api/health  - å¥åº·æ£€æŸ¥")
    print("  - POST /api/chat    - èŠå¤©æ¥å£")
    print("  - POST /api/upload  - æ–‡æ¡£ä¸Šä¼ ")
    print("  - GET  /api/history - èŠå¤©å†å²")
    print("  - POST /api/clear   - æ¸…ç©ºè®°å½•")
    print("  - GET  /api/stats   - ç³»ç»Ÿç»Ÿè®¡")
    print("=" * 50)
    
    if model:
        print("âœ… Gemini APIé…ç½®æ­£å¸¸")
    else:
        print("âŒ Gemini APIé…ç½®å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥")
    
    print("\nğŸ¯ ä½¿ç”¨è¯´æ˜:")
    print("1. è¿™æ˜¯ä¸€ä¸ªåœ¨çº¿RAGæœåŠ¡ï¼Œä½¿ç”¨Gemini AIä½œä¸ºåç«¯")
    print("2. æ— éœ€æœ¬åœ°éƒ¨ç½²å¤§æ¨¡å‹ï¼Œåªéœ€è¦Gemini APIå¯†é’¥")
    print("3. æ”¯æŒæ™ºèƒ½å¯¹è¯ã€æ–‡æ¡£å¤„ç†ç­‰åŠŸèƒ½")
    print("4. å‰ç«¯ä¼šè‡ªåŠ¨è¿æ¥åˆ°æ­¤APIæœåŠ¡")
    
    app.run(host='0.0.0.0', port=5000, debug=False)